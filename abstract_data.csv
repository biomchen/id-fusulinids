name,id,url,abstract
Elfar Adalsteinsson,arXiv:1608.03907,https://arxiv.org/abs/1608.03907,"Abstract:  We present a robust method to correct for motion and deformations for
in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI
requires robust alignment across time in the presence of substantial and
unpredictable motion. We make a Markov assumption on the nature of deformations
to take advantage of the temporal structure in the image data. Forward message
passing in the corresponding hidden Markov model (HMM) yields an estimation
algorithm that only has to account for relatively small motion between
consecutive frames. We demonstrate the utility of the temporal model by showing
that its use improves the accuracy of the segmentation propagation through
temporal registration. Our results suggest that the proposed model captures
accurately the temporal dynamics of deformations in in-utero MRI time series."
Elfar Adalsteinsson,arXiv:0907.2083,https://arxiv.org/abs/0907.2083,"Abstract:  A linear inverse problem is proposed that requires the determination of
multiple unknown signal vectors. Each unknown vector passes through a different
system matrix and the results are added to yield a single observation vector.
Given the matrices and lone observation, the objective is to find a
simultaneously sparse set of unknown vectors that solves the system. We will
refer to this as the multiple-system single-output (MSSO) simultaneous sparsity
problem. This manuscript contrasts the MSSO problem with other simultaneous
sparsity problems and conducts a thorough initial exploration of algorithms
with which to solve it. Seven algorithms are formulated that approximately
solve this NP-Hard problem. Three greedy techniques are developed (matching
pursuit, orthogonal matching pursuit, and least squares matching pursuit) along
with four methods based on a convex relaxation (iteratively reweighted least
squares, two forms of iterative shrinkage, and formulation as a second-order
cone program). The algorithms are evaluated across three experiments: the first
and second involve sparsity profile recovery in noiseless and noisy scenarios,
respectively, while the third deals with magnetic resonance imaging
radio-frequency excitation pulse design."
Akintunde Akinwande,arXiv:1812.07380,https://arxiv.org/abs/1812.07380,"Abstract:  We present a Machine Learning-based method for tomographic reconstruction of
dense layered objects, with range of projection angles limited to $\pm
$10$^\circ$. Whereas previous approaches to phase tomography generally require
two steps, first to retrieve phase projections from intensity projections and
then perform tomographic reconstruction on the retrieved phase projections, in
our work a physics-informed pre-processor followed by a Deep Neural Network
(DNN) conduct the three-dimensional reconstruction directly from the intensity
projections. We demonstrate this single-step method experimentally in the
visible optical domain on a scaled up integrated circuit phantom. We show that
even under conditions of highly attenuated photon fluxes a DNN trained only on
synthetic data can be used to successfully reconstruct physical samples
disjoint from the synthetic training set. Thus, the need of producing a large
number of physical examples for training is ameliorated. The method is
generally applicable to tomography with electromagnetic or other types of
radiation at all bands."
Mohammad Alizadeh,arXiv:1810.01963,https://arxiv.org/abs/1810.01963,"Abstract:  Efficiently scheduling data processing jobs on distributed compute clusters
requires complex algorithms. Current systems, however, use simple generalized
heuristics and ignore workload structure, since developing and tuning a bespoke
heuristic for each workload is infeasible. In this paper, we show that modern
machine learning techniques can generate highly-efficient policies
automatically.
Decima uses reinforcement learning (RL) and neural networks to learn
workload-specific scheduling algorithms without any human instruction beyond
specifying a high-level objective such as minimizing average job completion
time. Off-the-shelf RL techniques, however, cannot handle the complexity and
scale of the scheduling problem. To build Decima, we had to develop new
representations for jobs' dependency graphs, design scalable RL models, and
invent new RL training methods for continuous job arrivals.
Our prototype integration with Spark on a 25-node cluster shows that Decima
outperforms several heuristics, including hand-tuned ones, by at least 21%.
Further experiments with an industrial production workload trace demonstrate
that Decima delivers up to a 17% reduction in average job completion time and
scales to large clusters."
Mohammad Alizadeh,arXiv:1809.05088,https://arxiv.org/abs/1809.05088,"Abstract:  With the growing usage of Bitcoin and other cryptocurrencies, many
scalability challenges have emerged. A promising scaling solution, exemplified
by the Lightning Network, uses a network of bidirectional payment channels that
allows fast transactions between two parties. However, routing payments on
these networks efficiently is non-trivial, since payments require finding paths
with sufficient funds, and channels can become unidirectional over time
blocking further transactions through them. Today's payment channel networks
exacerbate these problems by attempting to deliver all payments atomically. In
this paper, we present the Spider network, a new packet-switched architecture
for payment channel networks. Spider splits payments into transaction units and
transmits them over time across different paths. Spider uses congestion
control, payment scheduling, and imbalance-aware routing to optimize delivery
of payments. Our results show that Spider improves the volume and number of
successful payments on the network by 10-45% and 5-40% respectively compared to
state-of-the-art approaches."
Mohammad Alizadeh,arXiv:1808.00826,https://arxiv.org/abs/1808.00826,"Abstract:  Prior research has proposed technical solutions to use peer-to-peer (P2P)
content delivery to serve Internet video, showing that it can reduce costs to
content providers. Yet, such methods have not become widespread except for a
few niche instances. An important challenge is incentivization: what tangible
benefits does P2P content delivery offer users who bring resources to the
table? In this paper, we ask whether monetary incentives can help attract peers
in P2P content delivery systems. We commissioned a professional survey of
people around theUnited States to answer several relevant questions. We found
that 51% of the 876 respondents--substantially larger than our
expectations--answered ""yes"" to whether they would participate for suitable
financial incentives. Encouraged by the results of the survey, we propose
Gringotts, a system to structure incentives and securely incorporate P2P
delivery into content delivery systems. Gringotts provides a novel Proof of
Delivery mechanism that allows content providers to verify correct delivery of
their files, and shows how to use cryptocurrency to pay peers while guarding
against liars and Sybil attacks."
Mohammad Alizadeh,arXiv:1807.02264,https://arxiv.org/abs/1807.02264,"Abstract:  We consider reinforcement learning in input-driven environments, where an
exogenous, stochastic input process affects the dynamics of the system. Input
processes arise in many applications, including queuing systems, robotics
control with disturbances, and object tracking. Since the state dynamics and
rewards depend on the input process, the state alone provides limited
information for the expected future returns. Therefore, policy gradient methods
with standard state-dependent baselines suffer high variance during training.
We derive a bias-free, input-dependent baseline to reduce this variance, and
analytically show its benefits over state-dependent baselines. We then propose
a meta-learning approach to overcome the complexity of learning a baseline that
depends on a long sequence of inputs. Our experimental results show that across
environments from queuing systems, computer networks, and MuJoCo robotic
locomotion, input-dependent baselines consistently improve training stability
and result in better eventual policies."
Mohammad Alizadeh,arXiv:1803.09615,https://arxiv.org/abs/1803.09615,"Abstract:  Homa is a new transport protocol for datacenter networks. It provides
exceptionally low latency, especially for workloads with a high volume of very
short messages, and it also supports large messages and high network
utilization. Homa uses in-network priority queues to ensure low latency for
short messages; priority allocation is managed dynamically by each receiver and
integrated with a receiver-driven flow control mechanism. Homa also uses
controlled overcommitment of receiver downlinks to ensure efficient bandwidth
utilization at high load. Our implementation of Homa delivers 99th percentile
round-trip times less than 15{\mu}s for short messages on a 10 Gbps network
running at 80% load. These latencies are almost 100x lower than the best
published measurements of an implementation. In simulations, Homa's latency is
roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for
almost all message sizes and workloads. Homa can also sustain higher network
loads than pFabric, pHost, or PIAS."
Mohammad Alizadeh,arXiv:1802.08730,https://arxiv.org/abs/1802.08730,"Abstract:  This paper develops a technique to detect whether the cross traffic competing
with a flow is elastic or not, and shows how to use the elasticity detector to
improve congestion control. If the cross traffic is elastic, i.e., made up of
flows like Cubic or NewReno that increase their rate when they perceive
available bandwidth, then one should use a scheme that competes well with such
traffic. Such a scheme will not be able to control delays because the cross
traffic will not cooperate to maintain low delays. If, however, cross traffic
is inelastic, then one can use a suitable delay-controlled algorithm. Our
elasticity detector uses an asymmetric sinusoidal pulse pattern and estimates
elasticity by computing the frequency response (FFT) of the cross traffic
estimate; we have measured its accuracy to be over 90%. We present the design
and evaluation of Nimbus, a congestion control protocol that uses the
elasticity detector to switch between delay-control and TCP-competitive modes.
Our results on emulated and real-world paths show that Nimbus achieves
throughput comparable to or better than Cubic always, but with delays that are
much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to
Cubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which
also switches between a delay-controlling and a TCP-competitive mode, Nimbus is
more robust at correctly detecting the nature of cross traffic, and unlike
Copa, it is usable by a variety of delay-based and TCP-competitive methods."
Mohammad Alizadeh,arXiv:1802.04948,https://arxiv.org/abs/1802.04948,"Abstract:  Neural networks have been shown to be an effective tool for learning
algorithms over graph-structured data. However, graph representation
techniques---that convert graphs to real-valued vectors for use with neural
networks---are still in their infancy. Recent works have proposed several
approaches (e.g., graph convolutional networks), but these methods have
difficulty scaling and generalizing to graphs with different sizes and shapes.
We present Graph2Seq, a new technique that represents vertices of graphs as
infinite time-series. By not limiting the representation to a fixed dimension,
Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq
is also reversible, allowing full recovery of the graph structure from the
sequences. By analyzing a formal computational model for graph representation,
we show that an unbounded sequence is necessary for scalability. Our
experimental results with Graph2Seq show strong generalization and new
state-of-the-art performance on a variety of graph combinatorial optimization
problems."
Mohammad Alizadeh,arXiv:1802.03680,https://arxiv.org/abs/1802.03680,"Abstract:  Mapping road networks is currently both expensive and labor-intensive.
High-resolution aerial imagery provides a promising avenue to automatically
infer a road network. Prior work uses convolutional neural networks (CNNs) to
detect which pixels belong to a road (segmentation), and then uses complex
post-processing heuristics to infer graph connectivity. We show that these
segmentation methods have high error rates because noisy CNN outputs are
difficult to correct. We propose RoadTracer, a new method to automatically
construct accurate road network maps from aerial images. RoadTracer uses an
iterative search process guided by a CNN-based decision function to derive the
road network graph directly from the output of the CNN. We compare our approach
with a segmentation method on fifteen cities, and find that at a 5% error rate,
RoadTracer correctly captures 45% more junctions across these cities."
Mohammad Alizadeh,arXiv:1801.04519,https://arxiv.org/abs/1801.04519,"Abstract:  We define the Fitzpatrick function of a $\sigma$-monotone operator in a way
similar to the original definition given by Fitzpatrick. We show that some
well-known properties of Fitzpatrick function remain valid for the larger class
of premonotone operators. Also, we find some conditions under which the
Fitzpatrick function of a $\sigma$-monotone operator is proper, and give some
results in Hilbert spaces."
Mohammad Alizadeh,arXiv:1702.02588,https://arxiv.org/abs/1702.02588,"Abstract:  As its price per bit drops, SSD is increasingly becoming the default storage
medium for cloud application databases. However, it has not become the
preferred storage medium for key-value caches, even though SSD offers more than
10x lower price per bit and sufficient performance compared to DRAM. This is
because key-value caches need to frequently insert, update and evict small
objects. This causes excessive writes and erasures on flash storage, since
flash only supports writes and erasures of large chunks of data. These
excessive writes and erasures significantly shorten the lifetime of flash,
rendering it impractical to use for key-value caches. We present Flashield, a
hybrid key-value cache that uses DRAM as a ""filter"" to minimize writes to SSD.
Flashield performs light-weight machine learning profiling to predict which
objects are likely to be read frequently before getting updated; these objects,
which are prime candidates to be stored on SSD, are written to SSD in large
chunks sequentially. In order to efficiently utilize the cache's available
memory, we design a novel in-memory index for the variable-sized objects stored
on flash that requires only 4 bytes per object in DRAM. We describe Flashield's
design and implementation and, we evaluate it on a real-world cache trace.
Compared to state-of-the-art systems that suffer a write amplification of 2.5x
or more, Flashield maintains a median write amplification of 0.5x without any
loss of hit rate or throughput."
Mohammad Alizadeh,arXiv:1602.06045,https://arxiv.org/abs/1602.06045,"Abstract:  Switches today provide a small set of scheduling algorithms. While we can
tweak scheduling parameters, we cannot modify algorithmic logic, or add a
completely new algorithm, after the switch has been designed. This paper
presents a design for a programmable packet scheduler, which allows scheduling
algorithms---potentially algorithms that are unknown today---to be programmed
into a switch without requiring hardware redesign.
Our design builds on the observation that scheduling algorithms make two
decisions: in what order to schedule packets and when to schedule them.
Further, in many scheduling algorithms these decisions can be made when packets
are enqueued. We leverage this observation to build a programmable scheduler
using a single abstraction: the push-in first-out queue (PIFO), a priority
queue that maintains the scheduling order and time for such algorithms.
We show that a programmable scheduler using PIFOs lets us program a wide
variety of scheduling algorithms. We present a detailed hardware design for
this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area
overhead on a 16-nm standard-cell library. Our design lets us program many
sophisticated algorithms, such as a 5-level hierarchical scheduler with
programmable scheduling algorithms at each level."
Mohammad Alizadeh,arXiv:1512.05023,https://arxiv.org/abs/1512.05023,"Abstract:  Many algorithms for congestion control, scheduling, network measurement,
active queue management, security, and load balancing require custom processing
of packets as they traverse the data plane of a network switch. To run at line
rate, these data-plane algorithms must be in hardware. With today's switch
hardware, algorithms cannot be changed, nor new algorithms installed, after a
switch has been built.
This paper shows how to program data-plane algorithms in a high-level
language and compile those programs into low-level microcode that can run on
emerging programmable line-rate switching chipsets. The key challenge is that
these algorithms create and modify algorithmic state. The key idea to achieve
line-rate programmability for stateful algorithms is the notion of a packet
transaction : a sequential code block that is atomic and isolated from other
such code blocks. We have developed this idea in Domino, a C-like imperative
language to express data-plane algorithms. We show with many examples that
Domino provides a convenient and natural way to express sophisticated
data-plane algorithms, and show that these algorithms can be run at line rate
with modest estimated die-area overhead."
Mohammad Alizadeh,arXiv:1512.01271,https://arxiv.org/abs/1512.01271,"Abstract:  Hybrid switching - in which a high bandwidth circuit switch (optical or
wireless) is used in conjunction with a low bandwidth packet switch - is a
promising alternative to interconnect servers in today's large scale
data-centers. Circuit switches offer a very high link rate, but incur a
non-trivial reconfiguration delay which makes their scheduling challenging. In
this paper, we demonstrate a lightweight, simple and nearly-optimal scheduling
algorithm that trades-off configuration costs with the benefits of
reconfiguration that match the traffic demands. The algorithm has strong
connections to submodular optimization, has performance at least half that of
the optimal schedule and strictly outperforms state of the art in a variety of
traffic demand settings. These ideas naturally generalize: we see that indirect
routing leads to exponential connectivity; this is another phenomenon of the
power of multi hop routing, distinct from the well-known load balancing
effects."
Mohammad Alizadeh,arXiv:1405.7143,https://arxiv.org/abs/1405.7143,"Abstract:  This paper presents a practical approach to rapidly introduce new dataplane
functionality into networks: End-hosts embed tiny programs into packets to
actively query and manipulate a network's internal state. We show how this
""tiny packet program"" (TPP) interface gives end-hosts unprecedented visibility
into network behavior, enabling them to work with the network to achieve a
common goal. Our design leverages what each component does best: (a) switches
forward and execute tiny packet programs (at most 5 instructions) at line rate,
and (b) end-hosts perform arbitrary computation on network state, which are
easy to evolve. Using a hardware prototype on a NetFPGA, we show our design is
feasible, at a reasonable cost. By implementing three different research
proposals, we show that TPPs are also useful. And finally, we present an
architecture in which they can be made secure."
Saman Amarasinghe,arXiv:1902.02816,https://arxiv.org/abs/1902.02816,"Abstract:  Modern microprocessors are equipped with Single Instruction Multiple Data
(SIMD) or vector instructions which expose data level parallelism at a fine
granularity. Programmers exploit this parallelism by using low-level vector
intrinsics in their code. However, once programs are written using vector
intrinsics of a specific instruction set, the code becomes non-portable. Modern
compilers are unable to analyze and retarget the code to newer vector
instruction sets. Hence, programmers have to manually rewrite the same code
using vector intrinsics of a newer generation to exploit higher data widths and
capabilities of new instruction sets. This process is tedious, error-prone and
requires maintaining multiple code bases. We propose Revec, a compiler
optimization pass which revectorizes already vectorized code, by retargeting it
to use vector instructions of newer generations. The transformation is
transparent, happening at the compiler intermediate representation level, and
enables performance portability of hand-vectorized code.
Revec can achieve performance improvements in real-world performance critical
kernels. In particular, Revec achieves geometric mean speedups of 1.160$\times$
and 1.430$\times$ on fast integer unpacking kernels, and speedups of
1.145$\times$ and 1.195$\times$ on hand-vectorized x265 media codec kernels
when retargeting their SSE-series implementations to use AVX2 and AVX-512
vector instructions respectively. We also extensively test Revec's impact on
216 intrinsic-rich implementations of image processing and stencil kernels
relative to hand-retargeting."
Saman Amarasinghe,arXiv:1808.07412,https://arxiv.org/abs/1808.07412,"Abstract:  Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure."
Saman Amarasinghe,arXiv:1807.01624,https://arxiv.org/abs/1807.01624,"Abstract:  Modern out-of-order processors have increased capacity to exploit instruction
level parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide
superscalar pipelines and vector execution units, as well as deep buffers for
in-flight memory requests. These resources, however, often exhibit poor
utilization rates on workloads with large working sets, e.g., in-memory
databases, key-value stores, and graph analytics, as compilers and hardware
struggle to expose ILP and MLP from the instruction stream automatically.
In this paper, we introduce the IMLP (Instruction and Memory Level
Parallelism) task programming model. IMLP tasks execute as coroutines that
yield execution at annotated long-latency operations, e.g., memory accesses,
divisions, or unpredictable branches. IMLP tasks are interleaved on a single
thread, and integrate well with thread parallelism and vectorization. Our DSL
embedded in C++, Cimple, allows exploration of task scheduling and
transformations, such as buffering, vectorization, pipelining, and prefetching.
We demonstrate state-of-the-art performance on core algorithms used in
in-memory databases that operate on arrays, hash tables, trees, and skip lists.
Cimple applications reach 2.5x throughput gains over hardware multithreading on
a multi-core, and 6.4x single thread speedup."
Saman Amarasinghe,arXiv:1805.00923,https://arxiv.org/abs/1805.00923,"Abstract:  The performance bottlenecks of graph applications depend not only on the
algorithm and the underlying hardware, but also on the size and structure of
the input graph. Programmers must try different combinations of a large set of
techniques to develop the best implementation for a specific algorithm and type
of graph. Existing graph frameworks lack flexibility, supporting only a limited
set of optimizations.
This paper introduces GraphIt, a new DSL for graph computations that
generates fast implementations for algorithms with different performance
characteristics running on graphs with different sizes and structures. GraphIt
separates what is computed (algorithm) from how it is computed (schedule).
Programmers specify the algorithm using an algorithm language, and performance
optimizations are specified using a scheduling language. The algorithm language
simplifies expressing the algorithms. We formulate graph optimizations,
including edge traversal direction, data layout, parallelization, cache, NUMA,
and kernel fusion optimizations, as tradeoffs among locality, parallelism, and
work-efficiency. The scheduling language enables programmers to easily search
through this complicated tradeoff space by composing together optimizations. We
also built an autotuner to automatically find high-performance schedules. The
compiler uses a new scheduling representation, the graph iteration space, to
model, compose, and ensure the validity of the large number of optimizations.
GraphIt outperforms the next fastest of six state-of-the-art shared-memory
frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24
out of 32 experiments by up to 4.8$\times$, and is never more than 43% slower
than the fastest framework on the other experiments. GraphIt also reduces the
lines of code by up to an order of magnitude compared to the next fastest
framework."
Saman Amarasinghe,arXiv:1804.10694,https://arxiv.org/abs/1804.10694,"Abstract:  This paper introduces Tiramisu, a polyhedral framework designed to generate
high performance code for multiple platforms including multicores, GPUs, and
distributed machines. Tiramisu introduces a scheduling language with novel
extensions to explicitly manage the complexities that arise when targeting
these systems. The framework is designed for the areas of image processing,
stencils, linear algebra and deep learning. Tiramisu has two main features: it
relies on a flexible representation based on the polyhedral model and it has a
rich scheduling language allowing fine-grained control of optimizations.
Tiramisu uses a four-level intermediate representation that allows full
separation between the algorithms, loop transformations, data layouts, and
communication. This separation simplifies targeting multiple hardware
architectures with the same algorithm. We evaluate Tiramisu by writing a set of
image processing, deep learning, and linear algebra benchmarks and compare them
with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu
matches or outperforms existing compilers and libraries on different hardware
architectures, including multicore CPUs, GPUs, and distributed machines."
Saman Amarasinghe,arXiv:1804.10112,https://arxiv.org/abs/1804.10112,"Abstract:  This paper shows how to build a sparse tensor algebra compiler that is
agnostic to tensor formats (data layouts). We develop an interface that
describes formats in terms of their capabilities and properties, and show how
to build a modular code generator where new formats can be added as plugins. We
then describe six implementations of the interface that compose to form the
dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants
thereof. With these implementations at hand, our code generator can generate
code to compute any tensor algebra expression on any combination of the
aforementioned formats.
To demonstrate our technique, we have implemented it in the taco tensor
algebra compiler. Our modular code generator design makes it simple to add
support for new tensor formats, and the performance of the generated code is
competitive with hand-optimized implementations. Furthermore, by extending taco
to support a wider range of formats specialized for different application and
data characteristics, we can improve end-user application performance. For
example, if input data is provided in the COO format, our technique allows
computing a single matrix-vector multiplication directly with the data in COO,
which is up to 3.6$\times$ faster than by first converting the data to CSR."
Saman Amarasinghe,arXiv:1804.08733,https://arxiv.org/abs/1804.08733,"Abstract:  Modern microprocessors are equipped with single instruction multiple data
(SIMD) or vector instruction sets which allow compilers to exploit superword
level parallelism (SLP), a type of fine-grained parallelism. Current SLP
auto-vectorization techniques use heuristics to discover vectorization
opportunities in high-level language code. These heuristics are fragile, local
and typically only present one vectorization strategy that is either accepted
or rejected by a cost model. We present goSLP, a novel SLP auto-vectorization
framework which solves the statement packing problem in a pairwise optimal
manner. Using an integer linear programming (ILP) solver, goSLP searches the
entire space of statement packing opportunities for a whole function at a time,
while limiting total compilation time to a few minutes. Furthermore, goSLP
optimally solves the vector permutation selection problem using dynamic
programming. We implemented goSLP in the LLVM compiler infrastructure,
achieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp
and 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer."
Saman Amarasinghe,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Saman Amarasinghe,arXiv:1803.00419,https://arxiv.org/abs/1803.00419,"Abstract:  High-performance DSL developers work hard to take advantage of modern
hardware. The DSL compilers have to build their own complex middle-ends before
they can target a common back-end such as LLVM, which only handles single
instruction streams with SIMD instructions. We introduce Tiramisu, a common
middle-end that can generate efficient code for modern processors and
accelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu
introduces a novel three-level IR that separates the algorithm, how that
algorithm is executed, and where intermediate data are stored. This separation
simplifies optimization and makes targeting multiple hardware architectures
from the same algorithm easier. As a result, DSL compilers can be made
considerably less complex with no loss of performance while immediately
targeting multiple hardware or hardware combinations such as distributed nodes
with both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for
the Halide and Julia compilers. We show that Tiramisu extends Halide and Julia
with many new capabilities including the ability to: express new algorithms
(such as recurrent filters and non-rectangular iteration spaces), perform new
complex loop nest transformations (such as wavefront parallelization, loop
shifting and loop fusion) and generate efficient code for more architectures
(such as combinations of distributed clusters, multicores, GPUs and FPGAs).
Finally, we demonstrate that Tiramisu can generate very efficient code that
matches the highly optimized Intel MKL gemm (generalized matrix multiplication)
implementation, we also show speedups reaching 4X in Halide and 16X in Julia
due to optimizations enabled by Tiramisu."
Saman Amarasinghe,arXiv:1802.10574,https://arxiv.org/abs/1802.10574,"Abstract:  This paper shows how to optimize sparse tensor algebraic expressions by
introducing temporary tensors, called workspaces, into the resulting loop
nests. We develop a new intermediate language for tensor operations called
concrete index notation that extends tensor index notation. Concrete index
notation expresses when and where sub-computations occur and what tensor they
are stored into. We then describe the workspace optimization in this language,
and how to compile it to sparse code by building on prior work in the
literature.
We demonstrate the importance of the optimization on several important sparse
tensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse
tensor addition (SpAdd), and the matricized tensor times Khatri-Rao product
(MTTKRP) used to factorize tensors. Our results show improvements over prior
work on tensor algebra compilation and brings the performance of these kernels
on par with state-of-the-art hand-optimized implementations. For example, SpMM
was not supported by prior tensor algebra compilers, the performance of MTTKRP
on the nell-2 data set improves by 35%, and MTTKRP can for the first time have
sparse results."
Saman Amarasinghe,arXiv:1709.06416,https://arxiv.org/abs/1709.06416,"Abstract:  Data analytics applications combine multiple functions from different
libraries and frameworks. Even when each function is optimized in isolation,
the performance of the combined application can be an order of magnitude below
hardware limits due to extensive data movement across these functions. To
address this problem, we propose Weld, a new interface between data-intensive
libraries that can optimize across disjoint libraries and functions. Weld
exposes a lazily-evaluated API where diverse functions can submit their
computations in a simple but general intermediate representation that captures
their data-parallel structure. It then optimizes data movement across these
functions and emits efficient code for diverse hardware. Weld can be integrated
into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without
changing their user-facing APIs. We demonstrate that Weld can speed up
applications using these frameworks by up to 29x."
Saman Amarasinghe,arXiv:1608.01362,https://arxiv.org/abs/1608.01362,"Abstract:  Modern hardware systems are heavily underutilized when running large-scale
graph applications. While many in-memory graph frameworks have made substantial
progress in optimizing these applications, we show that it is still possible to
achieve up to 4 $\times$ speedups over the fastest frameworks by greatly
improving cache utilization. Previous systems have applied out-of-core
processing techniques from the memory/disk boundary to the cache/DRAM boundary.
However, we find that blindly applying such techniques is ineffective because
of the much smaller performance gap between DRAM and cache. We present two
techniques that take advantage of the cache with minimal or no instruction
overhead. The first, frequency based clustering, groups together frequently
accessed vertices to improve the utilization of each cache line with no runtime
overhead. The second, CSR segmenting, partitions the graph to restrict all
random accesses to the cache, makes all DRAM access sequential, and merges
partition results using a very low overhead cache-aware merge. Both techniques
can be easily implemented on top of optimized graph frameworks. Our techniques
combined give speedups of up to 4 $\times$ for PageRank, Label Propagation and
Collaborative Filtering, and 2 $\times$ for Betweenness Centrality over the
best published results"
Dimitri Antoniadis,arXiv:1807.00272,https://arxiv.org/abs/1807.00272,"Abstract:  We present a compact model for Tunnel Field Effect Transistors (TFET), that
captures sev- eral non-idealities such as the Trap Assisted Tunneling (TAT)
originating from interface traps (Dit), along with Verilog-A implementation. We
show that the TAT, together with band edge non-abruptness known as the Urbach
tail, sets the lower limit of the sub-threshold swing and the minimum
achievable current at a given temperature. Presence of charged trap states also
contributes to reduced gate efficiency. We show that we can decouple the
contribution of each of these processes and extract the intrinsic sub-threshold
swing from a given experimental data. We derive closed form expressions of
channel potential, electric field and effective tunnel energy window to
accurately capture the essential device physics of TFETs. We test the model
against recently published exper- imental data, and simulate simple TFET
circuits using the Verilog-A model. The compact model provides a framework for
TFET technology projections with improved device metrics such as better
electrostatic design, reduced TAT, material with better transport properties
etc."
Dimitri Antoniadis,arXiv:1603.06654,https://arxiv.org/abs/1603.06654,"Abstract:  We provide a detailed study of the interface Trap Assisted Tunneling (TAT)
mechanism in tunnel field effect transistors to show how it contributes a major
leakage current path before the Band To Band Tunneling (BTBT) is initiated.
With a modified Shockley-Read-Hall formalism, we show that at room temperature,
the phonon assisted TAT current always dominates and obscures the steep turn ON
of the BTBT current for common densities of traps. Our results are applicable
to top gate, double gate and gate all around structures where the traps are
positioned between the source-channel tunneling region. Since the TAT has
strong dependence on electric field, any effort to increase the BTBT current by
enhancing local electric field also increases the leakage current. Unless the
BTBT current can be increased separately, calculations show that the trap
density Dit has to be decreased by 40-100 times compared with the state of the
art in order for the steep turn ON (for III-V materials) to be clearly
observable at room temperature. We find that the combination of the intrinsic
sharpness of the band edges (Urbach tail) and the surface trap density
determines the subthreshold swing."
Hari Balakrishnan,arXiv:1808.00826,https://arxiv.org/abs/1808.00826,"Abstract:  Prior research has proposed technical solutions to use peer-to-peer (P2P)
content delivery to serve Internet video, showing that it can reduce costs to
content providers. Yet, such methods have not become widespread except for a
few niche instances. An important challenge is incentivization: what tangible
benefits does P2P content delivery offer users who bring resources to the
table? In this paper, we ask whether monetary incentives can help attract peers
in P2P content delivery systems. We commissioned a professional survey of
people around theUnited States to answer several relevant questions. We found
that 51% of the 876 respondents--substantially larger than our
expectations--answered ""yes"" to whether they would participate for suitable
financial incentives. Encouraged by the results of the survey, we propose
Gringotts, a system to structure incentives and securely incorporate P2P
delivery into content delivery systems. Gringotts provides a novel Proof of
Delivery mechanism that allows content providers to verify correct delivery of
their files, and shows how to use cryptocurrency to pay peers while guarding
against liars and Sybil attacks."
Hari Balakrishnan,arXiv:1802.08730,https://arxiv.org/abs/1802.08730,"Abstract:  This paper develops a technique to detect whether the cross traffic competing
with a flow is elastic or not, and shows how to use the elasticity detector to
improve congestion control. If the cross traffic is elastic, i.e., made up of
flows like Cubic or NewReno that increase their rate when they perceive
available bandwidth, then one should use a scheme that competes well with such
traffic. Such a scheme will not be able to control delays because the cross
traffic will not cooperate to maintain low delays. If, however, cross traffic
is inelastic, then one can use a suitable delay-controlled algorithm. Our
elasticity detector uses an asymmetric sinusoidal pulse pattern and estimates
elasticity by computing the frequency response (FFT) of the cross traffic
estimate; we have measured its accuracy to be over 90%. We present the design
and evaluation of Nimbus, a congestion control protocol that uses the
elasticity detector to switch between delay-control and TCP-competitive modes.
Our results on emulated and real-world paths show that Nimbus achieves
throughput comparable to or better than Cubic always, but with delays that are
much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to
Cubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which
also switches between a delay-controlling and a TCP-competitive mode, Nimbus is
more robust at correctly detecting the nature of cross traffic, and unlike
Copa, it is usable by a variety of delay-based and TCP-competitive methods."
Hari Balakrishnan,arXiv:1802.03680,https://arxiv.org/abs/1802.03680,"Abstract:  Mapping road networks is currently both expensive and labor-intensive.
High-resolution aerial imagery provides a promising avenue to automatically
infer a road network. Prior work uses convolutional neural networks (CNNs) to
detect which pixels belong to a road (segmentation), and then uses complex
post-processing heuristics to infer graph connectivity. We show that these
segmentation methods have high error rates because noisy CNN outputs are
difficult to correct. We propose RoadTracer, a new method to automatically
construct accurate road network maps from aerial images. RoadTracer uses an
iterative search process guided by a CNN-based decision function to derive the
road network graph directly from the output of the CNN. We compare our approach
with a segmentation method on fifteen cities, and find that at a 5% error rate,
RoadTracer correctly captures 45% more junctions across these cities."
Hari Balakrishnan,arXiv:1602.06045,https://arxiv.org/abs/1602.06045,"Abstract:  Switches today provide a small set of scheduling algorithms. While we can
tweak scheduling parameters, we cannot modify algorithmic logic, or add a
completely new algorithm, after the switch has been designed. This paper
presents a design for a programmable packet scheduler, which allows scheduling
algorithms---potentially algorithms that are unknown today---to be programmed
into a switch without requiring hardware redesign.
Our design builds on the observation that scheduling algorithms make two
decisions: in what order to schedule packets and when to schedule them.
Further, in many scheduling algorithms these decisions can be made when packets
are enqueued. We leverage this observation to build a programmable scheduler
using a single abstraction: the push-in first-out queue (PIFO), a priority
queue that maintains the scheduling order and time for such algorithms.
We show that a programmable scheduler using PIFOs lets us program a wide
variety of scheduling algorithms. We present a detailed hardware design for
this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area
overhead on a 16-nm standard-cell library. Our design lets us program many
sophisticated algorithms, such as a 5-level hierarchical scheduler with
programmable scheduling algorithms at each level."
Hari Balakrishnan,arXiv:1512.05023,https://arxiv.org/abs/1512.05023,"Abstract:  Many algorithms for congestion control, scheduling, network measurement,
active queue management, security, and load balancing require custom processing
of packets as they traverse the data plane of a network switch. To run at line
rate, these data-plane algorithms must be in hardware. With today's switch
hardware, algorithms cannot be changed, nor new algorithms installed, after a
switch has been built.
This paper shows how to program data-plane algorithms in a high-level
language and compile those programs into low-level microcode that can run on
emerging programmable line-rate switching chipsets. The key challenge is that
these algorithms create and modify algorithmic state. The key idea to achieve
line-rate programmability for stateful algorithms is the notion of a packet
transaction : a sequential code block that is atomic and isolated from other
such code blocks. We have developed this idea in Domino, a C-like imperative
language to express data-plane algorithms. We show with many examples that
Domino provides a convenient and natural way to express sophisticated
data-plane algorithms, and show that these algorithms can be run at line rate
with modest estimated die-area overhead."
Hari Balakrishnan,arXiv:1206.0418,https://arxiv.org/abs/1206.0418,"Abstract:  This paper presents an analysis of spinal codes, a class of rateless codes
proposed recently. We prove that spinal codes achieve Shannon capacity for the
binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN)
channel with an efficient polynomial-time encoder and decoder. They are the
first rateless codes with proofs of these properties for BSC and AWGN. The key
idea in the spinal code is the sequential application of a hash function over
the message bits. The sequential structure of the code turns out to be crucial
for efficient decoding. Moreover, counter to the wisdom of having an expander
structure in good codes, we show that the spinal code, despite its sequential
structure, achieves capacity. The pseudo-randomness provided by a hash function
suffices for this purpose. Our proof introduces a variant of Gallager's result
characterizing the error exponent of random codes for any memoryless channel.
We present a novel application of these error-exponent results within the
framework of an efficient sequential code. The application of a hash function
over the message bits provides a methodical and effective way to de-randomize
Shannon's random codebook construction."
Hari Balakrishnan,arXiv:cs/0104012,https://arxiv.org/abs/cs/0104012,"Abstract:  This paper describes the implementation and evaluation of an operating system
module, the Congestion Manager (CM), which provides integrated network flow
management and exports a convenient programming interface that allows
applications to be notified of, and adapt to, changing network conditions. We
describe the API by which applications interface with the CM, and the
architectural considerations that factored into the design. To evaluate the
architecture and API, we describe our implementations of TCP; a streaming
layered audio/video application; and an interactive audio application using the
CM, and show that they achieve adaptive behavior without incurring much
end-system overhead. All flows including TCP benefit from the sharing of
congestion information, and applications are able to incorporate new
functionality such as congestion control and adaptive behavior."
Regina Barzilay,arXiv:1812.01070,https://arxiv.org/abs/1812.01070,"Abstract:  We view molecular optimization as a graph-to-graph translation problem. The
goal is to learn to map from one molecular graph to another with better
properties based on an available corpus of paired molecules. Since molecules
can be optimized in different ways, there are multiple viable translations for
each input graph. A key challenge is therefore to model diverse translation
outputs. Our primary contributions include a junction tree encoder-decoder for
learning diverse graph translations along with a novel adversarial training
method for aligning distributions of molecules. Diverse output distributions in
our model are explicitly realized by low-dimensional latent vectors that
modulate the translation process. We evaluate our model on multiple molecular
optimization tasks and show that our model outperforms previous
state-of-the-art baselines."
Regina Barzilay,arXiv:1810.13083,https://arxiv.org/abs/1810.13083,"Abstract:  Most modern Information Extraction (IE) systems are implemented as sequential
taggers and focus on modelling local dependencies. Non-local and non-sequential
context is, however, a valuable source of information to improve predictions.
In this paper, we introduce GraphIE, a framework that operates over a graph
representing both local and non-local dependencies between textual units (i.e.
words or sentences). The algorithm propagates information between connected
nodes through graph convolutions and exploits the richer representation to
improve word level predictions. The framework is evaluated on three different
tasks, namely social media, textual and visual information extraction. Results
show that GraphIE outperforms a competitive baseline (BiLSTM+CRF) in all tasks
by a significant margin."
Regina Barzilay,arXiv:1809.02256,https://arxiv.org/abs/1809.02256,"Abstract:  We propose a mixture-of-experts approach for unsupervised domain adaptation
from multiple sources. The key idea is to explicitly capture the relationship
between a target example and different source domains. This relationship,
expressed by a point-to-set metric, determines how to combine predictors
trained on various domains. The metric is learned in an unsupervised fashion
using meta-training. Experimental results on sentiment analysis and
part-of-speech tagging demonstrate that our approach consistently outperforms
multiple baselines and can robustly handle negative transfer."
Regina Barzilay,arXiv:1808.09367,https://arxiv.org/abs/1808.09367,"Abstract:  Attention-based models are successful when trained on large amounts of data.
In this paper, we demonstrate that even in the low-resource scenario, attention
can be learned effectively. To this end, we start with discrete human-annotated
rationales and map them into continuous attention. Our central hypothesis is
that this mapping is general across domains, and thus can be transferred from
resource-rich domains to low-resource ones. Our model jointly learns a
domain-invariant representation and induces the desired mapping between
rationales and attention. Our empirical results validate this hypothesis and
show that our approach delivers significant gains over state-of-the-art
baselines, yielding over 15% average error reduction on benchmark datasets."
Regina Barzilay,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Regina Barzilay,arXiv:1802.04364,https://arxiv.org/abs/1802.04364,"Abstract:  We seek to automate the design of molecules based on specific chemical
properties. In computational terms, this task involves continuous embedding and
generation of molecular graphs. Our primary contribution is the direct
realization of molecular graphs, a task previously approached by generating
linear SMILES strings instead of graphs. Our junction tree variational
autoencoder generates molecular graphs in two phases, by first generating a
tree-structured scaffold over chemical substructures, and then combining them
into a molecule with a graph message passing network. This approach allows us
to incrementally expand molecules while maintaining chemical validity at every
step. We evaluate our model on multiple tasks ranging from molecular generation
to optimization. Across these tasks, our model outperforms previous
state-of-the-art baselines by a significant margin."
Regina Barzilay,arXiv:1709.04555,https://arxiv.org/abs/1709.04555,"Abstract:  The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts."
Regina Barzilay,arXiv:1708.00133,https://arxiv.org/abs/1708.00133,"Abstract:  In this paper, we explore the utilization of natural language to drive
transfer for reinforcement learning (RL). Despite the wide-spread application
of deep RL techniques, learning generalized policy representations that work
across domains remains a challenging problem. We demonstrate that textual
descriptions of environments provide a compact intermediate channel to
facilitate effective policy transfer. Specifically, by learning to ground the
meaning of text to the dynamics of the environment such as transitions and
rewards, an autonomous agent can effectively bootstrap policy learning on a new
domain given its description. We employ a model-based RL approach consisting of
a differentiable planning module, a model-free component and a factorized state
representation to effectively use entity descriptions. Our model outperforms
prior work on both transfer and multi-task scenarios in a variety of different
environments. For instance, we achieve up to 14% and 11.5% absolute improvement
over previously existing models in terms of average and initial rewards,
respectively."
Regina Barzilay,arXiv:1707.03938,https://arxiv.org/abs/1707.03938,"Abstract:  The interpretation of spatial references is highly contextual, requiring
joint inference over both language and the environment. We consider the task of
spatial reasoning in a simulated environment, where an agent can act and
receive rewards. The proposed model learns a representation of the world
steered by instruction text. This design allows for precise alignment of local
neighborhoods with corresponding verbalizations, while also handling global
references in the instructions. We train our model with reinforcement learning
using a variant of generalized value iteration. The model outperforms
state-of-the-art approaches on several metrics, yielding a 45% reduction in
goal localization error."
Regina Barzilay,arXiv:1705.09655,https://arxiv.org/abs/1705.09655,"Abstract:  This paper focuses on style transfer on the basis of non-parallel text. This
is an instance of a broad family of problems including machine translation,
decipherment, and sentiment modification. The key challenge is to separate the
content from other aspects such as style. We assume a shared latent content
distribution across different text corpora, and propose a method that leverages
refined alignment of latent representations to perform style transfer. The
transferred sentences from one style should match example sentences from the
other style as a population. We demonstrate the effectiveness of this
cross-alignment method on three tasks: sentiment modification, decipherment of
word substitution ciphers, and recovery of word order."
Regina Barzilay,arXiv:1705.09037,https://arxiv.org/abs/1705.09037,"Abstract:  The design of neural architectures for structured objects is typically guided
by experimental insights rather than a formal process. In this work, we appeal
to kernels over combinatorial structures, such as sequences and graphs, to
derive appropriate neural operations. We introduce a class of deep recurrent
neural operations and formally characterize their associated kernel spaces. Our
recurrent modules compare the input to virtual reference objects (cf. filters
in CNN) via the kernels. Similar to traditional neural operations, these
reference objects are parameterized and directly optimized in end-to-end
training. We empirically evaluate the proposed class of neural architectures on
standard applications such as language modeling and molecular graph regression,
achieving state-of-the-art results across these applications."
Regina Barzilay,arXiv:1702.07015,https://arxiv.org/abs/1702.07015,"Abstract:  This paper focuses on unsupervised modeling of morphological families,
collectively comprising a forest over the language vocabulary. This formulation
enables us to capture edgewise properties reflecting single-step morphological
derivations, along with global distributional properties of the entire forest.
These global properties constrain the size of the affix set and encourage
formation of tight morphological families. The resulting objective is solved
using Integer Linear Programming (ILP) paired with contrastive estimation. We
train the model by alternating between optimizing the local log-linear model
and the global ILP objective. We evaluate our system on three tasks: root
detection, clustering of morphological families and segmentation. Our
experiments demonstrate that our model yields consistent gains in all three
tasks compared with the best published results."
Regina Barzilay,arXiv:1701.00188,https://arxiv.org/abs/1701.00188,"Abstract:  We introduce a neural method for transfer learning between two (source and
target) classification tasks or aspects over the same domain. Rather than
training on target labels, we use a few keywords pertaining to source and
target aspects indicating sentence relevance instead of document class labels.
Documents are encoded by learning to embed and softly select relevant sentences
in an aspect-dependent manner. A shared classifier is trained on the source
encoded documents and labels, and applied to target encoded documents. We
ensure transfer through aspect-adversarial training so that encoded documents
are, as sets, aspect-invariant. Experimental results demonstrate that our
approach outperforms different baselines and model variants on two datasets,
yielding an improvement of 27% on a pathology dataset and 5% on a review
dataset."
Regina Barzilay,arXiv:1608.03000,https://arxiv.org/abs/1608.03000,"Abstract:  This paper explores the task of translating natural language queries into
regular expressions which embody their meaning. In contrast to prior work, the
proposed neural model does not utilize domain-specific crafting, learning to
translate directly from a parallel corpus. To fully explore the potential of
neural models, we propose a methodology for collecting a large corpus of
regular expression, natural language pairs. Our resulting model achieves a
performance gain of 19.6% over previous state-of-the-art models."
Regina Barzilay,arXiv:1607.02902,https://arxiv.org/abs/1607.02902,"Abstract:  We present a novel technique for automatic program correction in MOOCs,
capable of fixing both syntactic and semantic errors without manual, problem
specific correction strategies. Given an incorrect student program, it
generates candidate programs from a distribution of likely corrections, and
checks each candidate for correctness against a test suite.
The key observation is that in MOOCs many programs share similar code
fragments, and the seq2seq neural network model, used in the natural-language
processing task of machine translation, can be modified and trained to recover
these fragments.
Experiment shows our scheme can correct 29% of all incorrect submissions and
out-performs state of the art approach which requires manual, problem specific
correction strategies."
Regina Barzilay,arXiv:1606.04155,https://arxiv.org/abs/1606.04155,"Abstract:  Prediction without justification has limited applicability. As a remedy, we
learn to extract pieces of input text as justifications -- rationales -- that
are tailored to be short and coherent, yet sufficient for making the same
prediction. Our approach combines two modular components, generator and
encoder, which are trained to operate well together. The generator specifies a
distribution over text fragments as candidate rationales and these are passed
through the encoder for prediction. Rationales are never given during training.
Instead, the model is regularized by desiderata for rationales. We evaluate the
approach on multi-aspect sentiment analysis against manually annotated test
cases. Our approach outperforms attention-based baseline by a significant
margin. We also successfully illustrate the method on the question retrieval
task."
Regina Barzilay,arXiv:1603.07954,https://arxiv.org/abs/1603.07954,"Abstract:  Most successful information extraction systems operate with access to a large
collection of documents. In this work, we explore the task of acquiring and
incorporating external evidence to improve extraction accuracy in domains where
the amount of training data is scarce. This process entails issuing search
queries, extraction from new sources and reconciliation of extracted values,
which are repeated until sufficient evidence is collected. We approach the
problem using a reinforcement learning framework where our model learns to
select optimal actions based on contextual information. We employ a deep
Q-network, trained to optimize a reward function that reflects extraction
accuracy while penalizing extra effort. Our experiments on two databases -- of
shooting incidents, and food adulteration cases -- demonstrate that our system
significantly outperforms traditional extractors and a competitive
meta-classifier baseline."
Regina Barzilay,arXiv:1512.05726,https://arxiv.org/abs/1512.05726,"Abstract:  Question answering forums are rapidly growing in size with no effective
automated ability to refer to and reuse answers already available for previous
posted questions. In this paper, we develop a methodology for finding
semantically related questions. The task is difficult since 1) key pieces of
information are often buried in extraneous details in the question body and 2)
available annotations on similar questions are scarce and fragmented. We design
a recurrent and convolutional model (gated convolution) to effectively map
questions to their semantic representations. The models are pre-trained within
an encoder-decoder framework (from body to title) on the basis of the entire
raw corpus, and fine-tuned discriminatively from limited annotations. Our
evaluation demonstrates that our model yields substantial gains over a standard
IR baseline and various neural network architectures (including CNNs, LSTMs and
GRUs)."
Regina Barzilay,arXiv:1508.04112,https://arxiv.org/abs/1508.04112,"Abstract:  The success of deep learning often derives from well-chosen operational
building blocks. In this work, we revise the temporal convolution operation in
CNNs to better adapt it to text processing. Instead of concatenating word
representations, we appeal to tensor algebra and use low-rank n-gram tensors to
directly exploit interactions between words already at the convolution stage.
Moreover, we extend the n-gram convolution to non-consecutive words to
recognize patterns with intervening words. Through a combination of low-rank
tensors, and pattern weighting, we can efficiently evaluate the resulting
convolution operation via dynamic programming. We test the resulting
architecture on standard sentiment classification and news categorization
tasks. Our model achieves state-of-the-art performance both in terms of
accuracy and training speed. For instance, we obtain 51.2% accuracy on the
fine-grained sentiment classification task."
Regina Barzilay,arXiv:1506.08941,https://arxiv.org/abs/1506.08941,"Abstract:  In this paper, we consider the task of learning control policies for
text-based games. In these games, all interactions in the virtual world are
through text and the underlying state is not observed. The resulting language
barrier makes such environments challenging for automatic game players. We
employ a deep reinforcement learning framework to jointly learn state
representations and action policies using game rewards as feedback. This
framework enables us to map text descriptions into vector representations that
capture the semantics of the game states. We evaluate our approach on two game
worlds, comparing against baselines using bag-of-words and bag-of-bigrams for
state representations. Our algorithm outperforms the baselines on both worlds
demonstrating the importance of learning expressive representations."
Regina Barzilay,arXiv:1503.02335,https://arxiv.org/abs/1503.02335,"Abstract:  Most state-of-the-art systems today produce morphological analysis based only
on orthographic patterns. In contrast, we propose a model for unsupervised
morphological analysis that integrates orthographic and semantic views of
words. We model word formation in terms of morphological chains, from base
words to the observed words, breaking the chains into parent-child relations.
We use log-linear models with morpheme and word-level features to predict
possible parents, including their modifications, for each word. The limited set
of candidate parents for each word render contrastive estimation feasible. Our
model consistently matches or outperforms five state-of-the-art systems on
Arabic, English and Turkish."
Regina Barzilay,arXiv:1401.6422,https://arxiv.org/abs/1401.6422,"Abstract:  We present a model for aggregation of product review snippets by joint aspect
identification and sentiment analysis. Our model simultaneously identifies an
underlying set of ratable aspects presented in the reviews of a product (e.g.,
sushi and miso for a Japanese restaurant) and determines the corresponding
sentiment of each aspect. This approach directly enables discovery of
highly-rated or inconsistent aspects of a product. Our generative model admits
an efficient variational mean-field inference algorithm. It is also easily
extensible, and we describe several modifications and their effects on model
structure and inference. We test our model on two tasks, joint aspect
identification and sentiment analysis on a set of Yelp reviews and aspect
identification alone on a set of medical summaries. We evaluate the performance
of the model on aspect identification, sentiment analysis, and per-word
labeling accuracy. We demonstrate that our model outperforms applicable
baselines by a considerable margin, yielding up to 32% relative error reduction
on aspect identification and up to 20% relative error reduction on sentiment
analysis."
Regina Barzilay,arXiv:1401.5695,https://arxiv.org/abs/1401.5695,"Abstract:  We demonstrate the effectiveness of multilingual learning for unsupervised
part-of-speech tagging. The central assumption of our work is that by combining
cues from multiple languages, the structure of each becomes more apparent. We
consider two ways of applying this intuition to the problem of unsupervised
part-of-speech tagging: a model that directly merges tag structures for a pair
of languages into a single sequence and a second model which instead
incorporates multilingual context using latent variables. Both approaches are
formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo
sampling techniques for inference. Our results demonstrate that by
incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily
as the number of available languages increases."
Regina Barzilay,arXiv:1401.5390,https://arxiv.org/abs/1401.5390,"Abstract:  Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization."
Regina Barzilay,arXiv:1401.3488,https://arxiv.org/abs/1401.3488,"Abstract:  We present a novel Bayesian topic model for learning discourse-level document
structure. Our model leverages insights from discourse theory to constrain
latent topic assignments in a way that reflects the underlying organization of
document topics. We propose a global model in which both topic selection and
ordering are biased to be similar across a collection of related documents. We
show that this space of orderings can be effectively represented using a
distribution over permutations called the Generalized Mallows Model. We apply
our method to three complementary discourse-level tasks: cross-document
alignment, document segmentation, and information ordering. Our experiments
show that incorporating our permutation-based model in these applications
yields substantial improvements in performance over previously proposed
methods."
Regina Barzilay,arXiv:1401.3457,https://arxiv.org/abs/1401.3457,"Abstract:  This paper presents a new method for inferring the semantic properties of
documents by leveraging free-text keyphrase annotations. Such annotations are
becoming increasingly abundant due to the recent dramatic growth in
semi-structured, user-generated online content. One especially relevant domain
is product reviews, which are often annotated by their authors with pros/cons
keyphrases such as a real bargain or good value. These annotations are
representative of the underlying semantic properties; however, unlike expert
annotations, they are noisy: lay authors may use different labels to denote the
same property, and some labels may be missing. To learn using such noisy
annotations, we find a hidden paraphrase structure which clusters the
keyphrases. The paraphrase structure is linked with a latent topic model of the
review texts, enabling the system to predict the properties of unannotated
documents and to effectively aggregate the semantic properties of multiple
reviews. Our approach is implemented as a hierarchical Bayesian model with
joint inference. We find that joint inference increases the robustness of the
keyphrase clustering and encourages the latent topics to correlate with
semantically meaningful properties. Multiple evaluations demonstrate that our
model substantially outperforms alternative approaches for summarizing single
and multiple documents into a set of semantically salient keyphrases."
Regina Barzilay,arXiv:cs/0405039,https://arxiv.org/abs/cs/0405039,"Abstract:  We consider the problem of modeling the content structure of texts within a
specific domain, in terms of the topics the texts address and the order in
which these topics appear. We first present an effective knowledge-lean method
for learning content models from un-annotated documents, utilizing a novel
adaptation of algorithms for Hidden Markov Models. We then apply our method to
two complementary tasks: information ordering and extractive summarization. Our
experiments show that incorporating content models in these applications yields
substantial improvement over previously-proposed methods."
Regina Barzilay,arXiv:cs/0304006,https://arxiv.org/abs/cs/0304006,"Abstract:  We address the text-to-text generation problem of sentence-level paraphrasing
-- a phenomenon distinct from and more difficult than word- or phrase-level
paraphrasing. Our approach applies multiple-sequence alignment to sentences
gathered from unannotated comparable corpora: it learns a set of paraphrasing
patterns represented by word lattice pairs and automatically determines how to
apply these patterns to rewrite new sentences. The results of our evaluation
experiments show that the system derives accurate paraphrases, outperforming
baseline systems."
Regina Barzilay,arXiv:cs/0205065,https://arxiv.org/abs/cs/0205065,"Abstract:  An important component of any generation system is the mapping dictionary, a
lexicon of elementary semantic expressions and corresponding natural language
realizations. Typically, labor-intensive knowledge-based methods are used to
construct the dictionary. We instead propose to acquire it automatically via a
novel multiple-pass algorithm employing multiple-sequence alignment, a
technique commonly used in bioinformatics. Crucially, our method leverages
latent information contained in multi-parallel corpora -- datasets that supply
several verbalizations of the corresponding semantics rather than just one.
We used our techniques to generate natural language versions of
computer-generated mathematical proofs, with good results on both a
per-component and overall-output basis. For example, in evaluations involving a
dozen human judges, our system produced output whose readability and
faithfulness to the semantic input rivaled that of a traditional generation
system."
Karl Berggren,arXiv:1901.09702,https://arxiv.org/abs/1901.09702,"Abstract:  Interaction-free measurement (IFM) has been proposed as a means of
high-resolution, low-damage imaging of radiation-sensitive samples, such as
biomolecules and proteins. The basic setup for IFM is a Mach-Zehnder
interferometer, and recent progress in nanofabricated electron diffraction
gratings has made it possible to incorporate a Mach-Zehnder interferometer in a
transmission-electron microscope (TEM). Therefore, the limits of performance of
IFM with such an interferometer and a shot-noise limited electron source (such
as that in a TEM) are of interest. In this work, we compared the error
probability and sample damage for ideal IFM and classical imaging schemes,
through theoretical analysis and numerical simulation. We considered a sample
that is either completely transparent or completely opaque at each pixel. In
our analysis, we also evaluated the impact of an additional detector for
scattered electrons. The additional detector resulted in reduction of error by
up to an order of magnitude, for both IFM and classical schemes. We also
investigated a sample re-illumination scheme based on updating priors after
each round of illumination and found that this scheme further reduced error by
a factor of two. Implementation of these methods is likely achievable with
existing instrumentation and would result in improved resolution in low-dose
electron microscopy."
Karl Berggren,arXiv:1901.03988,https://arxiv.org/abs/1901.03988,"Abstract:  Local, bulk response functions, e.g permittivity, and the macroscopic Maxwell
equations completely specify the classical electromagnetic problem, which
features only wavelength $\lambda$ and geometric scales. The above neglect of
intrinsic electronic length scales $L_{\text{e}}$ leads to an eventual
breakdown in the nanoscopic limit. Here, we present a general theoretical and
experimental framework for treating nanoscale electromagnetic phenomena. The
framework features surface-response functions---known as the Feibelman
$d$-parameters---which reintroduce the missing electronic length scales. As a
part of our framework, we establish an experimental procedure to measure these
complex, dispersive surface response functions, enabled by quasi-normal-mode
perturbation theory and observations of pronounced nonclassical
effects---spectral shifts in excess of 30% and the breakdown of Kreibig-like
broadening---in a quintessential multiscale architecture: film-coupled
nanoresonators, with feature-sizes comparable to both $L_{\text{e}}$ and
$\lambda$."
Karl Berggren,arXiv:1812.05559,https://arxiv.org/abs/1812.05559,"Abstract:  We present the use of a commercially available fixed-angle multi-wavelength
ellipsometer for quickly measuring the thickness of NbN thin films for the
fabrication and performance improvement of superconducting nanowire single
photon detectors. The process can determine the optical constants of absorbing
thin films, removing the need for inaccurate approximations. The tool can be
used to observe oxidation growth and allows thickness measurements to be
integrated into the characterization of various fabrication processes."
Karl Berggren,arXiv:1811.05192,https://arxiv.org/abs/1811.05192,"Abstract:  The method of negative-tone-PMMA electron-beam lithography is investigated to
improve the performance of nanowire-based superconducting detectors. Using this
approach, the superconducting nanowire single-photon detectors (SNSPDs) have
been fabricated from thick 5-nm NbN film sputtered at the room temperature. To
investigate the impact of this process, SNSPDs were prepared by positive-tone
and negative-tone-PMMA lithography, and their electrical and photodetection
characteristics at 4.2 K were compared. The SNSPDs made by negative-tone-PMMA
lithography show higher critical-current density and higher photon count rate
at various wavelengths. Our results suggest a higher negative-tone-PMMA
technology may be preferable to the standard positive-tone-PMMA lithography for
this application."
Karl Berggren,arXiv:1811.03991,https://arxiv.org/abs/1811.03991,"Abstract:  Conventional readout of a superconducting nanowire single-photon detector
(SNSPD) sets an upper bound on the output voltage to be the product of the bias
current and the load impedance, $I_\mathrm{B}\times Z_\mathrm{load}$, where
$Z_\mathrm{load}$ is limited to 50 $\Omega$ in standard r.f. electronics. Here,
we break this limit by interfacing the 50 $\Omega$ load and the SNSPD using an
integrated superconducting transmission line taper. The taper is a transformer
that effectively loads the SNSPD with high impedance without latching. It
increases the amplitude of the detector output while preserving the fast rising
edge. Using a taper with a starting width of 500 nm, we experimentally observed
a 3.6$\times$ higher pulse amplitude, 3.7$\times$ faster slew rate, and 25.1 ps
smaller timing jitter. The results match our numerical simulation, which
incorporates both the hotspot dynamics in the SNSPD and the distributed nature
in the transmission line taper. The taper studied here may become a useful tool
to interface high-impedance superconducting nanowire devices to conventional
low-impedance circuits."
Karl Berggren,arXiv:1810.09542,https://arxiv.org/abs/1810.09542,"Abstract:  The basis for superconducting electronics can broadly be divided between two
technologies: the Josephson junction and the superconducting nanowire. While
the Josephson junction (JJ) remains the dominant technology due to its high
speed and low power dissipation, recently proposed nanowire devices offer
improvements such as gain, high fanout, and compatibility with CMOS circuits.
Despite these benefits, nanowire-based electronics have largely been limited to
binary operations, with devices switching between the superconducting state and
a high-impedance resistive state dominated by uncontrolled hotspot dynamics.
Unlike the JJ, they cannot increment an output through successive switching,
and their operation speeds are limited by their slow thermal reset times. Thus,
there is a need for an intermediate device with the interfacing capabilities of
a nanowire but a faster, moderated response allowing for modulation of the
output. Here, we present a nanowire device based on controlled fluxon
transport. We show that the device is capable of responding proportionally to
the strength of its input, unlike other nanowire technologies. The device can
be operated to produce a multilevel output with distinguishable states, which
can be tuned by circuit parameters. Agreement between experimental results and
electrothermal circuit simulations demonstrates that the device is classical
and may be readily engineered for applications including use as a multilevel
memory."
Karl Berggren,arXiv:1808.03363,https://arxiv.org/abs/1808.03363,"Abstract:  Semi-transparent mirrors are standard elements in light optics for splitting
light beams or creating two versions of the same image. Such mirrors do not
exist in electron optics, although they could be beneficial in existing
techniques such as electron interferometry and holography and enable novel
electron imaging and spectroscopy techniques. We propose a design for an
electron beam splitter using the concept of quantum interaction-free
measurement (IFM). The design combines an electron resonator with a weak phase
grating. Fast switching gates allow electrons to enter and exit the resonator.
While in the resonator, the phase grating transfers intensity from the direct
beam into one of the weakly diffracted beams at each pass. To make the beam
splitter an efficient two-port splitter, the intensity in all other diffracted
beams is blocked by an aperture. The IFM principle minimizes the loss of total
intensity by this aperture. We use a scattering matrix method to analyze the
performance of the beam splitter, including the effects of inelastic scattering
in the phase grating. This design can be generalized to beam splitters for not
only electrons, but also photons, neutrons, atoms, and other quantum mechanical
systems."
Karl Berggren,arXiv:1805.05601,https://arxiv.org/abs/1805.05601,"Abstract:  To analyze the switching dynamics and output performance of a superconducting
nanowire single photon detector (SNSPD), the nanowire is usually modelled as an
inductor in series with a time-varying resistor induced by absorption of a
photon. Our recent experimental results show that, due to the effect of kinetic
inductance, for a SNSPD made of a nanowire of sufficient length, its geometry
length can be comparable to or even longer than the effective wavelength of
frequencies contained in the output pulse. In other words, a superconducting
nanowire can behave as a distributed transmission line so that the readout
pulse depends on the photon detection location and the transmission line
properties of the nanowire. Here, we develop a distributed model for a
superconducting nanowire and apply it to simulate the output performance of a
long nanowire designed into a coplanar waveguide. We compare this coplanar
waveguide geometry to a conventional meander nanowire geometry. The simulation
results agree well with our experimental observations. With this distributed
model, we discussed the importance of microwave design of a nanowire and how
impedance matching can affect the output pulse shape. We also discuss how the
distributed model affects the growth and decay of the photon-triggered
resistive hotspot."
Karl Berggren,arXiv:1805.00130,https://arxiv.org/abs/1805.00130,"Abstract:  We analyze the origin of the intrinsic timing jitter in superconducting
nanowire single photon detectors (SNSPDs) in terms of fluctuations in the
latency of the detector response, which is determined by the microscopic
physics of the photon detection process. We demonstrate that fluctuations in
the physical parameters which determine the latency give rise to the intrinsic
timing jitter. We develop a general description of latency by introducing the
explicit time dependence of the internal detection efficiency. By considering
the dynamic Fano fluctuations together with static spatial inhomogeneities, we
study the details of the connection between latency and timing jitter. We
develop both a simple phenomenological model and a more general microscopic
model of detector latency and timing jitter based on the solution of the
generalized time-dependent Ginzburg-Landau equations for the 1D hotbelt
geometry. While the analytical model is sufficient for qualitative
interpretation of recent data, the general approach establishes the framework
for a quantitative analysis of detector latency and the fundamental limits of
intrinsic timing jitter. These theoretical advances can be used to interpret
the results of recent experiments measuring the dependence of detection latency
and timing jitter on photon energy to the few-picosecond level."
Karl Berggren,arXiv:1803.11306,https://arxiv.org/abs/1803.11306,"Abstract:  Report of the first workshop to identify approaches and techniques in the
domain of quantum sensing that can be utilized by future High Energy Physics
applications to further the scientific goals of High Energy Physics."
Karl Berggren,arXiv:1711.10546,https://arxiv.org/abs/1711.10546,"Abstract:  Coincidence detection of single photons is crucial in numerous quantum
technologies and usually requires multiple time-resolved single-photon
detectors. However, the electronic readout becomes a major challenge when the
measurement basis scales to large numbers of spatial modes. Here, we address
this problem by introducing a two-terminal coincidence detector that enables
scalable readout of an array of detector segments based on superconducting
nanowire microstrip transmission line. Exploiting timing logic, we demonstrate
a 16-element detector that resolves all 136 possible single-photon and
two-photon coincidence events. We further explore the pulse shapes of the
detector output and resolve up to four-photon coincidence events in a 4-element
device, giving the detector photon-number-resolving capability. This new
detector architecture and operating scheme will be particularly useful for
multi-photon coincidence detection in large-scale photonic integrated circuits."
Karl Berggren,arXiv:1711.08290,https://arxiv.org/abs/1711.08290,"Abstract:  A superconducting loop stores persistent current without any ohmic loss,
making it an ideal platform for energy efficient memories. Conventional
superconducting memories use an architecture based on Josephson junctions (JJs)
and have demonstrated access times less than 10 ps and power dissipation as low
as $10^{-19}$ J. However, their scalability has been slow to develop due to the
challenges in reducing the dimensions of JJs and minimizing the area of the
superconducting loops. In addition to the memory itself, complex readout
circuits require additional JJs and inductors for coupling signals, increasing
the overall area. Here, we have demonstrated a superconducting memory based
solely on lithographic nanowires. The small dimensions of the nanowire ensure
that the device can be fabricated in a dense area in multiple layers, while the
high kinetic inductance makes the loop essentially independent of geometric
inductance, allowing it to be scaled down without sacrificing performance. The
memory is operated by a group of nanowire cryotrons patterned alongside the
storage loop, enabling us to reduce the entire memory cell to 3 {\mu}m $\times
$ 7 {\mu}m in our proof-of-concept device. In this work we present the
operation principles of a superconducting nanowire memory (nMem) and
characterize its bit error rate, speed, and power dissipation."
Karl Berggren,arXiv:1711.01305,https://arxiv.org/abs/1711.01305,"Abstract:  We present the performance of a superconducting nanowire that can be operated
in two detection modes: i) as a kinetic inductance detector (KID) or ii) as a
single-photon detector (SPD). Two superconducting nanowires developed for use
as single-photon detectors (SNSPDs) are embedded as the inductive (L) component
in resonant inductor/capacitor (LC) circuits coupled to a microwave
transmission line. The capacitors are low loss commercial chip capacitors and
limit the internal quality factor of the resonators to approximately $Q_i =
170$. The resonator quality factor, $Q_r \simeq 23$, is dominated by the
coupling to the feedline and limits the detection bandwidth to on the order of
1MHz. When operated in KID mode, the detectors are AC biased with tones at
their resonant frequencies of 45.85 and 91.81MHz. In the low-bias, standard KID
mode, a single photon produces a hot spot that does not turn an entire section
of the line normal but only increases the kinetic inductance. In the high-bias,
critical KID mode, a photon event turns a section of the line normal and the
resonance is destroyed until the normal region is dissipated. When operated as
an SPD in Geiger mode, the resonators are DC biased through cryogenic bias tees
and each photon produces a sharp voltage step followed by a ringdown signal at
the resonant frequency of the detector which is converted to a standard pulse
with an envelop detector. We show that AC biasing in the critical KID mode is
inferior to the sensitivity achieved in DC-biased SPD mode due to the small
fraction of time spent near the critical current with an AC bias."
Karl Berggren,arXiv:1710.05358,https://arxiv.org/abs/1710.05358,"Abstract:  Recent advances in the fabrication of nanostructures and nanoscale features
in metasurfaces offer a new prospect for generating visible, light emission
from low energy electrons. In this paper, we present the experimental
observation of visible light emission from low-energy free electrons
interacting with nanoscale periodic surfaces through the Smith-Purcell (SP)
effect. SP radiation is emitted when electrons pass in close proximity over a
periodic structure, inducing collective charge motion or dipole excitations
near the surface, thereby giving rise to electromagnetic radiation. We
demonstrate a controlled emission of SP light from nanoscale gold gratings with
periodicity as small as 50 nm, enabling the observation of visible SP radiation
by low energy electrons (1.5 to 6 keV), an order of magnitude lower than
previously reported. We study the emission wavelength and intensity dependence
on the grating pitch and electron energy, showing agreement between experiment
and theory. Further reduction of structure periodicity should enable the
production of SP-based devices that operate with even slower electrons that
allow an even smaller footprint and facilitate the investigation of quantum
effects for light generation in nanoscale devices. A tunable light source
integrated in an electron microscope would enable the development of novel
electron-optical correlated spectroscopic techniques, with additional
applications ranging from biological imaging to solid-state lighting."
Karl Berggren,arXiv:1709.06598,https://arxiv.org/abs/1709.06598,"Abstract:  Many superconducting technologies such as rapid single flux quantum computing
(RSFQ) and superconducting quantum interference devices (SQUIDs) rely on the
modulation of nonlinear dynamics in Josephson junctions for functionality. More
recently, however, superconducting devices have been developed based on the
switching and thermal heating of nanowires for use in fields such as single
photon detection and digital logic. In this paper, we use resistive shunting to
control the nonlinear heating of a superconducting nanowire and compare the
resulting dynamics to those observed in Josephson junctions. We show that
interaction of the hotspot growth with the external shunt produces high
frequency relaxation oscillations with similar behavior as observed in
Josephson junctions due to their rapid time constants and ability to be
modulated by a weak periodic signal. In particular, we use a microwave drive to
pull and mix the oscillation frequency, resulting in phase locked features that
resemble the AC Josephson effect. New nanowire devices based on these
conclusions have promising applications in fields such as parametric
amplification and frequency multiplexing."
Karl Berggren,arXiv:1703.08034,https://arxiv.org/abs/1703.08034,"Abstract:  The lack of energy dissipation and abrupt electrical phase transition of
superconductors favorite them for nanoscale technologies, including radiation
detectors, and quantum technologies. Moreover, understanding the nanoscale
behavior of superconductivity is significant for revealing the onset of
collective-electron behavior in nature. Nevertheless, the limited number of
accessible superconductors restricts availability of the superconducting
properties, encumbering the realization of their potential. Superconducting
nanowire single photon detectors (SNSPDs) sense single-IR photons faster and
more efficient with respect to competing technologies. However, these
advantageous properties are material-dependent causing an undesirable
speed-efficiency payoff. Usually, SNSPDs based on granular materials are
faster, while those based on amorphous materials are more efficient. Here we
optimized ultrathin films of granular NbN on SiO2 and of amorphous W5Si3. We
showed that hybrid superconducting nanowire single photon detectors (SNSPDs)
made of 2-nm-thick W5Si3 films over 2-nm-thick NbN films exhibit advantageous
coexistence of timing (< 5-ns reset time and 52-ps timing jitter) and
efficiency (> 96% quantum efficiency) performance. We propose that the
governing mechanism of this hybridization is the presence of a dual
superconducting behavior: native superconductivity of each of the films and
superconductivity that is induced from the neighboring film via the proximity
effect. In addition to improvement in SNSPDs performance, our results suggest
that such hybridization can expand the range of available superconducting
properties, impacting nano-superconducting technologies. Lastly, this
hybridization may be used to tune the amorphous character of superconducting
films and to illuminate the elusive onset of collective-electron behavior near
the superconducting-to-insulating transition."
Karl Berggren,arXiv:1610.09349,https://arxiv.org/abs/1610.09349,"Abstract:  Integration with conventional electronics offers a straightforward and
economical approach to upgrading existing superconducting technologies, such as
scaling up superconducting detectors into large arrays and combining single
flux quantum (SFQ) digital circuits with semiconductor logic and memories.
However, direct output signals from superconducting devices (e.g., Josephson
junctions) are usually not compatible with the input requirements of
conventional devices (e.g., transistors). Here, we demonstrate the use of a
single three-terminal superconducting-nanowire device, called the nanocryotron
(nTron), as a digital comparator to combine SFQ circuits with mature
semiconductor circuits such as complementary metal oxide semiconductor (CMOS)
circuits. Since SFQ circuits can digitize output signals from general
superconducting devices and CMOS circuits can interface existing
CMOS-compatible electronics, our results demonstrate the feasibility of a
general architecture that uses an nTron as an interface to realize a
super-hybrid system consisting of superconducting detectors, superconducting
quantum electronics, CMOS logic and memories, and other conventional
electronics."
Karl Berggren,arXiv:1608.08616,https://arxiv.org/abs/1608.08616,"Abstract:  We report a self-aligned, monolithic electron interferometer, consisting of
two 45 nm thick silicon layers separated by 20 $\mu$m. This interferometer was
fabricated from a single crystal silicon cantilever on a transmission electron
microscope grid by gallium focused ion-beam milling. Using this interferometer,
we demonstrate beam path-separation, and obtain interference fringes in a
Mach-Zehnder geometry, in an unmodified 200 kV transmission electron
microscope. The fringes have a period of 0.32 nm, which corresponds to the
$\left[\bar{1}\bar{1}1\right]$ lattice planes of silicon, and a maximum
contrast of 15 %. This design can potentially be scaled to millimeter-scale,
and used in electron holography. It can also be applied to perform fundamental
physics experiments, such as interaction-free measurement with electrons."
Karl Berggren,arXiv:1607.06713,https://arxiv.org/abs/1607.06713,"Abstract:  Detection jitter quantifies variance introduced by the detector in the
determination of photon arrival time. It is a crucial performance parameter for
systems using superconducting nanowire single photon detectors (SNSPDs). In
this work, we have demonstrated that the detection timing jitter is limited in
part by the spatial variation of photon detection events along the length of
the wire. This distribution causes the generated electrical pulses to arrive at
the readout at varied times. We define this jitter source as geometric jitter
since it is related to the length and area of the SNSPD. To characterize the
geometric jitter, we have constructed a novel differential cryogenic readout
with less than 7 ps of electronic jitter that can amplify the pulses generated
from the two ends of an SNSPD. By differencing the measured arrival times of
the two electrical pulses, we were able to partially cancel out the difference
of the propagation times and thus reduce the uncertainty of the photon arrival
time. Our experimental data indicates that the variation of the differential
propagation time was a few ps for a 3 {\mu}m x 3 {\mu}m device while it
increased up to 50 ps for a 20 {\mu}m x 20 {\mu}m device. In a 20 {\mu}m x 20
{\mu}m large SNSPD, we achieved a 20% reduction in the overall detection timing
jitter for detecting telecom-wavelength photons by using the differential
cryogenic readout. The geometric jitter hypothesis was further confirmed by
studying jitter in devices that consisted of long wires with 1-{\mu}m-long
narrowed regions used for sensing photons."
Karl Berggren,arXiv:1606.01395,https://arxiv.org/abs/1606.01395,"Abstract:  We describe a superconducting three-terminal device that uses a simple
geometric effect known as current crowding to sense the flow of current and
actuate a readout signal. The device consists of a ""Y""-shaped current combiner,
with two currents (sense and bias) entering through the top arms of the ""Y"",
intersecting, and then exiting through the bottom leg of the ""Y""'. This
geometry--mixing two inputs at a sharp intersection point--takes its
inspiration from Y-shaped combiners in fluid flow systems, where variations in
the input pressures can produce at turbulence and mixing at the intersection.
When current is added to or removed from one of the arms (the sense arm), the
superconducting critical current in the other arm (the bias arm) is modulated.
The current in the sense arm can thus be determined by measuring the critical
current of the bias arm. The dependence of the bias critical current on the
sense current is possible because current crowding causes the sense current to
interact locally with the bias arm. Measurement of the critical current in the
bias arm does not break the superconducting state of the sense arm or of the
bottom leg, and thus the signal to be sensed is fully restored after the
measurement process. This device thus has potential for broad applicability
across superconducting technologies and materials."
Karl Berggren,arXiv:1605.08693,https://arxiv.org/abs/1605.08693,"Abstract:  Detecting spatial and temporal information of individual photons by using
single-photon-detector (SPD) arrays is critical to applications in
spectroscopy, communication, biological imaging, astronomical observation, and
quantum-information processing. Among the current SPDs1,detectors based on
superconducting nanowires have outstanding performance2, but are limited in
their ability to be integrated into large scale arrays due to the engineering
difficulty of high-bandwidth cryogenic electronic readout3-8. Here, we address
this problem by demonstrating a scalable single-photon imager using a single
continuous photon-sensitive superconducting nanowire microwave-plasmon
transmission line. By appropriately designing the nanowire's local
electromagnetic environment so that the nanowire guides microwave plasmons, the
propagating voltages signals generated by a photon-detection event were slowed
down to ~ 2% of the speed of light. As a result, the time difference between
arrivals of the signals at the two ends of the nanowire naturally encoded the
position and time of absorption of the photon. Thus, with only two readout
lines, we demonstrated that a 19.7-mm-long nanowire meandered across an area of
286 {\mu}m * 193 {\mu}m was capable of resolving ~590 effective pixels while
simultaneously recording the arrival times of photons with a temporal
resolution of 50 ps. The nanowire imager presents a scalable approach to
realizing high-resolution photon imaging in time and space."
Karl Berggren,arXiv:1602.06895,https://arxiv.org/abs/1602.06895,"Abstract:  We study the microwave impedance of extremely high aspect ratio (length/width
~ 5,000) superconducting niobium nitride nanowires. The nanowires are
fabricated in a compact meander geometry that is in series with the center
conductor of a 50 ohm coplanar waveguide transmission line. The transmission
coefficient of the sample is measured up to 20 GHz. At high frequency, a peak
in the transmission coefficient is seen. Numerical simulations show that this
is a half-wave resonance along the length of the nanowire, where the nanowire
acts as a high impedance, slow wave transmission line. This resonance sets the
upper frequency limit for these nanowires as inductive elements. Fitting
simulations to the measured resonance enables a precise determination of the
nanowire's complex sheet impedance at the resonance frequency. The real part is
a measure of dissipation, while the imaginary part is dominated by kinetic
inductance. We characterize the dependence of the sheet resistance and sheet
inductance on both temperature and current and compare the results to recent
theoretical predictions for disordered superconductors. These results can aid
in the understanding of high frequency devices based on superconducting
nanowires. They may also lead to the development of novel superconducting
devices such as ultra-compact resonators and slow-wave structures."
Karl Berggren,arXiv:1511.05786,https://arxiv.org/abs/1511.05786,"Abstract:  This paper describes the construction of a cryostat and an optical system
with a free-space coupling efficiency of 56.5% +/- 3.4% to a superconducting
nanowire single-photon detector (SNSPD) for infrared quantum communication and
spectrum analysis. A 1K pot decreases the base temperature to T = 1.7 K from
the 2.9 K reached by the cold head cooled by a pulse-tube cryocooler. The
minimum spot size coupled to the detector chip was 6.6 +/- 0.11 {\mu}m starting
from a fiber source at wavelength, {\lambda} = 1.55 {\mu}m. We demonstrated
efficient photon counting on a detector with an 8 x 7.3 {\mu}m^2 area. We
measured a dark count rate of 95 +/- 3.35 kcps and a system detection
efficiency of 1.64% +/- 0.13%. We explain the key steps that are required to
further improve the coupling efficiency."
Karl Berggren,arXiv:1510.05946,https://arxiv.org/abs/1510.05946,"Abstract:  One of the astounding consequences of quantum mechanics is that it allows the
detection of a target using an incident probe, with only a low probability of
interaction of the probe and the target. This 'quantum weirdness' could be
applied in the field of electron microscopy to generate images of
beam-sensitive specimens with substantially reduced damage to the specimen. A
reduction of beam-induced damage to specimens is especially of great importance
if it can enable imaging of biological specimens with atomic resolution.
Following a recent suggestion that interaction-free measurements are possible
with electrons, we now analyze the difficulties of actually building an atomic
resolution interaction-free electron microscope, or ""quantum electron
microscope"". A quantum electron microscope would require a number of unique
components not found in conventional transmission electron microscopes. These
components include a coherent electron-beam splitter or two-state-coupler, and
a resonator structure to allow each electron to interrogate the specimen
multiple times, thus supporting high success probabilities for interaction-free
detection of the specimen. Different system designs are presented here, which
are based on four different choices of two-state-couplers: a thin crystal, a
grating mirror, a standing light wave and an electro-dynamical pseudopotential.
Challenges for the detailed electron optical design are identified as future
directions for development. While it is concluded that it should be possible to
build an atomic resolution quantum electron microscope, we have also identified
a number of hurdles to the development of such a microscope and further
theoretical investigations that will be required to enable a complete
interpretation of the images produced by such a microscope."
Karl Berggren,arXiv:1508.01877,https://arxiv.org/abs/1508.01877,"Abstract:  Methods for patterning biomolecules on a substrate at the single molecule
level have been studied as a route to sensors with single-molecular sensitivity
or as a way to probe biological phenomena at the single-molecule level.
However, the arrangement and orientation of single biomolecules on substrates
has been less investigated. Here, we examined the arrangement and orientation
of two rod-like coiled-coil proteins, cortexillin and tropomyosin, around
patterned gold nanostructures. The high aspect ratio of the coiled coils made
it possible to study their orientations and to pursue a strategy of protein
orientation via two-point attachment. The proteins were anchored to the
surfaces using thiol groups, and the number of cysteine residues in tropomyosin
was varied to test how this variation affected the structure and arrangement of
the surface-attached proteins. Molecular dynamics studies were used to
interpret the observed positional distributions. Based on initial studies of
protein attachment to gold post structures, two 31-nm-long tropomyosin
molecules were aligned between the two sidewalls of a trench with a width of 68
nm. Because the approach presented in this study uses one of twenty natural
amino acids, this method provides a convenient way to pattern biomolecules on
substrates using standard chemistry."
Karl Berggren,arXiv:1503.07135,https://arxiv.org/abs/1503.07135,"Abstract:  We present an optical setup that can be used to characterize the thicknesses
of thin NbN films to screen samples for fabrication and to better model the
performance of the resulting superconducting nanowire single photon detectors.
The infrared transmissometer reported here is easy to use, gives results within
minutes and is non-destructive. Thus, the thickness measurement can be easily
integrated into the workflow of deposition and characterization. Comparison to
a similar visible-wavelength transmissometer is provided."
Karl Berggren,arXiv:1408.1124,https://arxiv.org/abs/1408.1124,"Abstract:  Superconducting nanowire avalanche single-photon detectors (SNAPs) with n
parallel nanowires are advantageous over single-nanowire detectors because
their output signal amplitude scales linearly with n. However, the SNAP
architecture has not been viably demonstrated for n > 4. To increase n for
larger signal amplification, we designed a multi-stage, successive-avalanche
architecture which used nanowires, connected via choke inductors in a
binary-tree layout. We demonstrated an avalanche detector with n = 8 parallel
nanowires and achieved eight-fold signal amplification, with a timing jitter of
54 ps."
Karl Berggren,arXiv:1407.5945,https://arxiv.org/abs/1407.5945,"Abstract:  Thin superconducting films form a unique platform for geometrically-confined,
strongly-interacting electrons. They allow an inherent competition between
disorder and superconductivity, which in turn enables the intriguing
superconducting-to-insulator transition and believed to facilitate the
comprehension of high-Tc superconductivity. Furthermore, understanding thin
film superconductivity is technologically essential e.g. for photo-detectors,
and quantum-computers. Consequently, the absence of an established universal
relationships between critical temperature ($T_c$), film thickness ($d$) and
sheet resistance ($R_s$) hinders both our understanding of the onset of the
superconductivity and the development of miniaturised superconducting devices.
We report that in thin films, superconductivity scales as $d^.$$T_c(R_s)$. We
demonstrated this scaling by analysing the data published over the past 46
years for different materials (and facilitated this database for further
analysis). Moreover, we experimentally confirmed the discovered scaling for NbN
films, quantified it with a power law, explored its possible origin and
demonstrated its usefulness for superconducting film-based devices."
Karl Berggren,arXiv:1405.4244,https://arxiv.org/abs/1405.4244,"Abstract:  Photonic integrated circuits (PICs) have emerged as a scalable platform for
complex quantum technologies using photonic and atomic systems. A central goal
has been to integrate photon-resolving detectors to reduce optical losses,
latency, and wiring complexity associated with off-chip detectors.
Superconducting nanowire single-photon detectors (SNSPDs) are particularly
attractive because of high detection efficiency, sub-50-ps timing jitter,
nanosecond-scale reset time, and sensitivity from the visible to the
mid-infrared spectrum. However, while single SNSPDs have been incorporated into
individual waveguides, the system efficiency of multiple SNSPDs in one photonic
circuit has been limited below 0.2% due to low device yield. Here we introduce
a micrometer-scale flip-chip process that enables scalable integration of
SNSPDs on a range of PICs. Ten low-jitter detectors were integrated on one PIC
with 100% device yield. With an average system efficiency beyond 10% for
multiple SNSPDs on one PIC, we demonstrate high-fidelity on-chip photon
correlation measurements of non-classical light."
Karl Berggren,arXiv:1403.6423,https://arxiv.org/abs/1403.6423,"Abstract:  In existing superconducting electronic systems, Josephson junctions play a
central role in processing and transmitting small-amplitude electrical signals.
However, Josephson-junction-based devices have a number of limitations
including: (1) sensitivity to magnetic fields, (2) limited gain, (3) inability
to drive large impedances, and (4) difficulty in controlling the junction
critical current (which depends sensitively on sub-Angstrom-scale thickness
variation of the tunneling barrier). Here we present a nanowire-based
superconducting electronic device, which we call the nanocryotron (nTron), that
does not rely on Josephson junctions and can be patterned from a single thin
film of superconducting material with conventional electron-beam lithography.
The nTron is a 3-terminal, T-shaped planar device with a gain of ~20 that is
capable of driving impedances of more than 100 k{\Omega}, and operates in
typical ambient magnetic fields at temperatures of 4.2K. The device uses a
localized, Joule-heated hotspot formed in the gate to modulate current flow in
a perpendicular superconducting channel. We have characterized the nTron,
matched it to a theoretical framework, and applied it both as a digital logic
element in a half-adder circuit, and as a digital amplifier for superconducting
nanowire single-photon detectors pulses. The nTron has immediate applications
in classical and quantum communications, photon sensing and astronomy, and its
performance characteristics make it compatible with existing superconducting
technologies. Furthermore, because the hotspot effect occurs in all known
superconductors, we expect the design to be extensible to other materials,
providing a path to digital logic, switching, and amplification in
high-temperature superconductors."
Karl Berggren,arXiv:1202.2835,https://arxiv.org/abs/1202.2835,"Abstract:  The optimal orientations are determined for polarized substrate side
illumination of three superconducting nanowire single-photon detector (SNSPD)
designs: (1) periodic niobium-nitride (NbN) stripes standing in air with
dimensions according to conventional SNSPDs, (2) same NbN patterns below
~quarter-wavelength hydrogensilsesquioxane-filled nano-cavity, (3) analogous
NbN patterns in HSQ nano-cavity closed by a thin gold reflector. Numerical
computation results have shown that the optical response and near-field
distribution vary significantly with polar-angle, fi, and these variations are
analogous across all azimuthal-angles, gamma, but are fundamentally different
in various device designs. Larger absorptance is available due to p-polarized
illumination of NbN patterns in P-structure configuration, while s-polarized
illumination results in higher absorptance in S-structure arrangement. As a
result of p-polarized illumination a global maximum appears on absorptance of
bare NbN pattern at polar angle corresponding to NbN-related ATIR; integration
with HSQ nano-cavity results in a global absorptance maximum at polar angle
corresponding to TIR at sapphire-air interface; while the highest absorptance
is observable at perpendicular incidence on P-structures aligned below gold
reflector covered HSQ nano-cavity. S-polarized light illumination results in a
global absorptance maximum at TIR on bare NbN patterns; the highest absorptance
is available below HSQ nano-cavity at polar angle corresponding to ATIR
phenomenon; while the benefit of gold reflector is large and polar angle
independent absorptance."
Karl Berggren,arXiv:1109.4881,https://arxiv.org/abs/1109.4881,"Abstract:  In this paper we calculate the critical currents in thin superconducting
strips with sharp right-angle turns, 180-degree turnarounds, and more
complicated geometries, where all the line widths are much smaller than the
Pearl length $\Lambda = 2 \lambda^2/d$. We define the critical current as the
current that reduces the Gibbs free-energy barrier to zero. We show that
current crowding, which occurs whenever the current rounds a sharp turn, tends
to reduce the critical current, but we also show that when the radius of
curvature is less than the coherence length this effect is partially
compensated by a radius-of-curvature effect. We propose several patterns with
rounded corners to avoid critical-current reduction due to current crowding.
These results are relevant to superconducting nanowire single-photon detectors,
where they suggest a means of improving the bias conditions and reducing dark
counts. These results also have relevance to normal-metal nanocircuits, as
these patterns can reduce the electrical resistance, electromigration, and hot
spots caused by nonuniform heating."
Karl Berggren,arXiv:1106.3591,https://arxiv.org/abs/1106.3591,"Abstract:  A novel finite-element method for calculating the illumination-dependence of
absorption in three-dimensional nanostructures is presented based on the RF
module of the COMSOL software package. This method is capable of numerically
determining the optical response and near-field distribution of sub-wavelength
periodic structures as a function of illumination orientations specified by
polar angle, fi, and azimuthal angle, gamma. The method was applied to
determine the illumination-angle-dependent absorptance in cavity-based
superconducting-nanowire single-photon detector (SNSPD) designs.
Niobium-nitride stripes based on dimensions of conventional SNSPDs and
integrated with ~ quarter-wavelength hydrogensilsesquioxane-filled nano-optical
cavities and covered by a thin gold film acting as a reflector were illuminated
from below by p-polarized light in this study. The numerical results were
compared to results from complementary transfer-matrix-method calculations on
composite layers made of analogous film-stacks. This comparison helped to
uncover the optical phenomena contributing to the appearance of extrema in the
optical response. This paper presents an approach to optimizing the absorptance
of different sensing and detecting devices via simultaneous numerical
optimization of the polar and azimuthal illumination angles."
Karl Berggren,arXiv:1012.3964,https://arxiv.org/abs/1012.3964,"Abstract:  We developed an electro thermal model of NbN superconducting nanowire
avalanche photodetectors (SNAPs) on sapphire substrates. SNAPs are single
photon detectors consisting of the parallel connection of N superconducting
nanowires. We extrapolated the physical constants of the model from
experimental data and we simulated the time evolution of the device resistance,
temperature and current by solving two coupled electrical and thermal
differential equations describing the nanowires. The predictions of the model
were in good quantitative agreement with the experimental results."
Karl Berggren,arXiv:1010.6108,https://arxiv.org/abs/1010.6108,"Abstract:  We fabricate superconducting ion traps with niobium and niobium nitride and
trap single 88Sr ions at cryogenic temperatures. The superconducting transition
is verified and characterized by measuring the resistance and critical current
using a 4-wire measurement on the trap structure, and observing change in the
rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz
at 6 K and shows no significant change across the superconducting transition,
suggesting that anomalous heating is primarily caused by noise sources on the
surface. This demonstration of superconducting ion traps opens up possibilities
for integrating trapped ions and molecular ions with superconducting devices."
Karl Berggren,arXiv:0812.4670,https://arxiv.org/abs/0812.4670,"Abstract:  Transitions in an artificial atom, driven non-adiabatically through an
energy-level avoided crossing, can be controlled by carefully engineering the
driving protocol. We have driven a superconducting persistent-current qubit
with a large-amplitude, radio-frequency field. By applying a bi-harmonic
waveform generated by a digital source, we demonstrate a mapping between the
amplitude and phase of the harmonics produced at the source and those received
by the device. This allows us to image the actual waveform at the device. This
information is used to engineer a desired time dependence, as confirmed by
detailed comparison with simulation."
Karl Berggren,arXiv:0812.0290,https://arxiv.org/abs/0812.0290,"Abstract:  We investigate the role of electrothermal feedback in the operation of
superconducting nanowire single-photon detectors (SNSPDs). It is found that the
desired mode of operation for SNSPDs is only achieved if this feedback is
unstable, which happens naturally through the slow electrical response
associated with their relatively large kinetic inductance. If this response is
sped up in an effort to increase the device count rate, the electrothermal
feedback becomes stable and results in an effect known as latching, where the
device is locked in a resistive state and can no longer detect photons. We
present a set of experiments which elucidate this effect, and a simple model
which quantitatively explains the results."
Karl Berggren,arXiv:0806.3194,https://arxiv.org/abs/0806.3194,"Abstract:  We measured the optical absorptance of superconducting nanowire single photon
detectors. We found that 200-nm-pitch, 50%-fill-factor devices had an average
absorptance of 21% for normally-incident front-illumination of
1.55-um-wavelength light polarized parallel to the nanowires, and only 10% for
perpendicularly-polarized light. We also measured devices with lower
fill-factors and narrower wires that were five times more sensitive to
parallel-polarized photons than perpendicular-polarized photons. We developed a
numerical model that predicts the absorptance of our structures. We also used
our measurements, coupled with measurements of device detection efficiencies,
to determine the probability of photon detection after an absorption event. We
found that, remarkably, absorbed parallel-polarized photons were more likely to
result in detection events than perpendicular-polarized photons, and we present
a hypothesis that qualitatively explains this result. Finally, we also
determined the enhancement of device detection efficiency and absorptance due
to the inclusion of an integrated optical cavity over a range of wavelengths
(700-1700 nm) on a number of devices, and found good agreement with our
numerical model."
Karl Berggren,arXiv:0805.2397,https://arxiv.org/abs/0805.2397,"Abstract:  A photon-number-resolving detector based on a four-element superconducting
nanowire single photon detector is demonstrated to have sub-30-ps resolution in
measuring the arrival time of individual photons. This detector can be used to
characterize the photon statistics of non-pulsed light sources and to mitigate
dead-time effects in high-speed photon counting applications. Furthermore, a
25% system detection efficiency at 1550 nm was demonstrated, making the
detector useful for both low-flux source characterization and high-speed
photon-counting and quantum communication applications. The design, fabrication
and testing of this detector are described, and a comparison between the
measured and theoretical performance is presented."
Karl Berggren,arXiv:0805.1552,https://arxiv.org/abs/0805.1552,"Abstract:  The energy-level structure of a quantum system plays a fundamental role in
determining its behavior and manifests itself in a discrete absorption and
emission spectrum. Conventionally, spectra are probed via frequency
spectroscopy whereby the frequency \nu of a harmonic driving field is varied to
fulfill the conditions \Delta E = h \nu, where the driving field is resonant
with the level separation \Delta E (h is Planck's constant). Although this
technique has been successfully employed in a variety of physical systems,
including natural and artificial atoms and molecules, its application is not
universally straightforward, and becomes extremely challenging for frequencies
in the range of 10's and 100's of gigahertz. Here we demonstrate an alternative
approach, whereby a harmonic driving field sweeps the atom through its
energy-level avoided crossings at a fixed frequency, surmounting many of the
limitations of the conventional approach. Spectroscopic information is obtained
from the amplitude dependence of the system response. The resulting
``spectroscopy diamonds'' contain interference patterns and population
inversion that serve as a fingerprint of the atom's spectrum. By analyzing
these features, we determine the energy spectrum of a manifold of states with
energies from 0.01 to 120 GHz \times h in a superconducting artificial atom,
using a driving frequency near 0.1 GHz. This approach provides a means to
manipulate and characterize systems over a broad bandwidth, using only a single
driving frequency that may be orders of magnitude smaller than the energy
scales being probed."
Karl Berggren,arXiv:0711.5021,https://arxiv.org/abs/0711.5021,"Abstract:  Novel optical phenomena, including electromagnetically induced transparency,
slow light, superluminal light propagation, have recently been demonstrated in
diverse physical implementations. These phenomena are challenging to realize in
practical systems because they require quantum coherence as well as careful
preparation and control of prescribed quantum states. Here we present a unified
approach to engineering optical materials that exhibit these phenomena by using
mixtures of active and passive optical materials at frequencies near their
resonances. Our approach does not depend on quantum coherence and can realize
large and small (much less than 1) indices of refraction and negative
permittivity ($\epsilon<0$), normal and anomalous dispersion, all while
maintaining transparency."
Karl Berggren,arXiv:physics/0611260,https://arxiv.org/abs/physics/0611260,"Abstract:  We investigate the source of large variations in the observed detection
effiiencies of superconducting nanowire single-photon detectors between many
nominally identical devices. Through both electrical and optical measurements,
we infer that these variations arise from ""constrictions:"" highly localized
regions of the nanowires where the effective cross-sectional area for
superconducting current is reduced. These constrictions limit the DC bias
current density to well below its critical value over the remainder of the
wire, and thus prevent the detection efficiency from reaching the high values
that occur in these devices only when they are biased near the critical current
density."
Karl Berggren,arXiv:cond-mat/0609561,https://arxiv.org/abs/cond-mat/0609561,"Abstract:  A nonlinear resonant circuit comprising a SQUID magnetometer and a parallel
capacitor is studied as a readout scheme for a persistent-current (PC) qubit.
The flux state of the qubit is detected as a change in the Josephson inductance
of the SQUID magnetometer, which in turn mediates a shift in the resonance
frequency of the readout circuit. The nonlinearity and resulting hysteresis in
the resonant behavior are characterized as a function of the power of both the
input drive and the associated resonance peak response. Numerical simulations
based on a phenomenological circuit model are presented which display the
features of the observed nonlinearity."
Karl Berggren,arXiv:cond-mat/0512691,https://arxiv.org/abs/cond-mat/0512691,"Abstract:  We demonstrate Mach-Zehnder-type interferometry in a superconducting flux
qubit. The qubit is a tunable artificial atom, whose ground and excited states
exhibit an avoided crossing. Strongly driving the qubit with harmonic
excitation sweeps it through the avoided crossing two times per period. As the
induced Landau-Zener transitions act as coherent beamsplitters, the accumulated
phase between transitions, which varies with microwave amplitude, results in
quantum interference fringes for n=1...20 photon transitions. The
generalization of optical Mach-Zehnder interferometry, performed in qubit phase
space, provides an alternative means to manipulate and characterize the qubit
in the strongly-driven regime."
Karl Berggren,arXiv:physics/0510238,https://arxiv.org/abs/physics/0510238,"Abstract:  We investigate the recovery of superconducting NbN-nanowire photon counters
after detection of an optical pulse at a wavelength of 1550 nm, and present a
model that quantitatively accounts for our observations. The reset time is
found to be limited by the large kinetic inductance of these nanowires, which
forces a tradeoff between counting rate and either detection efficiency or
active area. Devices of usable size and high detection efficiency are found to
have reset times orders of magnitude longer than their intrinsic photoresponse
time."
Karl Berggren,arXiv:physics/0509228,https://arxiv.org/abs/physics/0509228,"Abstract:  Quantum optical techniques may yield immersion fluids with high indices of
refraction without absorption. We describe one such technique in which a probe
field experiences a large index of refraction with amplification rather than
absorption, and examine its practicality for an immersion lithography
application. Enhanced index can be observed in a three-level system with a
tunable, near-resonant, coherent probe and incoherent pump field that inverts
population of the probe transition. This observation contradicts the common
belief that large indices of refraction are impossible without absorption,
however it is well in accord with existing electromagnetic theory and practice.
Calculations show that a refractive index >> 2 is possible with practical
experimental parameters. A scheme with an incoherent mixture of pumped and
unpumped atoms is also examined, and is seen to have a lower refractive index
(~2) accompanied by neither gain nor loss."
Karl Berggren,arXiv:cond-mat/0501283,https://arxiv.org/abs/cond-mat/0501283,"Abstract:  We have implemented a resonant circuit that uses a SQUID as a flux-sensitive
Josephson inductor for qubit readout. In contrast to the conventional switching
current measurement that generates undesired quasi-particles when the SQUID
switches to the voltage state, our approach keeps the readout SQUID biased
along the supercurrent branch during the measurement. By incorporating the
SQUID inductor in a high-Q resonant circuit, we can distinguish the two flux
states of a niobium persistent-current (PC) qubit by observing a shift in the
resonant frequency of both the magnitude and the phase spectra. The readout
circuit was also characterized in the nonlinear regime to investigate its
potential use as a nonlinear amplifier."
Karl Berggren,arXiv:cond-mat/0311289,https://arxiv.org/abs/cond-mat/0311289,"Abstract:  We measured the intrawell energy relaxation time \tau_{d} between macroscopic
quantum levels in the double well potential of a Nb persistent-current qubit.
Interwell population transitions were generated by irradiating the qubit with
microwaves. Zero population in the initial well was then observed due to a
multi-level decay process in which the initial population relaxed to the lower
energy levels during transitions. The qubit's decoherence time, determined from
\tau_{d}, is longer than 20 microseconds, holding the promise of building a
quantum computer with Nb-based superconducting qubits."
Karl Berggren,arXiv:quant-ph/0310157,https://arxiv.org/abs/quant-ph/0310157,"Abstract:  A numerical method for solving Schrodinger's equation based upon a
Baker-Campbell-Hausdorff (BCH) expansion of the time evolution operator is
presented herein. The technique manifestly preserves wavefunction norm, and it
can be applied to problems in any number of spatial dimensions. We also
identify a particular dimensionless ratio of potential to kinetic energies as a
key coupling constant. This coupling establishes characteristic length and time
scales for a large class of low energy quantum states, and it guides the choice
of step sizes in numerical work. Using the BCH method in conjunction with an
imaginary time rotation, we compute low energy eigenstates for several quantum
systems coupled to non-trivial background potentials. The approach is
subsequently applied to the study of 1D propagating wave packets and 2D bound
state time development. Failures of classical expectations uncovered by
simulations of these simple systems help develop quantum intuition.
Finally, we investigate the response of a Superconducting Quantum
Interference Device (SQUID) to a time dependent potential. We discuss how to
engineer the potential's energy and time scales so that the SQUID acts as a
quantum NOT gate. The notional simulation we present for this gate provides
useful insight into the design of one candidate building block for a quantum
computer."
Karl Berggren,arXiv:nlin/0111010,https://arxiv.org/abs/nlin/0111010,"Abstract:  The probability current statistics of two-dimensional open chaotic ballistic
billiards is studied both analytically and numerically. Assuming that the real
and imaginary parts of the scattering wave function are both random Gaussian
fields, we find a universal distribution function for the probability current.
In by-passing we recover previous analytic forms for wave function statistics.
The expressions bridge the entire region from GOE to GUE type statistics. Our
analytic expressions are verified numerically by explicit quantum-mechanical
calculations of transport through a Bunimovich billiard."
Dimitri Bertsekas,arXiv:1804.04577,https://arxiv.org/abs/1804.04577,"Abstract:  In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller ""aggregate"" Markov decision problem, whose states
relate to the features. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with feature construction using deep neural networks
or other calculations. We argue that the cost function of a policy may be
approximated much more accurately by the nonlinear function of the features
provided by aggregation, than by the linear function of the features provided
by neural network-based reinforcement learning, thereby potentially leading to
more effective policy improvement."
Dimitri Bertsekas,arXiv:1712.06659,https://arxiv.org/abs/1712.06659,"Abstract:  We consider discrete-time infinite horizon deterministic optimal control
problems with nonnegative cost per stage, and a destination that is cost-free
and absorbing. The classical linear-quadratic regulator problem is a special
case. Our assumptions are very general, and allow the possibility that the
optimal policy may not be stabilizing the system, e.g., may not reach the
destination either asymptotically or in a finite number of steps. We introduce
a new unifying notion of stable feedback policy, based on perturbation of the
cost per stage, which in addition to implying convergence of the generated
states to the destination, quantifies the speed of convergence. We consider the
properties of two distinct cost functions: $\jstar$, the overall optimal, and
$\hat J$, the restricted optimal over just the stable policies. Different
classes of stable policies (with different speeds of convergence) may yield
different values of $\hat J$. We show that for any class of stable policies,
$\hat J$ is a solution of Bellman's equation, and we characterize the smallest
and the largest solutions: they are $\jstar$, and $J^+$, the restricted optimal
cost function over the class of (finitely) terminating policies. We also
characterize the regions of convergence of various modified versions of value
and policy iteration algorithms, as substitutes for the standard algorithms,
which may not work in general."
Dimitri Bertsekas,arXiv:1711.10129,https://arxiv.org/abs/1711.10129,"Abstract:  We consider stochastic shortest path problems with infinite state and control
spaces, a nonnegative cost per stage, and a termination state. We extend the
notion of a proper policy, a policy that terminates within a finite expected
number of steps, from the context of finite state space to the context of
infinite state space. We consider the optimal cost function $\jstar$, and the
optimal cost function $\hat J$ over just the proper policies. We show that
$\jstar$ and $\hat J$ are the smallest and largest solutions of Bellman's
equation, respectively, within a suitable class of Lyapounov-like functions. If
the cost per stage is bounded, these functions are those that are bounded over
the effective domain of $\hat J$. The standard value iteration algorithm may be
attracted to either $\jstar$ or $\hat J$, depending on the initial condition.
In the favorable case where $\jstar=\hat J$, strong analytical and algorithmic
results are obtained."
Dimitri Bertsekas,arXiv:1610.05427,https://arxiv.org/abs/1610.05427,"Abstract:  We consider large linear and nonlinear fixed point problems, and solution
with proximal algorithms. We show that there is a close connection between two
seemingly different types of methods from distinct fields: 1) Proximal
iterations for linear systems of equations, which are prominent in numerical
analysis and convex optimization, and 2) Temporal difference (TD) type methods,
such as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in
simulation-based approximate dynamic programming/reinforcement learning
(DP/RL), and its recent prominent successes in large-scale game contexts, among
others.
One benefit of this connection is a new and simple way to accelerate the
standard proximal algorithm by extrapolation towards the TD iteration, which
generically has a faster convergence rate. Another benefit is the potential
integration into the proximal algorithmic context of several new ideas that
have emerged in the DP/RL context. We discuss some of the possibilities, and in
particular, algorithms that project each proximal iterate onto the subspace
spanned by a small number of basis functions, using low-dimensional
calculations and simulation. A third benefit is that insights and analysis from
proximal algorithms can be brought to bear on the enhancement of TD methods.
The linear fixed point methodology can be extended to nonlinear fixed point
problems involving a contraction, thus providing guaranteed and potentially
substantial acceleration of the proximal and forward backward splitting
algorithms at no extra cost. Moreover, the connection of proximal and TD
methods can be extended to nonlinear (nondifferentiable) fixed point problems
through new proximal-like algorithms that involve successive linearization,
similar to policy iteration in DP."
Dimitri Bertsekas,arXiv:1609.03115,https://arxiv.org/abs/1609.03115,"Abstract:  We consider challenging dynamic programming models where the associated
Bellman equation, and the value and policy iteration algorithms commonly
exhibit complex and even pathological behavior. Our analysis is based on the
new notion of regular policies. These are policies that are well-behaved with
respect to value and policy iteration, and are patterned after proper policies,
which are central in the theory of stochastic shortest path problems. We show
that the optimal cost function over regular policies may have favorable value
and policy iteration properties, which the optimal cost function over all
policies need not have. We accordingly develop a unifying methodology to
address long standing analytical and algorithmic issues in broad classes of
undiscounted models, including stochastic and minimax shortest path problems,
as well as positive cost, negative cost, risk-sensitive, and multiplicative
cost problems."
Dimitri Bertsekas,arXiv:1608.01670,https://arxiv.org/abs/1608.01670,"Abstract:  In this paper we consider shortest path problems in a directed graph where
the transitions between nodes are subject to uncertainty. We use a minimax
formulation, where the objective is to guarantee that a special destination
state is reached with a minimum cost path under the worst possible instance of
the uncertainty. Problems of this type arise, among others, in planning and
pursuit-evasion contexts, and in model predictive control. Our analysis makes
use of the recently developed theory of abstract semicontractive dynamic
programming models. We investigate questions of existence and uniqueness of
solution of the optimality equation, existence of optimal paths, and the
validity of various algorithms patterned after the classical methods of value
and policy iteration, as well as a Dijkstra-like algorithm for problems with
nonnegative arc lengths."
Dimitri Bertsekas,arXiv:1608.01393,https://arxiv.org/abs/1608.01393,"Abstract:  In this paper we consider a broad class of infinite horizon discrete-time
optimal control models that involve a nonnegative cost function and an affine
mapping in their dynamic programming equation. They include as special cases
classical models such as stochastic undiscounted nonnegative cost problems,
stochastic multiplicative cost problems, and risk-sensitive problems with
exponential cost. We focus on the case where the state space is finite and the
control space has some compactness properties. We assume that the affine
mapping has a semicontractive character, whereby for some policies it is a
contraction, while for others it is not. In one line of analysis, we impose
assumptions that guarantee that the latter policies cannot be optimal. Under
these assumptions, we prove strong results that resemble those for discounted
Markovian decision problems, such as the uniqueness of solution of Bellman's
equation, and the validity of forms of value and policy iteration. In the
absence of these assumptions, the results are weaker and unusual in character:
the optimal cost function need not be a solution of Bellman's equation, and an
optimal policy may not be found by value or policy iteration. Instead the
optimal cost function over just the contractive policies solves Bellman's
equation, and can be computed by a variety of algorithms."
Dimitri Bertsekas,arXiv:1509.09257,https://arxiv.org/abs/1509.09257,"Abstract:  We consider minimization of the sum of a large number of convex functions,
and we propose an incremental aggregated version of the proximal algorithm,
which bears similarity to the incremental aggregated gradient and subgradient
methods that have received a lot of recent attention. Under cost function
differentiability and strong convexity assumptions, we show linear convergence
for a sufficiently small constant stepsize. This result also applies to
distributed asynchronous variants of the method, involving bounded
interprocessor communication delays.
We then consider dual versions of incremental proximal algorithms, which are
incremental augmented Lagrangian methods for separable equality-constrained
optimization problems. Contrary to the standard augmented Lagrangian method,
these methods admit decomposition in the minimization of the augmented
Lagrangian, and update the multipliers far more frequently. Our incremental
aggregated augmented Lagrangian methods bear similarity to several known
decomposition algorithms, including the alternating direction method of
multipliers (ADMM) and more recent variations. We compare these methods in
terms of their properties, and highlight their potential advantages and
limitations.
We also address the solution of separable inequality-constrained optimization
problems through the use of nonquadratic augmented Lagrangiias such as the
exponential, and we dually consider a corresponding incremental aggregated
version of the proximal algorithm that uses nonquadratic regularization, such
as an entropy function. We finally propose a closely related linearly
convergent method for minimization of large differentiable sums subject to an
orthant constraint, which may be viewed as an incremental aggregated version of
the mirror descent method."
Dimitri Bertsekas,arXiv:1507.01030,https://arxiv.org/abs/1507.01030,"Abstract:  We survey incremental methods for minimizing a sum $\sum_{i=1}^mf_i(x)$
consisting of a large number of convex component functions $f_i$. Our methods
consist of iterations applied to single components, and have proved very
effective in practice. We introduce a unified algorithmic framework for a
variety of such methods, some involving gradient and subgradient iterations,
which are known, and some involving combinations of subgradient and proximal
methods, which are new and offer greater flexibility in exploiting the special
structure of $f_i$. We provide an analysis of the convergence and rate of
convergence properties of these methods, including the advantages offered by
randomization in the selection of components. We also survey applications in
inference/machine learning, signal processing, and large-scale and distributed
optimization."
Dimitri Bertsekas,arXiv:1507.01029,https://arxiv.org/abs/1507.01029,"Abstract:  In this paper we discuss $\l$-policy iteration, a method for exact and
approximate dynamic programming. It is intermediate between the classical value
iteration (VI) and policy iteration (PI) methods, and it is closely related to
optimistic (also known as modified) PI, whereby each policy evaluation is done
approximately, using a finite number of VI. We review the theory of the method
and associated questions of bias and exploration arising in simulation-based
cost function approximation. We then discuss various implementations, which
offer advantages over well-established PI methods that use LSPE($\l$),
LSTD($\l$), or TD($\l$) for policy evaluation with cost function approximation.
One of these implementations is based on a new simulation scheme, called
geometric sampling, which uses multiple short trajectories rather than a single
infinitely long trajectory."
Dimitri Bertsekas,arXiv:1507.01026,https://arxiv.org/abs/1507.01026,"Abstract:  In this paper, we consider discrete-time infinite horizon problems of optimal
control to a terminal set of states. These are the problems that are often
taken as the starting point for adaptive dynamic programming. Under very
general assumptions, we establish the uniqueness of solution of Bellman's
equation, and we provide convergence results for value and policy iteration."
Dimitri Bertsekas,arXiv:1507.00702,https://arxiv.org/abs/1507.00702,"Abstract:  We consider Newton methods for common types of single commodity and
multi-commodity network flow problems. Despite the potentially very large
dimension of the problem, they can be implemented using the conjugate gradient
method and low-dimensional network operations, as shown nearly thirty years
ago. We revisit these methods, compare them to more recent proposals, and
describe how they can be implemented in a distributed computing system. We also
discuss generalizations, including the treatment of arc gains, linear side
constraints, and related special structures."
Dimitri Bertsekas,arXiv:1308.3814,https://arxiv.org/abs/1308.3814,"Abstract:  We consider stochastic control models with Borel spaces and universally
measurable policies. For such models the standard policy iteration is known to
have difficult measurability issues and cannot be carried out in general. We
present a mixed value and policy iteration method that circumvents this
difficulty. The method allows the use of stationary policies in computing the
optimal cost function, in a manner that resembles policy iteration. It can also
be used to address similar difficulties of policy iteration in the context of
upper and lower semicontinuous models. We analyze the convergence of the method
in infinite horizon total cost problems, for the discounted case where the
one-stage costs are bounded, and for the undiscounted case where the one-stage
costs are nonpositive or nonnegative.
For undiscounted total cost problems with nonnegative one-stage costs, we
also give a new convergence theorem for value iteration, which shows that value
iteration converges whenever it is initialized with a function that is above
the optimal cost function and yet bounded by a multiple of the optimal cost
function. This condition resembles Whittle's bridging condition and is partly
motivated by it. The theorem is also partly motivated by a result of Maitra and
Sudderth, which showed that value iteration, when initialized with the constant
function zero, could require a transfinite number of iterations to converge. We
use the new convergence theorem for value iteration to establish the
convergence of our mixed value and policy iteration method for the nonnegative
cost case."
Dimitri Bertsekas,arXiv:1207.4154,https://arxiv.org/abs/1207.4154,"Abstract:  In this paper, we propose a new lower approximation scheme for POMDP with
discounted and average cost criterion. The approximating functions are
determined by their values at a finite number of belief points, and can be
computed efficiently using value iteration algorithms for finite-state MDP.
While for discounted problems several lower approximation schemes have been
proposed earlier, ours seems the first of its kind for average cost problems.
We focus primarily on the average cost case, and we show that the corresponding
approximation can be computed efficiently using multi-chain algorithms for
finite-state MDP. We give a preliminary analysis showing that regardless of the
existence of the optimal average cost J in the POMDP, the approximation
obtained is a lower bound of the liminf optimal average cost function, and can
also be used to calculate an upper bound on the limsup optimal average cost
function, as well as bounds on the cost of executing the stationary policy
associated with the approximation. Weshow the convergence of the cost
approximation, when the optimal average cost is constant and the optimal
differential cost is continuous."
Robert Berwick,arXiv:1811.02611,https://arxiv.org/abs/1811.02611,"Abstract:  While long short-term memory (LSTM) neural net architectures are designed to
capture sequence information, human language is generally composed of
hierarchical structures. This raises the question as to whether LSTMs can learn
hierarchical structures. We explore this question with a well-formed bracket
prediction task using two types of brackets modeled by an LSTM. Demonstrating
that such a system is learnable by an LSTM is the first step in demonstrating
that the entire class of CFLs is also learnable. We observe that the model
requires exponential memory in terms of the number of characters and embedded
depth, where a sub-linear memory should suffice. Still, the model does more
than memorize the training input. It learns how to distinguish between relevant
and irrelevant information. On the other hand, we also observe that the model
does not generalize well. We conclude that LSTMs do not learn the relevant
underlying context-free rules, suggesting the good overall performance is
attained rather by an efficient way of evaluating nuisance variables. LSTMs are
a way to quickly reach good results for many natural language tasks, but to
understand and generate natural language one has to investigate other concepts
that can make more direct use of natural language's structural nature."
Robert Berwick,arXiv:1803.09832,https://arxiv.org/abs/1803.09832,"Abstract:  We consider two different data sets of syntactic parameters and we discuss
how to detect relations between parameters through a heat kernel method
developed by Belkin-Niyogi, which produces low dimensional representations of
the data, based on Laplace eigenfunctions, that preserve neighborhood
information. We analyze the different connectivity and clustering structures
that arise in the two datasets, and the regions of maximal variance in the
two-parameter space of the Belkin-Niyogi construction, which identify
preferable choices of independent variables. We compute clustering coefficients
and their variance."
Robert Berwick,arXiv:1712.01719,https://arxiv.org/abs/1712.01719,"Abstract:  Using Phylogenetic Algebraic Geometry, we analyze computationally the
phylogenetic tree of subfamilies of the Indo-European language family, using
data of syntactic structures. The two main sources of syntactic data are the
SSWL database and Longobardi's recent data of syntactic parameters. We compute
phylogenetic invariants and likelihood functions for two sets of Germanic
languages, a set of Romance languages, a set of Slavic languages and a set of
early Indo-European languages, and we compare the results with what is known
through historical linguistics."
Robert Berwick,arXiv:cmp-lg/9503012,https://arxiv.org/abs/cmp-lg/9503012,"Abstract:  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the
basis of Zipf rank frequency data that noncoding DNA sequence regions are more
like natural languages than coding regions. We argue on the contrary that an
empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to
natural languages. Although DNA is a presumably an ``organized system of
signs'' in Mandelbrot's (1961) sense, an observation of statistical features of
the sort presented in the Mantegna et al. paper does not shed light on the
similarity between DNA's ``grammar'' and natural language grammars, just as the
observation of exact Zipf-like behavior cannot distinguish between the
underlying processes of tossing an $M$ sided die or a finite-state branching
process."
Sangeeta Bhatia,arXiv:1610.00077,https://arxiv.org/abs/1610.00077,"Abstract:  Modellers of large scale genome rearrangement events, in which segments of
DNA are inverted, moved, swapped, or even inserted or deleted, have found a
natural syntax in the language of permutations. Despite this, there has been a
wide range of modelling choices, assumptions and interpretations that make
navigating the literature a significant challenge. Indeed, even authors of
papers that use permutations to model genome rearrangement can struggle to
interpret each others' work, because of subtle differences in basic assumptions
that are often deeply ingrained (and consequently sometimes not even
mentioned). In this paper, we describe the different ways in which permutations
have been used to model genomes and genome rearrangement events, presenting
some features and limitations of each approach, and show how the various models
are related. This paper will help researchers navigate the landscape of genome
rearrangement models, and make it easier for authors to present clear and
consistent models."
Sangeeta Bhatia,arXiv:1409.7146,https://arxiv.org/abs/1409.7146,"Abstract:  Establishing a distance between genomes is a significant problem in
computational genomics, because its solution can be used to establish
evolutionary relationships including phylogeny.
The ""double cut and join"" (DCJ) model of chromosomal rearrangement proposed
by Yancopoulos et al. has received attention as it can model inversions,
translocations, fusion and fission on a multichromosomal genome that may
contain both linear and circular chromosomes. In this paper, we realize the DCJ
operator as a group action on the space of multichromosomal genomes. We study
this group action, deriving some properties of the group and finding
group-theoretic analogues for the key results in the DCJ theory."
Duane Boning,arXiv:1901.04684,https://arxiv.org/abs/1901.04684,"Abstract:  The adversarial training procedure proposed by Madry et al. (2018) is one of
the most effective methods to defend against adversarial examples in deep
neural networks (DNNs). In our paper, we shed some lights on the practicality
and the hardness of adversarial training by showing that the effectiveness
(robustness on test set) of adversarial training has a strong correlation with
the distance between a test point and the manifold of training data embedded by
the network. Test examples that are relatively far away from this manifold are
more likely to be vulnerable to adversarial attacks. Consequentially, an
adversarial training based defense is susceptible to a new class of attacks,
the ""blind-spot attack"", where the input images reside in ""blind-spots"" (low
density regions) of the empirical distribution of training data but is still on
the ground-truth data manifold. For MNIST, we found that these blind-spots can
be easily found by simply scaling and shifting image pixel values. Most
importantly, for large datasets with high dimensional and complex data manifold
(CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training
makes defending on any valid test examples difficult due to the curse of
dimensionality and the scarcity of training data. Additionally, we find that
blind-spots also exist on provable defenses including (Wong & Kolter, 2018) and
(Sinha et al., 2018) because these trainable robustness certificates can only
be practically optimized on a limited set of training data."
Duane Boning,arXiv:1804.09699,https://arxiv.org/abs/1804.09699,"Abstract:  Verifying the robustness property of a general Rectified Linear Unit (ReLU)
network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer
CAV17]. Although finding the exact minimum adversarial distortion is hard,
giving a certified lower bound of the minimum distortion is possible. Current
available methods of computing such a bound are either time-consuming or
delivering low quality bounds that are too loose to be useful. In this paper,
we exploit the special structure of ReLU networks and provide two
computationally efficient algorithms Fast-Lin and Fast-Lip that are able to
certify non-trivial lower bounds of minimum distortions, by bounding the ReLU
units with appropriate linear functions Fast-Lin, or by bounding the local
Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods
deliver bounds close to (the gap is 2-3X) exact minimum distortion found by
Reluplex in small MNIST networks while our algorithms are more than 10,000
times faster; (2) our methods deliver similar quality of bounds (the gap is
within 35% and usually around 10%; sometimes our bounds are even better) for
larger networks compared to the methods based on solving linear programming
problems but our algorithms are 33-14,000 times faster; (3) our method is
capable of solving large MNIST and CIFAR networks up to 7 layers with more than
10,000 neurons within tens of seconds on a single CPU core.
In addition, we show that, in fact, there is no polynomial time algorithm
that can approximately find the minimum $\ell_1$ adversarial distortion of a
ReLU network with a $0.99\ln n$ approximation ratio unless
$\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network."
Duane Boning,arXiv:1703.09876,https://arxiv.org/abs/1703.09876,"Abstract:  In this paper, we propose a novel method to estimate and characterize spatial
variations on dies or wafers. This new technique exploits recent developments
in matrix completion, enabling estimation of spatial variation across wafers or
dies with a small number of randomly picked sampling points while still
achieving fairly high accuracy. This new approach can be easily generalized,
including for estimation of mixed spatial and structure or device type
information."
Guy Bresler,arXiv:1902.06916,https://arxiv.org/abs/1902.06916,"Abstract:  In the general submatrix detection problem, the task is to detect the
presence of a small $k \times k$ submatrix with entries sampled from a
distribution $\mathcal{P}$ in an $n \times n$ matrix of samples from
$\mathcal{Q}$. This formulation includes a number of well-studied problems,
such as biclustering when $\mathcal{P}$ and $\mathcal{Q}$ are Gaussians and the
planted dense subgraph formulation of community detection when the submatrix is
a principal minor and $\mathcal{P}$ and $\mathcal{Q}$ are Bernoulli random
variables. These problems all seem to exhibit a universal phenomenon: there is
a statistical-computational gap depending on $\mathcal{P}$ and $\mathcal{Q}$
between the minimum $k$ at which this task can be solved and the minimum $k$ at
which it can be solved in polynomial time.
Our main result is to tightly characterize this computational barrier as a
tradeoff between $k$ and the KL divergences between $\mathcal{P}$ and
$\mathcal{Q}$ through average-case reductions from the planted clique
conjecture. These computational lower bounds hold given mild assumptions on
$\mathcal{P}$ and $\mathcal{Q}$ arising naturally from classical binary
hypothesis testing. Our results recover and generalize the planted clique lower
bounds for Gaussian biclustering in Ma-Wu (2015) and Brennan et al. (2018) and
for the sparse and general regimes of planted dense subgraph in Hajek et al.
(2015) and Brennan et al. (2018). This yields the first universality principle
for computational lower bounds obtained through average-case reductions."
Guy Bresler,arXiv:1811.10106,https://arxiv.org/abs/1811.10106,"Abstract:  Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR)
have a wide range of applications and have attracted a tremendous amount of
attention in the last two decades as canonical examples of statistical problems
in high dimension. A variety of algorithms have been proposed for both SPCA and
SLR, but an explicit connection between the two had not been made. We show how
to efficiently transform a black-box solver for SLR into an algorithm for SPCA:
assuming the SLR solver satisfies prediction error guarantees achieved by
existing efficient algorithms such as those based on the Lasso, the SPCA
algorithm derived from it achieves near state of the art guarantees for testing
and for support recovery for the single spiked covariance model as obtained by
the current best polynomialtime algorithms. Our reduction not only highlights
the inherent similarity between the two problems, but also, from a practical
standpoint, allows one to obtain a collection of algorithms for SPCA directly
from known algorithms for SLR. We provide experimental results on simulated
data comparing our proposed framework to other algorithms for SPCA."
Guy Bresler,arXiv:1806.07508,https://arxiv.org/abs/1806.07508,"Abstract:  The prototypical high-dimensional statistics problem entails finding a
structured signal in noise. Many of these problems exhibit an intriguing
phenomenon: the amount of data needed by all known computationally efficient
algorithms far exceeds what is needed for inefficient algorithms that search
over all possible structures. A line of work initiated by Berthet and Rigollet
in 2013 has aimed to explain these statistical-computational gaps by reducing
from conjecturally hard average-case problems in computer science. However, the
delicate nature of average-case reductions has limited the applicability of
this approach. In this work we introduce several new techniques to give a web
of average-case reductions showing strong computational lower bounds based on
the planted clique conjecture using natural problems as intermediates. These
include tight lower bounds for Planted Independent Set, Planted Dense Subgraph,
Sparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block
Model and a biased variant of Sparse PCA. We also give algorithms matching our
lower bounds and identify the information-theoretic limits of the models we
consider."
Guy Bresler,arXiv:1805.10262,https://arxiv.org/abs/1805.10262,"Abstract:  Graphical models are a rich language for describing high-dimensional
distributions in terms of their dependence structure. While there are
algorithms with provable guarantees for learning undirected graphical models in
a variety of settings, there has been much less progress in the important
scenario when there are latent variables. Here we study Restricted Boltzmann
Machines (or RBMs), which are a popular model with wide-ranging applications in
dimensionality reduction, collaborative filtering, topic modeling, feature
extraction and deep learning.
The main message of our paper is a strong dichotomy in the feasibility of
learning RBMs, depending on the nature of the interactions between variables:
ferromagnetic models can be learned efficiently, while general models cannot.
In particular, we give a simple greedy algorithm based on influence
maximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn
a description of the distribution on the observed variables as a Markov Random
Field. Our analysis is based on tools from mathematical physics that were
developed to show the concavity of magnetization. Our algorithm extends
straighforwardly to general ferromagnetic Ising models with latent variables.
Conversely, we show that even for a contant number of latent variables with
constant degree, without ferromagneticity the problem is as hard as sparse
parity with noise. This hardness result is based on a sharp and surprising
characterization of the representational power of bounded degree RBMs: the
distribution on their observed variables can simulate any bounded order MRF.
This result is of independent interest since RBMs are the building blocks of
deep belief networks."
Guy Bresler,arXiv:1805.03027,https://arxiv.org/abs/1805.03027,"Abstract:  Most information systems store data by modifying the local state of matter,
in the hope that atomic (or sub-atomic) local interactions would stabilize the
state for a sufficiently long time, thereby allowing later recovery. In this
work we initiate the study of information retention in locally-interacting
systems. The evolution in time of the interacting particles is modeled via the
stochastic Ising model (SIM). The initial spin configuration $X_0$ serves as
the user-controlled input. The output configuration $X_t$ is produced by
running $t$ steps of the Glauber chain. Our main goal is to evaluate the
information capacity $I_n(t)\triangleq\max_{p_{X_0}}I(X_0;X_t)$ when the time
$t$ scales with the size of the system $n$. For the zero-temperature SIM on the
two-dimensional $\sqrt{n}\times\sqrt{n}$ grid and free boundary conditions, it
is easy to show that $I_n(t) = \Theta(n)$ for $t=O(n)$. In addition, we show
that on the order of $\sqrt{n}$ bits can be stored for infinite time in striped
configurations. The $\sqrt{n}$ achievability is optimal when $t\to\infty$ and
$n$ is fixed.
One of the main results of this work is an achievability scheme that stores
more than $\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$)
times. The analysis of the scheme decomposes the system into $\Omega(\sqrt{n})$
independent Z-channels whose crossover probability is found via the (recently
rigorously established) Lifshitz law of phase boundary movement. We also
provide results for the positive but small temperature regime. We show that an
initial configuration drawn according to the Gibbs measure cannot retain more
than a single bit for $t\geq e^{cn^{\frac{1}{4}+\epsilon}}$. On the other hand,
when scaling time with $\beta$, the stripe-based coding scheme (that stores for
infinite time at zero temperature) is shown to retain its bits for time that is
exponential in $\beta$."
Guy Bresler,arXiv:1802.06186,https://arxiv.org/abs/1802.06186,"Abstract:  We study the problem of testing, using only a single sample, between mean
field distributions (like Curie-Weiss, Erds-Rnyi) and structured Gibbs
distributions (like Ising model on sparse graphs and Exponential Random
Graphs). Our goal is to test without knowing the parameter values of the
underlying models: only the \emph{structure} of dependencies is known. We
develop a new approach that applies to both the Ising and Exponential Random
Graph settings based on a general and natural statistical test. The test can
distinguish the hypotheses with high probability above a certain threshold in
the (inverse) temperature parameter, and is optimal in that below the threshold
no test can distinguish the hypotheses.
The thresholds do not correspond to the presence of long-range order in the
models. By aggregating information at a global scale, our test works even at
very high temperatures.
The proofs are based on distributional approximation and sharp concentration
of quadratic forms, when restricted to Hamming spheres. The restriction to
Hamming spheres is necessary, since otherwise any scalar statistic is useless
without explicit knowledge of the temperature parameter. At the same time, this
restriction radically changes the behavior of the functions under
consideration, resulting in a much smaller variance than in the independent
setting; this makes it hard to directly apply standard methods (i.e., Stein's
method) for concentration of weakly dependent variables. Instead, we carry out
an additional tensorization argument using a Markov chain that respects the
symmetry of the Hamming sphere."
Guy Bresler,arXiv:1712.05743,https://arxiv.org/abs/1712.05743,"Abstract:  We develop a new technique, based on Stein's method, for comparing two
stationary distributions of irreducible Markov Chains whose update rules are
`close enough'. We apply this technique to compare Ising models on $d$-regular
expander graphs to the Curie-Weiss model (complete graph) in terms of pairwise
correlations and more generally $k$th order moments. Concretely, we show that
$d$-regular Ramanujan graphs approximate the $k$th order moments of the
Curie-Weiss model to within average error $k/\sqrt{d}$ (averaged over the size
$k$ subsets). The result applies even in the low-temperature regime; we also
derive some simpler approximation results for functionals of Ising models that
hold only at high enough temperatures."
Guy Bresler,arXiv:1711.02198,https://arxiv.org/abs/1711.02198,"Abstract:  We consider an online model for recommendation systems, with each user being
recommended an item at each time-step and providing 'like' or 'dislike'
feedback. A latent variable model specifies the user preferences: both users
and items are clustered into types. All users of a given type have identical
preferences for the items, and similarly, items of a given type are either all
liked or all disliked by a given user. The model captures structure in both the
item and user spaces, and in this paper, we assume that the type preference
matrix is randomly generated. We describe two algorithms inspired by user-user
and item-item collaborative filtering (CF), modified to explicitly make
exploratory recommendations, and prove performance guarantees in terms of their
expected regret. For two regimes of model parameters, with structure only in
item space or only in user space, we prove information-theoretic lower bounds
on regret that match our upper bounds up to logarithmic factors. Our analysis
elucidates system operating regimes in which existing CF algorithms are nearly
optimal."
Guy Bresler,arXiv:1604.06749,https://arxiv.org/abs/1604.06749,"Abstract:  We study the problem of learning a tree Ising model from samples such that
subsequent predictions made using the model are accurate. The prediction task
considered in this paper is that of predicting the values of a subset of
variables given values of some other subset of variables. Virtually all
previous work on graphical model learning has focused on recovering the true
underlying graph. We define a distance (""small set TV"" or ssTV) between
distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$
of a given size, of the total variation between the marginals of $P$ and $Q$ on
$\mathcal{S}$; this distance captures the accuracy of the prediction task of
interest. We derive non-asymptotic bounds on the number of samples needed to
get a distribution (from the same class) with small ssTV relative to the one
generating the samples. One of the main messages of this paper is that far
fewer samples are needed than for recovering the underlying tree, which means
that accurate predictions are possible using the wrong tree."
Guy Bresler,arXiv:1507.05371,https://arxiv.org/abs/1507.05371,"Abstract:  There is much empirical evidence that item-item collaborative filtering works
well in practice. Motivated to understand this, we provide a framework to
design and analyze various recommendation algorithms. The setup amounts to
online binary matrix completion, where at each time a random user requests a
recommendation and the algorithm chooses an entry to reveal in the user's row.
The goal is to minimize regret, or equivalently to maximize the number of +1
entries revealed at any time. We analyze an item-item collaborative filtering
algorithm that can achieve fundamentally better performance compared to
user-user collaborative filtering. The algorithm achieves good ""cold-start""
performance (appropriately defined) by quickly making good recommendations to
new users about whom there is little information."
Guy Bresler,arXiv:1412.1443,https://arxiv.org/abs/1412.1443,"Abstract:  In this paper we investigate the computational complexity of learning the
graph structure underlying a discrete undirected graphical model from i.i.d.
samples. We first observe that the notoriously difficult problem of learning
parities with noise can be captured as a special case of learning graphical
models. This leads to an unconditional computational lower bound of $\Omega
(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree
$d$, for the class of so-called statistical algorithms recently introduced by
Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime
required to exhaustively search over neighborhoods cannot be significantly
improved without restricting the class of models.
Aside from structural assumptions on the graph such as it being a tree,
hypertree, tree-like, etc., many recent papers on structure learning assume
that the model has the correlation decay property. Indeed, focusing on
ferromagnetic Ising models, Bento and Montanari (2009) showed that all known
low-complexity algorithms fail to learn simple graphs when the interaction
strength exceeds a number related to the correlation decay threshold. Our
second set of results gives a class of repelling (antiferromagnetic) models
that have the opposite behavior: very strong interaction allows efficient
learning in time $O(p^2)$. We provide an algorithm whose performance
interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the
repulsion."
Guy Bresler,arXiv:1411.6591,https://arxiv.org/abs/1411.6591,"Abstract:  Despite the prevalence of collaborative filtering in recommendation systems,
there has been little theoretical development on why and how well it works,
especially in the ""online"" setting, where items are recommended to users over
time. We address this theoretical gap by introducing a model for online
recommendation systems, cast item recommendation under the model as a learning
problem, and analyze the performance of a cosine-similarity collaborative
filtering method. In our model, each of $n$ users either likes or dislikes each
of $m$ items. We assume there to be $k$ types of users, and all the users of a
given type share a common string of probabilities determining the chance of
liking each item. At each time step, we recommend an item to each user, where a
key distinction from related bandit literature is that once a user consumes an
item (e.g., watches a movie), then that item cannot be recommended to the same
user again. The goal is to maximize the number of likable items recommended to
users over time. Our main result establishes that after nearly $\log(km)$
initial learning time steps, a simple collaborative filtering algorithm
achieves essentially optimal performance without knowing $k$. The algorithm has
an exploitation step that uses cosine similarity and two types of exploration
steps, one to explore the space of items (standard in the literature) and the
other to explore similarity between users (novel to this work)."
Guy Bresler,arXiv:1411.6156,https://arxiv.org/abs/1411.6156,"Abstract:  We consider the problem of reconstructing the graph underlying an Ising model
from i.i.d. samples. Over the last fifteen years this problem has been of
significant interest in the statistics, machine learning, and statistical
physics communities, and much of the effort has been directed towards finding
algorithms with low computational cost for various restricted classes of
models. Nevertheless, for learning Ising models on general graphs with $p$
nodes of degree at most $d$, it is not known whether or not it is possible to
improve upon the $p^{d}$ computation needed to exhaustively search over all
possible neighborhoods for each node.
In this paper we show that a simple greedy procedure allows to learn the
structure of an Ising model on an arbitrary bounded-degree graph in time on the
order of $p^2$. We make no assumptions on the parameters except what is
necessary for identifiability of the model, and in particular the results hold
at low-temperatures as well as for highly non-uniform models. The proof rests
on a new structural property of Ising models: we show that for any node there
exists at least one neighbor with which it has a high mutual information. This
structural property may be of independent interest."
Guy Bresler,arXiv:1410.7659,https://arxiv.org/abs/1410.7659,"Abstract:  In this paper we consider the problem of learning undirected graphical models
from data generated according to the Glauber dynamics. The Glauber dynamics is
a Markov chain that sequentially updates individual nodes (variables) in a
graphical model and it is frequently used to sample from the stationary
distribution (to which it converges given sufficient time). Additionally, the
Glauber dynamics is a natural dynamical model in a variety of settings. This
work deviates from the standard formulation of graphical model learning in the
literature, where one assumes access to i.i.d. samples from the distribution.
Much of the research on graphical model learning has been directed towards
finding algorithms with low computational cost. As the main result of this
work, we establish that the problem of reconstructing binary pairwise graphical
models is computationally tractable when we observe the Glauber dynamics.
Specifically, we show that a binary pairwise graphical model on $p$ nodes with
maximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function
$f(d)$, using nearly the information-theoretic minimum number of samples."
Guy Bresler,arXiv:1409.3836,https://arxiv.org/abs/1409.3836,"Abstract:  We consider the problem of learning the canonical parameters specifying an
undirected graphical model (Markov random field) from the mean parameters. For
graphical models representing a minimal exponential family, the canonical
parameters are uniquely determined by the mean parameters, so the problem is
feasible in principle. The goal of this paper is to investigate the
computational feasibility of this statistical task. Our main result shows that
parameter estimation is in general intractable: no algorithm can learn the
canonical parameters of a generic pair-wise binary graphical model from the
mean parameters in time bounded by a polynomial in the number of variables
(unless RP = NP). Indeed, such a result has been believed to be true (see the
monograph by Wainwright and Jordan (2008)) but no proof was known.
Our proof gives a polynomial time reduction from approximating the partition
function of the hard-core model, known to be hard, to learning approximate
parameters. Our reduction entails showing that the marginal polytope boundary
has an inherent repulsive property, which validates an optimization procedure
over the polytope that does not use any knowledge of its structure (as required
by the ellipsoid method and others)."
Guy Bresler,arXiv:1303.5678,https://arxiv.org/abs/1303.5678,"Abstract:  We study vector space interference alignment for the MIMO interference
channel with no time or frequency diversity, and no symbol extensions. We prove
both necessary and sufficient conditions for alignment. In particular, we
characterize the feasibility of alignment for the symmetric three-user channel
where all users transmit along d dimensions, all transmitters have M antennas
and all receivers have N antennas, as well as feasibility of alignment for the
fully symmetric (M=N) channel with an arbitrary number of users.
An implication of our results is that the total degrees of freedom available
in a K-user interference channel, using only spatial diversity from the
multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees
of freedom shown to be possible by Cadambe and Jafar with arbitrarily large
time or frequency diversity.
Moving beyond the question of feasibility, we additionally discuss
computation of the number of solutions using Schubert calculus in cases where
there are a finite number of solutions."
Guy Bresler,arXiv:1301.0068,https://arxiv.org/abs/1301.0068,"Abstract:  We present a framework for the design of optimal assembly algorithms for
shotgun sequencing under the criterion of complete reconstruction. We derive a
lower bound on the read length and the coverage depth required for
reconstruction in terms of the repeat statistics of the genome. Building on
earlier works, we design a de Brujin graph based assembly algorithm which can
achieve very close to the lower bound for repeat statistics of a wide range of
sequenced genomes, including the GAGE datasets. The results are based on a set
of necessary and sufficient conditions on the DNA sequence and the reads for
reconstruction. The conditions can be viewed as the shotgun sequencing analogue
of Ukkonen-Pevzner's necessary and sufficient conditions for Sequencing by
Hybridization."
Guy Bresler,arXiv:1203.6233,https://arxiv.org/abs/1203.6233,"Abstract:  DNA sequencing is the basic workhorse of modern day biology and medicine.
Shotgun sequencing is the dominant technique used: many randomly located short
fragments called reads are extracted from the DNA sequence, and these reads are
assembled to reconstruct the original sequence. A basic question is: given a
sequencing technology and the statistics of the DNA sequence, what is the
minimum number of reads required for reliable reconstruction? This number
provides a fundamental limit to the performance of {\em any} assembly
algorithm. For a simple statistical model of the DNA sequence and the read
process, we show that the answer admits a critical phenomena in the asymptotic
limit of long DNA sequences: if the read length is below a threshold,
reconstruction is impossible no matter how many reads are observed, and if the
read length is above the threshold, having enough reads to cover the DNA
sequence is sufficient to reconstruct. The threshold is computed in terms of
the Renyi entropy rate of the DNA sequence. We also study the impact of noise
in the read process on the performance."
Guy Bresler,arXiv:1110.5092,https://arxiv.org/abs/1110.5092,"Abstract:  This paper studies vector space interference alignment for the three-user
MIMO interference channel with no time or frequency diversity. The main result
is a characterization of the feasibility of interference alignment in the
symmetric case where all transmitters have M antennas and all receivers have N
antennas. If N >= M and all users desire d transmit dimensions, then alignment
is feasible if and only if (2r+1)d <= max(rN,(r+1)M) for all nonnegative
integers r. The analogous result holds with M and N switched if M >= N.
It turns out that, just as for the 3-user parallel interference channel
\cite{BT09}, the length of alignment paths captures the essence of the problem.
In fact, for each feasible value of M and N the maximum alignment path length
dictates both the converse and achievability arguments.
One of the implications of our feasibility criterion is that simply counting
equations and comparing to the number of variables does not predict
feasibility. Instead, a more careful investigation of the geometry of the
alignment problem is required. The necessary condition obtained by counting
equations is implied by our new feasibility criterion."
Guy Bresler,arXiv:1104.0888,https://arxiv.org/abs/1104.0888,"Abstract:  Determining the feasibility conditions for vector space interference
alignment in the K-user MIMO interference channel with constant channel
coefficients has attracted much recent attention yet remains unsolved. The main
result of this paper is restricted to the symmetric square case where all
transmitters and receivers have N antennas, and each user desires d transmit
dimensions. We prove that alignment is possible if and only if the number of
antennas satisfies N>= d(K+1)/2. We also show a necessary condition for
feasibility of alignment with arbitrary system parameters. An algebraic
geometry approach is central to the results."
Guy Bresler,arXiv:0812.2265,https://arxiv.org/abs/0812.2265,"Abstract:  Exponential random graphs are used extensively in the sociology literature.
This model seeks to incorporate in random graphs the notion of reciprocity,
that is, the larger than expected number of triangles and other small
subgraphs. Sampling from these distributions is crucial for parameter
estimation hypothesis testing, and more generally for understanding basic
features of the network model itself. In practice sampling is typically carried
out using Markov chain Monte Carlo, in particular either the Glauber dynamics
or the Metropolis-Hasting procedure.
In this paper we characterize the high and low temperature regimes of the
exponential random graph model. We establish that in the high temperature
regime the mixing time of the Glauber dynamics is $\Theta(n^2 \log n)$, where
$n$ is the number of vertices in the graph; in contrast, we show that in the
low temperature regime the mixing is exponentially slow for any local Markov
chain. Our results, moreover, give a rigorous basis for criticisms made of such
models. In the high temperature regime, where sampling with MCMC is possible,
we show that any finite collection of edges are asymptotically independent;
thus, the model does not possess the desired reciprocity property, and is not
appreciably different from the Erds-Rnyi random graph."
Guy Bresler,arXiv:0809.3554,https://arxiv.org/abs/0809.3554,"Abstract:  Recently, Etkin, Tse, and Wang found the capacity region of the two-user
Gaussian interference channel to within one bit/s/Hz. A natural goal is to
apply this approach to the Gaussian interference channel with an arbitrary
number of users. We make progress towards this goal by finding the capacity
region of the many-to-one and one-to-many Gaussian interference channels to
within a constant number of bits. The result makes use of a deterministic model
to provide insight into the Gaussian channel. The deterministic model makes
explicit the dimension of signal scale. A central theme emerges: the use of
lattice codes for alignment of interfering signals on the signal scale."
Guy Bresler,arXiv:0807.3222,https://arxiv.org/abs/0807.3222,"Abstract:  This paper explores the two-user Gaussian interference channel through the
lens of a natural deterministic channel model. The main result is that the
deterministic channel uniformly approximates the Gaussian channel, the capacity
regions differing by a universal constant. The problem of finding the capacity
of the Gaussian channel to within a constant error is therefore reduced to that
of finding the capacity of the far simpler deterministic channel. Thus, the
paper provides an alternative derivation of the recent constant gap capacity
characterization of Etkin, Tse, and Wang. Additionally, the deterministic model
gives significant insight towards the Gaussian channel."
Guy Bresler,arXiv:0712.1402,https://arxiv.org/abs/0712.1402,"Abstract:  Markov random fields are used to model high dimensional distributions in a
number of applied areas. Much recent interest has been devoted to the
reconstruction of the dependency structure from independent samples from the
Markov random fields. We analyze a simple algorithm for reconstructing the
underlying graph defining a Markov random field on $n$ nodes and maximum degree
$d$ given observations. We show that under mild non-degeneracy conditions it
reconstructs the generating graph with high probability using $\Theta(d
\epsilon^{-2}\delta^{-4} \log n)$ samples where $\epsilon,\delta$ depend on the
local interactions. For most local interaction $\eps,\delta$ are of order
$\exp(-O(d))$.
Our results are optimal as a function of $n$ up to a multiplicative constant
depending on $d$ and the strength of the local interactions. Our results seem
to be the first results for general models that guarantee that {\em the}
generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}
\epsilon^{-2}\delta^{-4} \log n)$ running time bound. In cases where the
measure on the graph has correlation decay, the running time is $O(n^2 \log n)$
for all fixed $d$. We also discuss the effect of observing noisy samples and
show that as long as the noise level is low, our algorithm is effective. On the
other hand, we construct an example where large noise implies
non-identifiability even for generic noise and interactions. Finally, we
briefly show that in some simple cases, models with hidden nodes can also be
recovered."
Tamara Broderick,arXiv:1811.11790,https://arxiv.org/abs/1811.11790,"Abstract:  Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring
the underlying expression patterns of individual cells in favor of a global
average. Thanks to technological advances, we can now profile gene expression
across thousands or millions of individual cells in parallel. This new type of
data has led to the intriguing discovery that individual cell profiles can
reflect the imprint of time or dynamic processes. However, synthesizing this
information to reconstruct dynamic biological phenomena from data that are
noisy, heterogenous, and sparse---and from processes that may unfold
asynchronously---poses a complex computational and statistical challenge. Here,
we develop a full generative model for probabilistically reconstructing trees
of cellular differentiation from single-cell RNA-seq data. Specifically, we
extend the framework of the classical Dirichlet diffusion tree to
simultaneously infer branch topology and latent cell states along continuous
trajectories over the full tree. In tandem, we construct a novel Markov chain
Monte Carlo sampler that interleaves Metropolis-Hastings and message passing to
leverage model structure for efficient inference. Finally, we demonstrate that
these techniques can recover latent trajectories from simulated single-cell
transcriptomes. While this work is motivated by cellular differentiation, we
derive a tractable model that provides flexible densities for any data (coupled
with an appropriate noise model) that arise from continuous evolution along a
latent nonparametric tree."
Tamara Broderick,arXiv:1810.06587,https://arxiv.org/abs/1810.06587,"Abstract:  A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. A Bayesian nonparametric
(BNP) model addresses this question by placing a generative process on cluster
assignment. However, like all Bayesian approaches, BNP requires the
specification of a prior. In practice, it is important to quantitatively
establish that the prior is not too informative, particularly when the
particular form of the prior is chosen for mathematical convenience rather than
because of a considered subjective belief.
We derive local sensitivity measures for a truncated variational Bayes (VB)
approximation and approximate nonlinear dependence of a VB optimum on prior
parameters using a local Taylor series approximation. Using a stick-breaking
representation of a Dirichlet process, we consider perturbations both to the
scalar concentration parameter and to the functional form of the stick-
breaking distribution.
Unlike previous work on local Bayesian sensitivity for BNP, we pay special
attention to the ability of our sensitivity measures to extrapolate to
different priors, rather than treating the sensitivity as a measure of
robustness per se. Extrapolation motivates the use of multiplicative
perturbations to the functional form of the prior for VB. Additionally, we
linearly approximate only the computationally intensive part of inference --
the optimization of the global parameters -- and retain the nonlinearity of
easily computed quantities as functions of the global parameters.
We apply our methods to estimate sensitivity of the expected number of
distinct clusters present in the Iris dataset to the BNP prior specification.
We evaluate the accuracy of our approximations by comparing to the much more
expensive process of re-fitting the model."
Tamara Broderick,arXiv:1810.04249,https://arxiv.org/abs/1810.04249,"Abstract:  Kernel methods offer the flexibility to learn complex relationships in
modern, large data sets while enjoying strong theoretical guarantees on
quality. Unfortunately, these methods typically require cubic running time in
the data set size, a prohibitive cost in the large-data setting. Random feature
maps (RFMs) and the Nystrom method both consider low-rank approximations to the
kernel matrix as a potential solution. But, in order to achieve desirable
theoretical guarantees, the former may require a prohibitively large number of
features J+, and the latter may be prohibitively expensive for high-dimensional
problems. We propose to combine the simplicity and generality of RFMs with a
data-dependent feature selection scheme to achieve desirable theoretical
approximation properties of Nystrom with just O(log J+) features. Our key
insight is to begin with a large set of random features, then reduce them to a
small number of weighted features in a data-dependent, computationally
efficient way, while preserving the statistical guarantees of using the
original large set of features. We demonstrate the efficacy of our method with
theory and experiments--including on a data set with over 50 million
observations. In particular, we show that our method achieves small kernel
matrix approximation error and better test set accuracy with provably fewer
random features than state- of-the-art methods."
Tamara Broderick,arXiv:1809.09505,https://arxiv.org/abs/1809.09505,"Abstract:  Bayesian inference typically requires the computation of an approximation to
the posterior distribution. An important requirement for an approximate
Bayesian inference algorithm is to output high-accuracy posterior mean and
uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain
Monte Carlo, remain the gold standard for approximate Bayesian inference
because they have a robust finite-sample theory and reliable convergence
diagnostics. However, alternative methods, which are more scalable or apply to
problems where Markov Chain Monte Carlo cannot be used, lack the same
finite-data approximation theory and tools for evaluating their accuracy. In
this work, we develop a flexible new approach to bounding the error of mean and
uncertainty estimates of scalable inference algorithms. Our strategy is to
control the estimation errors in terms of Wasserstein distance, then bound the
Wasserstein distance via a generalized notion of Fisher distance. Unlike
computing the Wasserstein distance, which requires access to the normalized
posterior distribution, the Fisher distance is tractable to compute because it
requires access only to the gradient of the log posterior density. We
demonstrate the usefulness of our Fisher distance approach by deriving bounds
on the Wasserstein error of the Laplace approximation and Hilbert coresets. We
anticipate that our approach will be applicable to many other approximate
inference methods such as the integrated Laplace approximation, variational
inference, and approximate Bayesian computation"
Tamara Broderick,arXiv:1806.10234,https://arxiv.org/abs/1806.10234,"Abstract:  Gaussian processes (GPs) offer a flexible class of priors for nonparametric
Bayesian regression, but popular GP posterior inference methods are typically
prohibitively slow or lack desirable finite-data guarantees on quality. We
develop an approach to scalable approximate GP regression with finite-data
guarantees on the accuracy of pointwise posterior mean and variance estimates.
Our main contribution is a novel objective for approximate inference in the
nonparametric setting: the preconditioned Fisher (pF) divergence. We show that
unlike the Kullback--Leibler divergence (used in variational inference), the pF
divergence bounds the 2-Wasserstein distance, which in turn provides tight
bounds the pointwise difference of the mean and variance functions. We
demonstrate that, for sparse GP likelihood approximations, we can minimize the
pF divergence efficiently. Our experiments show that optimizing the pF
divergence has the same computational requirements as variational sparse GPs
while providing comparable empirical performance--in addition to our novel
finite-data quality guarantees."
Tamara Broderick,arXiv:1806.00550,https://arxiv.org/abs/1806.00550,"Abstract:  The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data. The ubiquitous tools of cross-validation (CV) and the bootstrap are
examples of this technique. These methods are powerful in large part due to
their model agnosticism but can be slow to run on modern, large data sets due
to the need to repeatedly re-fit the model. In this work, we use a linear
approximation to the dependence of the fitting procedure on the weights,
producing results that can be faster than repeated re-fitting by orders of
magnitude. This linear approximation is sometimes known as the ""infinitesimal
jackknife"" in the statistics literature, where it is mostly used to as a
theoretical tool to prove asymptotic results. We provide explicit finite-sample
error bounds for the infinitesimal jackknife in terms of a small number of
simple, verifiable assumptions. Our results apply whether the weights and data
are stochastic, deterministic, or even adversarially chosen, and so can be used
as a tool for proving the accuracy of the infinitesimal jackknife on a wide
variety of problems. As a corollary, we state mild regularity conditions under
which our approximation consistently estimates true leave-k-out
cross-validation for any fixed k. These theoretical results, together with
modern automatic differentiation software, support the application of the
infinitesimal jackknife to a wide variety of practical problems in machine
learning, providing a ""Swiss Army infinitesimal jackknife."" We demonstrate the
accuracy of our methods on a range of simulated and real datasets."
Tamara Broderick,arXiv:1803.05554,https://arxiv.org/abs/1803.05554,"Abstract:  Learning a Bayesian network (BN) from data can be useful for decision-making
or discovering causal relationships. However, traditional methods often fail in
modern applications, which exhibit a larger number of observed variables than
data points. The resulting uncertainty about the underlying network as well as
the desire to incorporate prior information recommend a Bayesian approach to
learning the BN, but the highly combinatorial structure of BNs poses a striking
challenge for inference. The current state-of-the-art methods such as order
MCMC are faster than previous methods but prevent the use of many natural
structural priors and still have running time exponential in the maximum
indegree of the true directed acyclic graph (DAG) of the BN. We here propose an
alternative posterior approximation based on the observation that, if we
incorporate empirical conditional independence tests, we can focus on a
high-probability DAG associated with each order of the vertices. We show that
our method allows the desired flexibility in prior specification, removes
timing dependence on the maximum indegree and yields provably good posterior
approximations; in addition, we show that it achieves superior accuracy,
scalability, and sampler mixing on several datasets."
Tamara Broderick,arXiv:1802.01737,https://arxiv.org/abs/1802.01737,"Abstract:  Coherent uncertainty quantification is a key strength of Bayesian methods.
But modern algorithms for approximate Bayesian posterior inference often
sacrifice accurate posterior uncertainty estimation in the pursuit of
scalability. This work shows that previous Bayesian coreset construction
algorithms---which build a small, weighted subset of the data that approximates
the full dataset---are no exception. We demonstrate that these algorithms scale
the coreset log-likelihood suboptimally, resulting in underestimated posterior
uncertainty. To address this shortcoming, we develop greedy iterative geodesic
ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales
the coreset log-likelihood optimally. GIGA provides geometric decay in
posterior approximation error as a function of coreset size, and maintains the
fast running time of its predecessors. The paper concludes with validation of
GIGA on both synthetic and real datasets, demonstrating that it reduces
posterior approximation error by orders of magnitude compared with previous
coreset constructions."
Tamara Broderick,arXiv:1712.01435,https://arxiv.org/abs/1712.01435,"Abstract:  Clustering procedures typically estimate which data points are clustered
together, a quantity of primary importance in many analyses. Often used as a
preliminary step for dimensionality reduction or to facilitate interpretation,
finding robust and stable clusters is often crucial for appropriate for
downstream analysis. In the present work, we consider Bayesian nonparametric
(BNP) models, a particularly popular set of Bayesian models for clustering due
to their flexibility. Because of its complexity, the Bayesian posterior often
cannot be computed exactly, and approximations must be employed. Mean-field
variational Bayes forms a posterior approximation by solving an optimization
problem and is widely used due to its speed. An exact BNP posterior might vary
dramatically when presented with different data. As such, stability and
robustness of the clustering should be assessed.
A popular mean to assess stability is to apply the bootstrap by resampling
the data, and rerun the clustering for each simulated data set. The time cost
is thus often very expensive, especially for the sort of exploratory analysis
where clustering is typically used. We propose to use a fast and automatic
approximation to the full bootstrap called the ""linear bootstrap"", which can be
seen by local data perturbation. In this work, we demonstrate how to apply this
idea to a data analysis pipeline, consisting of an MFVB approximation to a BNP
clustering posterior of time course gene expression data. We show that using
auto-differentiation tools, the necessary calculations can be done
automatically, and that the linear bootstrap is a fast but approximate
alternative to the bootstrap."
Tamara Broderick,arXiv:1710.05053,https://arxiv.org/abs/1710.05053,"Abstract:  The automation of posterior inference in Bayesian data analysis has enabled
experts and nonexperts alike to use more sophisticated models, engage in faster
exploratory modeling and analysis, and ensure experimental reproducibility.
However, standard automated posterior inference algorithms are not tractable at
the scale of massive modern datasets, and modifications to make them so are
typically model-specific, require expert tuning, and can break theoretical
guarantees on inferential quality. Building on the Bayesian coresets framework,
this work instead takes advantage of data redundancy to shrink the dataset
itself as a preprocessing step, providing fully-automated, scalable Bayesian
inference with theoretical guarantees. We begin with an intuitive reformulation
of Bayesian coreset construction as sparse vector sum approximation, and
demonstrate that its automation and performance-based shortcomings arise from
the use of the supremum norm. To address these shortcomings we develop Hilbert
coresets, i.e., Bayesian coresets constructed under a norm induced by an
inner-product on the log-likelihood function space. We propose two Hilbert
coreset construction algorithms---one based on importance sampling, and one
based on the Frank-Wolfe algorithm---along with theoretical guarantees on
approximation quality as a function of coreset size. Since the exact
computation of the proposed inner-products is model-specific, we automate the
construction with a random finite-dimensional projection of the log-likelihood
functions. The resulting automated coreset construction algorithm is simple to
implement, and experiments on a variety of models with real and synthetic
datasets show that it provides high-quality posterior approximations and a
significant reduction in the computational cost of inference."
Tamara Broderick,arXiv:1709.09216,https://arxiv.org/abs/1709.09216,"Abstract:  Generalized linear models (GLMs) -- such as logistic regression, Poisson
regression, and robust regression -- provide interpretable models for diverse
data types. Probabilistic approaches, particularly Bayesian ones, allow
coherent estimates of uncertainty, incorporation of prior information, and
sharing of power across experiments via hierarchical models. In practice,
however, the approximate Bayesian methods necessary for inference have either
failed to scale to large data sets or failed to provide theoretical guarantees
on the quality of inference. We propose a new approach based on constructing
polynomial approximate sufficient statistics for GLMs (PASS-GLM). We
demonstrate that our method admits a simple algorithm as well as trivial
streaming and distributed extensions that do not compound error across
computations. We provide theoretical guarantees on the quality of point (MAP)
estimates, the approximate posterior, and posterior mean and uncertainty
estimates. We validate our approach empirically in the case of logistic
regression using a quadratic approximation and show competitive performance
with stochastic gradient descent, MCMC, and the Laplace approximation in terms
of speed and multiple measures of accuracy -- including on an advertising data
set with 40 million data points and 20,000 covariates."
Tamara Broderick,arXiv:1709.02536,https://arxiv.org/abs/1709.02536,"Abstract:  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC."
Tamara Broderick,arXiv:1612.05519,https://arxiv.org/abs/1612.05519,"Abstract:  Many popular network models rely on the assumption of (vertex)
exchangeability, in which the distribution of the graph is invariant to
relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that
these graphs are dense or empty with probability one, whereas many real-world
graphs are sparse. We present an alternative notion of exchangeability for
random graphs, which we call edge exchangeability, in which the distribution of
a graph sequence is invariant to the order of the edges. We demonstrate that
edge-exchangeable models, unlike models that are traditionally vertex
exchangeable, can exhibit sparsity. To do so, we outline a general framework
for graph generative models; by contrast to the pioneering work of Caron and
Fox (2015), models within our framework are stationary across steps of the
graph sequence. In particular, our model grows the graph by instantiating more
latent atoms of a single random measure as the dataset size increases, rather
than adding new atoms to the measure."
Tamara Broderick,arXiv:1611.07469,https://arxiv.org/abs/1611.07469,"Abstract:  In Bayesian analysis, the posterior follows from the data and a choice of a
prior and a likelihood. One hopes that the posterior is robust to reasonable
variation in the choice of prior, since this choice is made by the modeler and
is often somewhat subjective. A different, equally subjectively plausible
choice of prior may result in a substantially different posterior, and so
different conclusions drawn from the data. Were this to be the case, our
conclusions would not be robust to the choice of prior. To determine whether
our model is robust, we must quantify how sensitive our posterior is to
perturbations of our prior. Despite the importance of the problem and a
considerable body of literature, generic, easy-to-use methods to quantify
Bayesian robustness are still lacking.
Abstract In this paper, we demonstrate that powerful measures of robustness
can be easily calculated from Variational Bayes (VB) approximate posteriors. We
begin with local robustness, which measures the effect of infinitesimal changes
to the prior on a posterior mean of interest. In particular, we show that the
influence function of Gustafson (2012) has a simple, easy-to-calculate closed
form expression for VB approximations. We then demonstrate how local robustness
measures can be inadequate for non-local prior changes, such as replacing one
prior entirely with another. We propose a simple approximate non-local
robustness measure and demonstrate its effectiveness on a simulated data set."
Tamara Broderick,arXiv:1611.05559,https://arxiv.org/abs/1611.05559,"Abstract:  Variational inference (VI) provides fast approximations of a Bayesian
posterior in part because it formulates posterior approximation as an
optimization problem: to find the closest distribution to the exact posterior
over some family of distributions. For practical reasons, the family of
distributions in VI is usually constrained so that it does not include the
exact posterior, even as a limit point. Thus, no matter how long VI is run, the
resulting approximation will not approach the exact posterior. We propose to
instead consider a more flexible approximating family consisting of all
possible finite mixtures of a parametric base distribution (e.g., Gaussian).
For efficient inference, we borrow ideas from gradient boosting to develop an
algorithm we call boosting variational inference (BVI). BVI iteratively
improves the current approximation by mixing it with a new component from the
base distribution family and thereby yields progressively more accurate
posterior approximations as more computing time is spent. Unlike a number of
common VI variants including mean-field VI, BVI is able to capture
multimodality, general posterior covariance, and nonstandard posterior shapes."
Tamara Broderick,arXiv:1609.09147,https://arxiv.org/abs/1609.09147,"Abstract:  Trait allocations are a class of combinatorial structures in which data may
belong to multiple groups and may have different levels of belonging in each
group. Often the data are also exchangeable, i.e., their joint distribution is
invariant to reordering. In clustering---a special case of trait
allocation---exchangeability implies the existence of both a de Finetti
representation and an exchangeable partition probability function (EPPF),
distributional representations useful for computational and theoretical
purposes. In this work, we develop the analogous de Finetti representation and
exchangeable trait probability function (ETPF) for trait allocations, along
with a characterization of all trait allocations with an ETPF. Unlike previous
feature allocation characterizations, our proofs fully capture
single-occurrence ""dust"" groups. We further introduce a novel constrained
version of the ETPF that we use to establish an intuitive connection between
the probability functions for clustering, feature allocations, and trait
allocations. As an application of our general theory, we characterize the
distribution of all edge-exchangeable graphs, a class of recently-developed
models that captures realistic sparse graph sequences."
Tamara Broderick,arXiv:1606.07153,https://arxiv.org/abs/1606.07153,"Abstract:  Bayesian hierarchical models are increasing popular in economics. When using
hierarchical models, it is useful not only to calculate posterior expectations,
but also to measure the robustness of these expectations to reasonable
alternative prior choices. We use variational Bayes and linear response methods
to provide fast, accurate posterior means and robustness measures with an
application to measuring the effectiveness of microcredit in the developing
world."
Tamara Broderick,arXiv:1605.06423,https://arxiv.org/abs/1605.06423,"Abstract:  The use of Bayesian methods in large-scale data settings is attractive
because of the rich hierarchical models, uncertainty quantification, and prior
specification they provide. Standard Bayesian inference algorithms are
computationally expensive, however, making their direct application to large
datasets difficult or infeasible. Recent work on scaling Bayesian inference has
focused on modifying the underlying algorithms to, for example, use only a
random data subsample at each iteration. We leverage the insight that data is
often redundant to instead obtain a weighted subset of the data (called a
coreset) that is much smaller than the original dataset. We can then use this
small coreset in any number of existing posterior inference algorithms without
modification. In this paper, we develop an efficient coreset construction
algorithm for Bayesian logistic regression models. We provide theoretical
guarantees on the size and approximation quality of the coreset -- both for
fixed, known datasets, and in expectation for a wide class of data generative
models. Crucially, the proposed approach also permits efficient construction of
the coreset in both streaming and parallel settings, with minimal additional
effort. We demonstrate the efficacy of our approach on a number of synthetic
and real-world datasets, and find that, in practice, the size of the coreset is
independent of the original dataset size. Furthermore, constructing the coreset
takes a negligible amount of time compared to that required to run MCMC on it."
Tamara Broderick,arXiv:1603.06915,https://arxiv.org/abs/1603.06915,"Abstract:  Network data appear in a number of applications, such as online social
networks and biological networks, and there is growing interest in both
developing models for networks as well as studying the properties of such data.
Since individual network datasets continue to grow in size, it is necessary to
develop models that accurately represent the real-life scaling properties of
networks. One behavior of interest is having a power law in the degree
distribution. However, other types of power laws that have been observed
empirically and considered for applications such as clustering and feature
allocation models have not been studied as frequently in models for graph data.
In this paper, we enumerate desirable asymptotic behavior that may be of
interest for modeling graph data, including sparsity and several types of power
laws. We outline a general framework for graph generative models using
completely random measures; by contrast to the pioneering work of Caron and Fox
(2015), we consider instantiating more of the existing atoms of the random
measure as the dataset size increases rather than adding new atoms to the
measure. We see that these two models can be complementary; they respectively
yield interpretations as (1) time passing among existing members of a network
and (2) new individuals joining a network. We detail a particular instance of
this framework and show simulated results that suggest this model exhibits some
desirable asymptotic power-law behavior."
Tamara Broderick,arXiv:1603.06898,https://arxiv.org/abs/1603.06898,"Abstract:  A known failing of many popular random graph models is that the Aldous-Hoover
Theorem guarantees these graphs are dense with probability one; that is, the
number of edges grows quadratically with the number of nodes. This behavior is
considered unrealistic in observed graphs. We define a notion of edge
exchangeability for random graphs in contrast to the established notion of
infinite exchangeability for random graphs --- which has traditionally relied
on exchangeability of nodes (rather than edges) in a graph. We show that,
unlike node exchangeability, edge exchangeability encompasses models that are
known to provide a projective sequence of random graphs that circumvent the
Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the
number of edges with the number of nodes. We show how edge-exchangeability of
graphs relates naturally to existing notions of exchangeability from clustering
(a.k.a. partitions) and other familiar combinatorial structures."
Tamara Broderick,arXiv:1603.00861,https://arxiv.org/abs/1603.00861,"Abstract:  Completely random measures (CRMs) and their normalizations are a rich source
of Bayesian nonparametric priors. Examples include the beta, gamma, and
Dirichlet processes. In this paper we detail two major classes of sequential
CRM representations---series representations and superposition
representations---within which we organize both novel and existing sequential
representations that can be used for simulation and posterior inference. These
two classes and their constituent representations subsume existing ones that
have previously been developed in an ad hoc manner for specific processes.
Since a complete infinite-dimensional CRM cannot be used explicitly for
computation, sequential representations are often truncated for tractability.
We provide truncation error analyses for each type of sequential
representation, as well as their normalized versions, thereby generalizing and
improving upon existing truncation error bounds in the literature. We analyze
the computational complexity of the sequential representations, which in
conjunction with our error bounds allows us to directly compare representations
and discuss their relative efficiency. We include numerous applications of our
theoretical results to commonly-used (normalized) CRMs, demonstrating that our
results enable a straightforward representation and analysis of CRMs that has
not previously been available in a Bayesian nonparametric context."
Tamara Broderick,arXiv:1512.02578,https://arxiv.org/abs/1512.02578,"Abstract:  In Bayesian analysis, the posterior follows from the data and a choice of a
prior and a likelihood. One hopes that the posterior is robust to reasonable
variation in the choice of prior and likelihood, since this choice is made by
the modeler and is necessarily somewhat subjective. Despite the fundamental
importance of the problem and a considerable body of literature, the tools of
robust Bayes are not commonly used in practice. This is in large part due to
the difficulty of calculating robustness measures from MCMC draws. Although
methods for computing robustness measures from MCMC draws exist, they lack
generality and often require additional coding or computation.
In contrast to MCMC, variational Bayes (VB) techniques are readily amenable
to robustness analysis. The derivative of a posterior expectation with respect
to a prior or data perturbation is a measure of local robustness to the prior
or likelihood. Because VB casts posterior inference as an optimization problem,
its methodology is built on the ability to calculate derivatives of posterior
quantities with respect to model parameters, even in very complex models. In
the present work, we develop local prior robustness measures for mean-field
variational Bayes(MFVB), a VB technique which imposes a particular
factorization assumption on the variational posterior approximation. We start
by outlining existing local prior measures of robustness. Next, we use these
results to derive closed-form measures of the sensitivity of mean-field
variational posterior approximation to prior specification. We demonstrate our
method on a meta-analysis of randomized controlled interventions in access to
microcredit in developing countries."
Tamara Broderick,arXiv:1512.01229,https://arxiv.org/abs/1512.01229,"Abstract:  This article is a translation of Bruno de Finetti's paper ""Funzione
Caratteristica di un fenomeno aleatorio"" which appeared in Atti del Congresso
Internazionale dei Matematici, Bologna 3-10 Settembre 1928, Tomo VI, pp.
179-190, originally published by Nicola Zanichelli Editore S.p.A. The
translation was made as close as possible to the original in form and style,
except for apparent mistakes found in the original document, which were
corrected and are mentioned as footnotes. Most of these were resolved by
comparing against a longer version of this work by de Finetti, published
shortly after this one under the same titlea. The interested reader is highly
encouraged to consult this other version for a more detailed treatment of the
topics covered here. Footnotes regarding the translation are labeled with
letters to distinguish them from de Finetti's original footnotes."
Tamara Broderick,arXiv:1506.04088,https://arxiv.org/abs/1506.04088,"Abstract:  Mean field variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is that it underestimates the uncertainty of
model variables (sometimes severely) and provides no information about model
variable covariance.
We generalize linear response methods from statistical physics to deliver
accurate uncertainty estimates for model variables---both for individual
variables and coherently across variables. We call our method linear response
variational Bayes (LRVB). When the MFVB posterior approximation is in the
exponential family, LRVB has a simple, analytic form, even for non-conjugate
models. Indeed, we make no assumptions about the form of the true posterior. We
demonstrate the accuracy and scalability of our method on a range of models for
both simulated and real data."
Tamara Broderick,arXiv:1502.07685,https://arxiv.org/abs/1502.07685,"Abstract:  Mean field variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is that it underestimates the uncertainty of
model variables (sometimes severely) and provides no information about model
variable covariance. We develop a fast, general methodology for exponential
families that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We also show how LRVB can be
used to quickly calculate a measure of the influence of individual data points
on parameter point estimates. We demonstrate the accuracy and scalability of
our method by learning Gaussian mixture models for both simulated and real
data."
Tamara Broderick,arXiv:1410.6853,https://arxiv.org/abs/1410.6853,"Abstract:  Mean Field Variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is its (sometimes severe) underestimates of
the uncertainty of model variables and lack of information about model variable
covariance. We develop a fast, general methodology for exponential families
that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We demonstrate the accuracy of
our method on simulated data sets."
Tamara Broderick,arXiv:1410.6843,https://arxiv.org/abs/1410.6843,"Abstract:  We demonstrate how to calculate posteriors for general CRM-based priors and
likelihoods for Bayesian nonparametric models. We further show how to represent
Bayesian nonparametric priors as a sequence of finite draws using a
size-biasing approach---and how to represent full Bayesian nonparametric models
via finite marginals. Motivated by conjugate priors based on exponential family
representations of likelihoods, we introduce a notion of exponential families
for CRMs, which we call exponential CRMs. This construction allows us to
specify automatic Bayesian nonparametric conjugate priors for exponential CRM
likelihoods. We demonstrate that our exponential CRMs allow particularly
straightforward recipes for size-biased and marginal representations of
Bayesian nonparametric models. Along the way, we prove that the gamma process
is a conjugate prior for the Poisson likelihood process and the beta prime
process is a conjugate prior for a process we call the odds Bernoulli process.
We deliver a size-biased representation of the gamma process and a marginal
representation of the gamma process coupled with a Poisson likelihood process."
Tamara Broderick,arXiv:1410.4792,https://arxiv.org/abs/1410.4792,"Abstract:  Bayesian entity resolution merges together multiple, noisy databases and
returns the minimal collection of unique individuals represented, together with
their true, latent record values. Bayesian methods allow flexible generative
models that share power across databases as well as principled quantification
of uncertainty for queries of the final, resolved database. However, existing
Bayesian methods for entity resolution use Markov monte Carlo method (MCMC)
approximations and are too slow to run on modern databases containing millions
or billions of records. Instead, we propose applying variational approximations
to allow scalable Bayesian inference in these models. We derive a
coordinate-ascent approximation for mean-field variational Bayes, qualitatively
compare our algorithm to existing methods, note unique challenges for inference
that arise from the expected distribution of cluster sizes in entity
resolution, and discuss directions for future work in this domain."
Tamara Broderick,arXiv:1307.8049,https://arxiv.org/abs/1307.8049,"Abstract:  Research on distributed machine learning algorithms has focused primarily on
one of two extremes - algorithms that obey strict concurrency constraints or
algorithms that obey few or no such constraints. We consider an intermediate
alternative in which algorithms optimistically assume that conflicts are
unlikely and if conflicts do arise a conflict-resolution protocol is invoked.
We view this ""optimistic concurrency control"" paradigm as particularly
appropriate for large-scale machine learning algorithms, particularly in the
unsupervised setting. We demonstrate our approach in three problem areas:
clustering, feature learning and online facility location. We evaluate our
methods via large-scale experiments in a cluster computing environment."
Tamara Broderick,arXiv:1307.6769,https://arxiv.org/abs/1307.6769,"Abstract:  We present SDA-Bayes, a framework for (S)treaming, (D)istributed,
(A)synchronous computation of a Bayesian posterior. The framework makes
streaming updates to the estimated posterior according to a user-specified
approximation batch primitive. We demonstrate the usefulness of our framework,
with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet
allocation model to two large-scale document collections. We demonstrate the
advantages of our algorithm over stochastic variational inference (SVI) by
comparing the two after a single pass through a known amount of data---a case
where SVI may be applied---and in the streaming setting, where SVI does not
apply."
Tamara Broderick,arXiv:1301.6647,https://arxiv.org/abs/1301.6647,"Abstract:  The problem of inferring a clustering of a data set has been the subject of
much research in Bayesian analysis, and there currently exists a solid
mathematical foundation for Bayesian approaches to clustering. In particular,
the class of probability distributions over partitions of a data set has been
characterized in a number of ways, including via exchangeable partition
probability functions (EPPFs) and the Kingman paintbox. Here, we develop a
generalization of the clustering problem, called feature allocation, where we
allow each data point to belong to an arbitrary, non-negative integer number of
groups, now called features or topics. We define and study an ""exchangeable
feature probability function"" (EFPF)---analogous to the EPPF in the clustering
setting---for certain types of feature models. Moreover, we introduce a
""feature paintbox"" characterization---analogous to the Kingman paintbox for
clustering---of the class of exchangeable feature models. We provide a further
characterization of the subclass of feature allocations that have EFPF
representations."
Tamara Broderick,arXiv:1212.2126,https://arxiv.org/abs/1212.2126,"Abstract:  The classical mixture of Gaussians model is related to K-means via
small-variance asymptotics: as the covariances of the Gaussians tend to zero,
the negative log-likelihood of the mixture of Gaussians model approaches the
K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis
& Jordan (2012) used this observation to obtain a novel K-means-like algorithm
from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead
consider applying small-variance asymptotics directly to the posterior in
Bayesian nonparametric models. This framework is independent of any specific
Bayesian inference algorithm, and it has the major advantage that it
generalizes immediately to a range of models beyond the DP mixture. To
illustrate, we apply our framework to the feature learning setting, where the
beta process and Indian buffet process provide an appropriate Bayesian
nonparametric prior. We obtain a novel objective function that goes beyond
clustering to learn (and penalize new) groupings for which we relax the mutual
exclusivity and exhaustivity assumptions of clustering. We demonstrate several
other algorithms, all of which are scalable and simple to implement. Empirical
results demonstrate the benefits of the new framework."
Tamara Broderick,arXiv:1209.3550,https://arxiv.org/abs/1209.3550,"Abstract:  We develop algorithms for performing semiparametric regression analysis in
real time, with data processed as it is collected and made immediately
available via modern telecommunications technologies. Our definition of
semiparametric regression is quite broad and includes, as special cases,
generalized linear mixed models, generalized additive models, geostatistical
models, wavelet nonparametric regression models and their various combinations.
Fast updating of regression fits is achieved by couching semiparametric
regression into a Bayesian hierarchical model or, equivalently, graphical model
framework and employing online mean field variational ideas. An internet site
attached to this article, realtime-semiparametric-regression.net, illustrates
the methodology for continually arriving stock market, real estate and airline
data. Flexible real-time analyses, based on increasingly ubiquitous streaming
data sources stand to benefit."
Tamara Broderick,arXiv:1206.5862,https://arxiv.org/abs/1206.5862,"Abstract:  One of the focal points of the modern literature on Bayesian nonparametrics
has been the problem of clustering, or partitioning, where each data point is
modeled as being associated with one and only one of some collection of groups
called clusters or partition blocks. Underlying these Bayesian nonparametric
models are a set of interrelated stochastic processes, most notably the
Dirichlet process and the Chinese restaurant process. In this paper we provide
a formal development of an analogous problem, called feature modeling, for
associating data points with arbitrary nonnegative integer numbers of groups,
now called features or topics. We review the existing combinatorial stochastic
process representations for the clustering problem and develop analogous
representations for the feature modeling problem. These representations include
the beta process and the Indian buffet process as well as new representations
that provide insight into the connections between these processes. We thereby
bring the same level of completeness to the treatment of Bayesian nonparametric
feature modeling that has previously been achieved for Bayesian nonparametric
clustering."
Tamara Broderick,arXiv:1203.3486,https://arxiv.org/abs/1203.3486,"Abstract:  We introduce a new graphical model for tracking radio-tagged animals and
learning their movement patterns. The model provides a principled way to
combine radio telemetry data with an arbitrary set of userdefined, spatial
features. We describe an efficient stochastic gradient algorithm for fitting
model parameters to data and demonstrate its effectiveness via asymptotic
analysis and synthetic experiments. We also apply our model to real datasets,
and show that it outperforms the most popular radio telemetry software package
used in ecology. We conclude that integration of different data sources under a
single statistical framework, coupled with appropriate parameter and state
estimation procedures, produces both accurate location estimates and an
interpretable statistical model of animal movement."
Tamara Broderick,arXiv:1112.3654,https://arxiv.org/abs/1112.3654,"Abstract:  As the number of observed Gamma-Ray Bursts (GRBs) continues to grow,
follow-up resources need to be used more efficiently in order to maximize
science output from limited telescope time. As such, it is becoming
increasingly important to rapidly identify bursts of interest as soon as
possible after the event, before the afterglows fade beyond detectability.
Studying the most distant (highest redshift) events, for instance, remains a
primary goal for many in the field. Here we present our Random forest Automated
Triage Estimator for GRB redshifts (RATE GRB-z) for rapid identification of
high-redshift candidates using early-time metrics from the three telescopes
onboard Swift. While the basic RATE methodology is generalizable to a number of
resource allocation problems, here we demonstrate its utility for
telescope-constrained follow-up efforts with the primary goal to identify and
study high-z GRBs. For each new GRB, RATE GRB-z provides a recommendation -
based on the available telescope time - of whether the event warrants
additional follow-up resources. We train RATE GRB-z using a set consisting of
135 Swift bursts with known redshifts, only 18 of which are z > 4.
Cross-validated performance metrics on this training data suggest that ~56% of
high-z bursts can be captured from following up the top 20% of the ranked
candidates, and ~84% of high-z bursts are identified after following up the top
~40% of candidates. We further use the method to rank 200+ Swift bursts with
unknown redshifts according to their likelihood of being high-z."
Tamara Broderick,arXiv:1111.1802,https://arxiv.org/abs/1111.1802,"Abstract:  We develop a Bayesian nonparametric approach to a general family of latent
class problems in which individuals can belong simultaneously to multiple
classes and where each class can be exhibited multiple times by an individual.
We introduce a combinatorial stochastic process known as the negative binomial
process (NBP) as an infinite-dimensional prior appropriate for such problems.
We show that the NBP is conjugate to the beta process, and we characterize the
posterior distribution under the beta-negative binomial process (BNBP) and
hierarchical models based on the BNBP (the HBNBP). We study the asymptotic
properties of the BNBP and develop a three-parameter extension of the BNBP that
exhibits power-law behavior. We derive MCMC algorithms for posterior inference
under the HBNBP, and we present experiments using these algorithms in the
domains of image segmentation, object recognition, and document analysis."
Tamara Broderick,arXiv:1106.0539,https://arxiv.org/abs/1106.0539,"Abstract:  The beta-Bernoulli process provides a Bayesian nonparametric prior for models
involving collections of binary-valued features. A draw from the beta process
yields an infinite collection of probabilities in the unit interval, and a draw
from the Bernoulli process turns these into binary-valued features. Recent work
has provided stick-breaking representations for the beta process analogous to
the well-known stick-breaking representation for the Dirichlet process. We
derive one such stick-breaking representation directly from the
characterization of the beta process as a completely random measure. This
approach motivates a three-parameter generalization of the beta process, and we
study the power laws that can be obtained from this generalized beta process.
We present a posterior inference algorithm for the beta-Bernoulli process that
exploits the stick-breaking representation, and we present experimental results
for a discrete factor-analysis model."
Tamara Broderick,arXiv:0909.2450,https://arxiv.org/abs/0909.2450,"Abstract:  Selection methods that require only a single-switch input, such as a button
click or blink, are potentially useful for individuals with motor impairments,
mobile technology users, and individuals wishing to transmit information
securely. We present a single-switch selection method, ""Nomon,"" that is general
and efficient. Existing single-switch selection methods require selectable
options to be arranged in ways that limit potential applications. By contrast,
traditional operating systems, web browsers, and free-form applications (such
as drawing) place options at arbitrary points on the screen. Nomon, however,
has the flexibility to select any point on a screen. Nomon adapts automatically
to an individual's clicking ability; it allows a person who clicks precisely to
make a selection quickly and allows a person who clicks imprecisely more time
to make a selection without error. Nomon reaps gains in information rate by
allowing the specification of beliefs (priors) about option selection
probabilities and by avoiding tree-based selection schemes in favor of direct
(posterior) inference. We have developed both a Nomon-based writing application
and a drawing application. To evaluate Nomon's performance, we compared the
writing application with a popular existing method for single-switch writing
(row-column scanning). Novice users wrote 35% faster with the Nomon interface
than with the scanning interface. An experienced user (author TB, with > 10
hours practice) wrote at speeds of 9.3 words per minute with Nomon, using 1.2
clicks per character and making no errors in the final text."
Tamara Broderick,arXiv:0904.4891,https://arxiv.org/abs/0904.4891,"Abstract:  Recognizing the successes of treed Gaussian process (TGP) models as an
interpretable and thrifty model for nonparametric regression, we seek to extend
the model to classification. Both treed models and Gaussian processes (GPs)
have, separately, enjoyed great success in application to classification
problems. An example of the former is Bayesian CART. In the latter, real-valued
GP output may be utilized for classification via latent variables, which
provide classification rules by means of a softmax function. We formulate a
Bayesian model averaging scheme to combine these two models and describe a
Monte Carlo method for sampling from the full posterior distribution with joint
proposals for the tree topology and the GP parameters corresponding to latent
variables at the leaves. We concentrate on efficient sampling of the latent
variables, which is important to obtain good mixing in the expanded parameter
space. The tree structure is particularly helpful for this task and also for
developing an efficient scheme for handling categorical predictors, which
commonly arise in classification problems. Our proposed classification TGP
(CTGP) methodology is illustrated on a collection of synthetic and real data
sets. We assess performance relative to existing methods and thereby show how
CTGP is highly flexible, offers tractable inference, produces rules that are
easy to interpret, and performs well out of sample."
Tamara Broderick,arXiv:0712.2437,https://arxiv.org/abs/0712.2437,"Abstract:  Recent work has shown that probabilistic models based on pairwise
interactions-in the simplest case, the Ising model-provide surprisingly
accurate descriptions of experiments on real biological networks ranging from
neurons to genes. Finding these models requires us to solve an inverse problem:
given experimentally measured expectation values, what are the parameters of
the underlying Hamiltonian? This problem sits at the intersection of
statistical physics and machine learning, and we suggest that more efficient
solutions are possible by merging ideas from the two fields. We use a
combination of recent coordinate descent algorithms with an adaptation of the
histogram Monte Carlo method, and implement these techniques to take advantage
of the sparseness found in data on real neurons. The resulting algorithm learns
the parameters of an Ising model describing a network of forty neurons within a
few minutes. This opens the possibility of analyzing much larger data sets now
emerging, and thus testing hypotheses about the collective behaviors of these
networks."
Tamara Broderick,arXiv:astro-ph/0507108,https://arxiv.org/abs/astro-ph/0507108,"Abstract:  We present the results of attempts to detect the ellipticity of dark matter
halos using galaxy-galaxy weak lensing with SDSS data. We use 2,020,256
galaxies brighter than r=19 with photometric redshifts (divided into colour and
luminosity subsamples) as lenses and 31,697,869 source galaxies. We search for
and identify several signal contaminants, which if not removed lead to a
spurious detection. These include systematic shear that leads to a slight
spurious alignment of lens and source ellipticities, intrinsic alignments (due
to contamination of the source sample by physically-associated lens source
pairs), and anisotropic magnification bias. We develop methods that allow us to
remove these contaminants to the signal. We split the analysis into blue
(spiral) and red (elliptical) galaxies. Assuming Gaussian errors as in previous
work and a power-law profile, we find f_h=e_h/e_g=0.1+/-0.06 for red galaxies
and -0.8+/-0.4 for blue galaxies using 20-300 kpc/h, averaged over luminosity.
Inclusion of the more realistic non-Gaussian error distributions and of the NFW
density profile (which predicts much smaller ellipticity of the shear for
scales above the scale radius) yields 0.60+/-0.38 for ellipticals and
-1.4+1.7-2.0 for spirals. While there is no concrete detection of alignment in
either case, there is a suggestion in the data of a positive alignment in the
brightest lens sample of ellipticals. Our results appear to be mildly
inconsistent with a previously reported detection by Hoekstra et al. (2004),
but more data and further tests are needed to clarify whether the discrepancy
is real or a consequence of differences in the lens galaxy samples used and
analysis methods."
Tamara Broderick,arXiv:astro-ph/0402002,https://arxiv.org/abs/astro-ph/0402002,"Abstract:  We investigate the required redshift accuracy of type Ia supernova and
cluster number-count surveys in order for the redshift uncertainties not to
contribute appreciably to the dark energy parameter error budget. For the SNAP
supernova experiment, we find that, without the assistance of ground-based
measurements, individual supernova redshifts would need to be determined to
about 0.002 or better, which is a challenging but feasible requirement for a
low-resolution spectrograph. However, we find that accurate redshifts for z<0.1
supernovae, obtained with ground-based experiments, are sufficient to immunize
the results against even relatively large redshift errors at high z. For the
future cluster number-count surveys such as the South Pole Telescope, Planck or
DUET, we find that the purely statistical error in photometric redshift is less
important, and that the irreducible, systematic bias in redshift drives the
requirements. The redshift bias will have to be kept below 0.001-0.005 per
redshift bin (which is determined by the filter set), depending on the sky
coverage and details of the definition of the minimal mass of the survey.
Furthermore, we find that X-ray surveys have a more stringent required redshift
accuracy than Sunyaev-Zeldovich (SZ) effect surveys since they use a shorter
lever arm in redshift; conversely, SZ surveys benefit from their high redshift
reach only so long as some redshift information is available for distant (z>1)
clusters."
Rodney Brooks,arXiv:1710.10291,https://arxiv.org/abs/1710.10291,"Abstract:  The measurement problem and three other vexing experiments in quantum physics
are described. It is shown how Quantum Field Theory, as formulated by Julian
Schwinger, provides simple solutions for all four experiments. It is also shown
how this theory resolves many other problems of Quantum Mechanics and
Relativity, including a new and simple derivation of E = mc2."
Vladimir Bulovic,arXiv:1902.05973,https://arxiv.org/abs/1902.05973,"Abstract:  Lead halide-based perovskite thin films have attracted great attention due to
the explosive increase in perovskite solar cell efficiencies. The same
optoelectronic properties that make perovskites ideal absorber materials in
solar cells are also beneficial in other light-harvesting applications and make
them prime candidates as triplet sensitizers in upconversion via
triplet-triplet annihilation in rubrene. In this contribution, we take
advantage of long carrier lifetimes and carrier diffusion lengths in perovskite
thin films, their high absorption cross sections throughout the visible
spectrum, as well as the strong spin-orbit coupling owing to the abundance of
heavy atoms to sensitize the upconverter rubrene. Employing bulk perovskite
thin films as the absorber layer and spin-mixer in inorganic/organic
heterojunction upconversion devices allows us to forego the additional
tunneling barrier owing from the passivating ligands required for colloidal
sensitizers. Our bilayer device exhibits an upconversion efficiency in excess
of 3% under 785 nm illumination."
Vladimir Bulovic,arXiv:1901.08637,https://arxiv.org/abs/1901.08637,"Abstract:  Photon recycling is required for a solar cell to achieve an open-circuit
voltage ($V_{OC}$) and power conversion efficiency (PCE) approaching the
Shockley-Queisser theoretical limit. In metal halide perovskite solar cells,
the achievable performance gains from photon recycling remain uncertain due to
high variability in perovskite material quality and the non-radiative
recombination rate ($k_{1}$). In this work, we study state-of-the-art
$\textrm{Cs}_{0.05}(\textrm{MA}_{0.17}\textrm{FA}_{0.83})_{0.95}\textrm{Pb}(\textrm{I}_{0.83}\textrm{Br}_{0.17})_{3}$
films and analyze the impact of varying non-radiative recombination rates on
photon recycling and device performance. Importantly, we predict the impact of
photon recycling at the maximum power point (MPP), demonstrating an absolute
PCE increase of up to 2.0% in the radiative limit, primarily due to a 77 mV
increase in $V_{MPP}$. Even with finite non-radiative recombination, benefits
from photon recycling can be achieved when non-radiative lifetimes and external
LED electroluminescence efficiencies measured at open-circuit,
$Q_{e}^{LED}(\textrm{V}_{OC})$, exceed 2 $\mu$s and 10%, respectively. This
analysis clarifies the opportunity to fully exploit photon recycling to push
the real-world performance of perovskite solar cells toward theoretical limits."
Vladimir Bulovic,arXiv:1803.01192,https://arxiv.org/abs/1803.01192,"Abstract:  Halide perovskites are promising semiconductors for inexpensive,
high-performance optoelectronics. Despite a remarkable defect tolerance
compared to conventional semiconductors, perovskite thin films still show
substantial microscale heterogeneity in key properties such as luminescence
efficiency and device performance. This behavior has been attributed to spatial
fluctuations in the population of sub-bandgap electronic states that act as
trap-mediated non-radiative recombination sites. However, the origin of the
variations, trap states and extent of the defect tolerance remains a topic of
debate, and a precise understanding is critical to the rational design of
defect management strategies. By combining scanning X-ray diffraction beamlines
at two different synchrotrons with high-resolution transmission electron
microscopy, we reveal levels of heterogeneity on the ten-micrometer scale
(super-grains) and even ten-nanometer scale (sub-grain domains). We find that
local strain is associated with enhanced defect concentrations, and
correlations between the local structure and time-resolved photoluminescence
reveal that these strain-related defects are the cause of non-radiative
recombination. We reveal a direct connection between defect concentrations and
non-radiative losses, as well as complex heterogeneity across multiple length
scales, shedding new light on the presence and influence of structural defects
in halide perovskites."
Vladimir Bulovic,arXiv:1609.04643,https://arxiv.org/abs/1609.04643,"Abstract:  Unique optical properties of colloidal semiconductor quantum dots (QDs),
arising from quantum mechanical confinement of charge within these structures,
present a versatile testbed for the study of how high electric fields affect
the electronic structure of nanostructured solids. Earlier studies of quasi-DC
electric field modulation of QD properties have been limited by the
electrostatic breakdown processes under the high externally applied electric
fields, which have restricted the range of modulation of QD properties. In
contrast, in the present work we drive CdSe:CdS core:shell QD films with
high-field THz-frequency electromagnetic pulses whose duration is only a few
picoseconds. Surprisingly, in response to the THz excitation we observe QD
luminescence even in the absence of an external charge source. Our experiments
show that QD luminescence is associated with a remarkably high and rapid
modulation of the QD band-gap, which is changing by more than 0.5 eV
(corresponding to 25% of the unperturbed bandgap energy) within the picosecond
timeframe of THz field profile. We show that these colossal energy shifts can
be consistently explained by the quantum confined Stark effect. Our work
demonstrates a route to extreme modulation of material properties without
configurational changes in material sets or geometries. Additionally, we expect
that this platform can be adapted to a novel compact THz detection scheme where
conversion of THz fields (with meV-scale photon energies) to the
visible/near-IR band (with eV-scale photon energies) can be achieved at room
temperature with high bandwidth and sensitivity."
Vladimir Bulovic,arXiv:1509.03687,https://arxiv.org/abs/1509.03687,"Abstract:  Plexcitons are polaritonic modes that result from the strong coupling between
excitons and plasmons. We consider plexcitons emerging from the interaction of
excitons in an organic molecular layer with surface plasmons in a metallic
film. We predict the emergence of Dirac cones in the two-dimensional
bandstructure of plexcitons due to the inherent alignment of the excitonic
transitions in the organic layer. These Dirac cones may open up in energy by
simultaneously interfacing the metal with a magneto-optical layer and
subjecting the whole system to a perpendicular magnetic field. The resulting
energy gap becomes populated with topologically protected one-way modes which
travel at the interface of this plexcitonic system. Our theoretical proposal
suggests that plexcitons are a convenient and simple platform for the
exploration of exotic phases of matter as well as of novel ways to direct
energy flow at the nanoscale."
Michael Carbin,arXiv:1809.05859,https://arxiv.org/abs/1809.05859,"Abstract:  When a computational task tolerates a relaxation of its specification or when
an algorithm tolerates the effects of noise in its execution, hardware,
programming languages, and system software can trade deviations from correct
behavior for lower resource usage. We present, for the first time, a synthesis
of research results on computing systems that only make as many errors as their
users can tolerate, from across the disciplines of computer aided design of
circuits, digital system design, computer architecture, programming languages,
operating systems, and information theory.
Rather than over-provisioning resources at each layer to avoid errors, it can
be more efficient to exploit the masking of errors occurring at one layer which
can prevent them from propagating to a higher layer. We survey tradeoffs for
individual layers of computing systems from the circuit level to the operating
system level and illustrate the potential benefits of end-to-end approaches
using two illustrative examples. To tie together the survey, we present a
consistent formalization of terminology, across the layers, which does not
significantly deviate from the terminology traditionally used by research
communities in their layer of focus."
Michael Carbin,arXiv:1808.07412,https://arxiv.org/abs/1808.07412,"Abstract:  Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure."
Michael Carbin,arXiv:1805.06090,https://arxiv.org/abs/1805.06090,"Abstract:  Researchers have recently designed a number of application-specific fault
tolerance mechanisms that enable applications to either be naturally resilient
to errors or include additional detection and correction steps that can bring
the overall execution of an application back into an envelope for which an
acceptable execution is eventually guaranteed. A major challenge to building an
application that leverages these mechanisms, however, is to verify that the
implementation satisfies the basic invariants that these mechanisms
require--given a model of how faults may manifest during the application's
execution.
To this end we present Leto, an SMT based automatic verification system that
enables developers to verify their applications with respect to a first-class
execution model specification. Namely, Leto enables software and platform
developers to programmatically specify the execution semantics of the
underlying hardware system as well as verify assertions about the behavior of
the application's resulting execution. In this paper, we present the Leto
programming language and its corresponding verification system. We also
demonstrate Leto on several applications that leverage application-specific
fault tolerance mechanisms."
Michael Carbin,arXiv:1805.01863,https://arxiv.org/abs/1805.01863,"Abstract:  Researchers have recently proposed several systems that ease the process of
performing Bayesian probabilistic inference. These include systems for
automatic inference algorithm synthesis as well as stronger abstractions for
manual algorithm development. However, existing systems whose performance
relies on the developer manually constructing a part of the inference algorithm
have limited support for reasoning about the correctness of the resulting
algorithm.
In this paper, we present Shuffle, a programming language for manually
developing inference procedures that 1) enforces the basic rules of probability
theory, 2) enforces the statistical dependencies of the algorithm's
corresponding probabilistic model, and 3) generates an optimized
implementation. We have used Shuffle to develop inference algorithms for
several standard probabilistic models. Our results demonstrate that Shuffle
enables a developer to deliver correct and performant implementations of these
algorithms."
Michael Carbin,arXiv:1805.00468,https://arxiv.org/abs/1805.00468,"Abstract:  Though many safety-critical software systems use floating point to represent
real-world input and output, programmers usually have idealized versions in
mind that compute with real numbers. Significant deviations from the ideal can
cause errors and jeopardize safety. Some programming systems implement exact
real arithmetic, which resolves this matter but complicates others, such as
decision making. In these systems, it is impossible to compute (total and
deterministic) discrete decisions based on connected spaces such as
$\mathbb{R}$. We present programming-language semantics based on constructive
topology with variants allowing nondeterminism and/or partiality. Either
nondeterminism or partiality suffices to allow computable decision making on
connected spaces such as $\mathbb{R}$. We then introduce pattern matching on
spaces, a language construct for creating programs on spaces, generalizing
pattern matching in functional programming, where patterns need not represent
decidable predicates and also may overlap or be inexhaustive, giving rise to
nondeterminism or partiality, respectively. Nondeterminism and/or partiality
also yield formal logics for constructing approximate decision procedures. We
implemented these constructs in the Marshall language for exact real
arithmetic."
Michael Carbin,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Michael Carbin,arXiv:1803.03635,https://arxiv.org/abs/1803.03635,"Abstract:  Neural network pruning techniques can reduce the parameter counts of trained
networks by over 90%, decreasing storage requirements and improving
computational performance of inference without compromising accuracy. However,
contemporary experience is that the sparse architectures produced by pruning
are difficult to train from the start, which would similarly improve training
performance.
We find that a standard technique for pruning weights naturally uncovers
subnetworks whose initializations made them capable of training effectively.
Based on these results, we articulate the ""lottery ticket hypothesis"": dense,
randomly-initialized feed-forward networks contain subnetworks (""winning
tickets"") that - when trained in isolation - arrive at comparable test accuracy
in a comparable number of iterations. The winning tickets we find have won the
initialization lottery: their connections have initial weights that make
training particularly effective.
We present an algorithm to identify winning tickets and a series of
experiments that support the lottery ticket hypothesis and the importance of
these fortuitous initializations. We consistently find winning tickets that are
less than 10-20% of the size of several fully-connected and convolutional
feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning
tickets we find above that size learn faster than the original network and
exhibit higher test accuracy."
Michael Carbin,arXiv:1202.0359,https://arxiv.org/abs/1202.0359,"Abstract:  We propose a novel approach to improving software security called
Cryptographic Path Hardening, which is aimed at hiding security vulnerabilities
in software from attackers through the use of provably secure and obfuscated
cryptographic devices to harden paths in programs.
By ""harden"" we mean that certain error-checking if-conditionals in a given
program P are replaced by equivalent"" we mean that adversaries cannot use
semi-automatic program analysis techniques to reason about the hardened program
paths and thus cannot discover as-yet-unknown errors along those paths, except
perhaps through black-box dictionary attacks or random testing (which we can
never prevent).
Other than these unpreventable attack methods, we can make program analysis
aimed at error-finding ""provably hard"" for a resource-bounded attacker, in the
same sense that cryptographic schemes are hard to break. Unlike
security-through-obscurity, in Cryptographic Path Hardening we use
provably-secure crypto devices to hide errors and our mathematical arguments of
security are the same as the standard ones used in cryptography.
One application of Cryptographic Path Hardening is that software patches or
filters often reveal enough information to an attacker that they can be used to
construct error-revealing inputs to exploit an unpatched version of the
program. By ""hardening"" the patch we make it difficult for the attacker to
analyze the patched program to construct error-revealing inputs, and thus
prevent him from potentially constructing exploits."
Vincent Chan,arXiv:1307.1174,https://arxiv.org/abs/1307.1174,"Abstract:  Let $E \subseteq R^n$ be a closed set of Hausdorff dimension $\alpha$. For $m
\geq n$, let $\{B_1,\ldots,B_k\}$ be $n \times (m-n)$ matrices. We prove that
if the system of matrices $B_j$ is non-degenerate in a suitable sense, $\alpha$
is sufficiently close to $n$, and if $E$ supports a probability measure obeying
appropriate dimensionality and Fourier decay conditions, then for a range of
$m$ depending on $n$ and $k$, the set $E$ contains a translate of a non-trivial
$k$-point configuration $\{B_1y,\ldots,B_ky\}$. As a consequence, we are able
to establish existence of certain geometric configurations in Salem sets (such
as parallelograms in $ R^n$ and isosceles right triangles in $R^2$). This can
be viewed as a multidimensional analogue of an earlier result of Laba and
Pramanik on 3-term arithmetic progressions in subsets of $R$."
Vincent Chan,arXiv:1107.1000,https://arxiv.org/abs/1107.1000,"Abstract:  We study the spindown of isolated neutron stars from initially rapid rotation
rates, driven by two factors: (i) gravitational wave emission due to r-modes
and (ii) magnetic braking. In the context of isolated neutron stars, we present
the first study including self-consistently the magnetic damping of r-modes in
the spin evolution. We track the spin evolution employing the RNS code, which
accounts for the rotating structure of neutron stars for various equations of
state. We find that, despite the strong damping due to the magnetic field,
r-modes alter the braking rate from pure magnetic braking for B<10^{13}G. For
realistic values of the saturation amplitude, the r-mode can also decrease the
time to reach the threshold central density for quark deconfinement. Within a
phenomenological model, we assess the gravitational waveform that would result
from r-mode driven spindown of a magnetized neutron star. To contrast with the
persistent signal during the spindown phase, we also present a preliminary
estimate of the transient gravitational wave signal from an explosive
quark-hadron phase transition, which can be a signal for the deconfinement of
quarks inside neutron stars."
Anantha Chandrakasan,arXiv:1801.05507,https://arxiv.org/abs/1801.05507,"Abstract:  The growing popularity of cloud-based machine learning raises a natural
question about the privacy guarantees that can be provided in such a setting.
Our work tackles this problem in the context where a client wishes to classify
private images using a convolutional neural network (CNN) trained by a server.
Our goal is to build efficient protocols whereby the client can acquire the
classification result without revealing their input to the server, while
guaranteeing the privacy of the server's neural network.
To this end, we design Gazelle, a scalable and low-latency system for secure
neural network inference, using an intricate combination of homomorphic
encryption and traditional two-party computation techniques (such as garbled
circuits). Gazelle makes three contributions. First, we design the Gazelle
homomorphic encryption library which provides fast algorithms for basic
homomorphic operations such as SIMD (single instruction multiple data)
addition, SIMD multiplication and ciphertext permutation. Second, we implement
the Gazelle homomorphic linear algebra kernels which map neural network layers
to optimized homomorphic matrix-vector multiplication and convolution routines.
Third, we design optimized encryption switching protocols which seamlessly
convert between homomorphic and garbled circuit encodings to enable
implementation of complete neural network inference.
We evaluate our protocols on benchmark neural networks trained on the MNIST
and CIFAR-10 datasets and show that Gazelle outperforms the best existing
systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint
2017/1164) by 30 times in online runtime. Similarly when compared with fully
homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders
of magnitude faster online run-time."
Anantha Chandrakasan,arXiv:0710.4815,https://arxiv.org/abs/0710.4815,"Abstract:  Ultra-wideband (UWB) communication is an emerging wireless technology that
promises high data rates over short distances and precise locationing. The
large available bandwidth and the constraint of a maximum power spectral
density drives a unique set of system challenges. This paper addresses these
challenges using two UWB transceivers and a discrete prototype platform."
Anantha Chandrakasan,arXiv:0710.4732,https://arxiv.org/abs/0710.4732,"Abstract:  Wireless microsensor networks, which have been the topic of intensive
research in recent years, are now emerging in industrial applications. An
important milestone in this transition has been the release of the IEEE
802.15.4 standard that specifies interoperable wireless physical and medium
access control layers targeted to sensor node radios. In this paper, we
evaluate the potential of an 802.15.4 radio for use in an ultra low power
sensor node operating in a dense network. Starting from measurements carried
out on the off-the-shelf radio, effective radio activation and link adaptation
policies are derived. It is shown that, in a typical sensor network scenario,
the average power per node can be reduced down to 211m mm mW. Next, the energy
consumption breakdown between the different phases of a packet transmission is
presented, indicating which part of the transceiver architecture can most
effectively be optimized in order to further reduce the radio power, enabling
self-powered wireless microsensor networks."
Adam Chlipala,arXiv:1805.00468,https://arxiv.org/abs/1805.00468,"Abstract:  Though many safety-critical software systems use floating point to represent
real-world input and output, programmers usually have idealized versions in
mind that compute with real numbers. Significant deviations from the ideal can
cause errors and jeopardize safety. Some programming systems implement exact
real arithmetic, which resolves this matter but complicates others, such as
decision making. In these systems, it is impossible to compute (total and
deterministic) discrete decisions based on connected spaces such as
$\mathbb{R}$. We present programming-language semantics based on constructive
topology with variants allowing nondeterminism and/or partiality. Either
nondeterminism or partiality suffices to allow computable decision making on
connected spaces such as $\mathbb{R}$. We then introduce pattern matching on
spaces, a language construct for creating programs on spaces, generalizing
pattern matching in functional programming, where patterns need not represent
decidable predicates and also may overlap or be inexhaustive, giving rise to
nondeterminism or partiality, respectively. Nondeterminism and/or partiality
also yield formal logics for constructing approximate decision procedures. We
implemented these constructs in the Marshall language for exact real
arithmetic."
Adam Chlipala,arXiv:1803.04870,https://arxiv.org/abs/1803.04870,"Abstract:  It is a neat result from functional programming that libraries of parser
combinators can support rapid construction of decoders for quite a range of
formats. With a little more work, the same combinator program can denote both a
decoder and an encoder. Unfortunately, the real world is full of gnarly
formats, as with the packet formats that make up the standard Internet protocol
stack. Most past parser-combinator approaches cannot handle these formats, and
the few exceptions require redundancy -- one part of the natural grammar needs
to be hand-translated into hints in multiple parts of a parser program. We show
how to recover very natural and nonredundant format specifications, covering
all popular network packet formats and generating both decoders and encoders
automatically. The catch is that we use the Coq proof assistant to derive both
kinds of artifacts using tactics, automatically, in a way that guarantees that
they form inverses of each other. We used our approach to reimplement packet
processing for a full Internet protocol stack, inserting our replacement into
the OCaml-based MirageOS unikernel, resulting in minimal performance
degradation."
Adam Chlipala,arXiv:1401.7694,https://arxiv.org/abs/1401.7694,"Abstract:  We describe our experience implementing a broad category-theory library in
Coq. Category theory and computational performance are not usually mentioned in
the same breath, but we have needed substantial engineering effort to teach Coq
to cope with large categorical constructions without slowing proof script
processing unacceptably. In this paper, we share the lessons we have learned
about how to represent very abstract mathematical objects and arguments in Coq
and how future proof assistants might be designed to better support such
reasoning. One particular encoding trick to which we draw attention allows
category-theoretic arguments involving duality to be internalized in Coq's
logic with definitional equality. Ours may be the largest Coq development to
date that uses the relatively new Coq version developed by homotopy type
theorists, and we reflect on which new features were especially helpful."
Adam Chlipala,arXiv:1305.6543,https://arxiv.org/abs/1305.6543,"Abstract:  We describe a method for building composable and extensible verification
procedures within the Coq proof assistant. Unlike traditional methods that rely
on run-time generation and checking of proofs, we use verified-correct
procedures with Coq soundness proofs. Though they are internalized in Coq's
logic, our provers support sound extension by users with hints over new
domains, enabling automated reasoning about user-defined abstract predicates.
We maintain soundness by developing an architecture for modular packaging,
construction, and composition of hint databases, which had previously only been
implemented in Coq at the level of its dynamically typed, proof-generating
tactic language. Our provers also include rich handling of unification
variables, enabling integration with other tactic-based deduction steps within
Coq. We have implemented our techniques in MirrorShard, an open-source
framework for reflective verification. We demonstrate its applicability by
instantiating it to separation logic in order to reason about imperative
program verification."
Adam Chlipala,arXiv:1301.4779,https://arxiv.org/abs/1301.4779,"Abstract:  We report on the implementation of a certified compiler for a high-level
hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).
Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded
atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.
The target language of the compiler corresponds to a synthesisable subset of
Verilog or VHDL. A key aspect of our approach is that input programs to the
compiler can be defined and proved correct inside Coq. Then, we use extraction
and a Verilog back-end (written in OCaml) to get a certified version of a
hardware design."
Isaac Chuang,arXiv:1811.02124,https://arxiv.org/abs/1811.02124,"Abstract:  While quantum devices rely on interactions between constituent subsystems and
with their environment to operate, native interactions alone often fail to
deliver targeted performance. Coherent pulsed control provides the ability to
tailor effective interactions, known as Hamiltonian engineering. We propose a
Hamiltonian engineering method that maximizes desired interactions while
mitigating deleterious ones by conducting a pulse sequence search using
constrained optimization. The optimization formulation incorporates pulse
sequence length and cardinality penalties consistent with linear or integer
programming. We apply the general technique to magnetometry with solid state
spin ensembles in which inhomogeneous interactions between sensing spins limit
coherence. Defining figures of merit for broadband Ramsey magnetometry, we
present novel pulse sequences which outperform known techniques for homonuclear
spin decoupling in both spin-1/2 and spin-1 systems. When applied to nitrogen
vacancy (NV) centers in diamond, this scheme partially preserves the Zeeman
interaction while zeroing dipolar coupling between negatively charged
NV$^{\text -}$ centers. Such a scheme is of interest for NV$^\text{-}$
magnetometers which have reached the NV$^\text{-}$-NV$^\text{-}$ coupling
limit. We discuss experimental implementation in NV ensembles, as well as
applicability of the current approach to more general spin bath decoupling and
superconducting qubit control."
Isaac Chuang,arXiv:1807.09912,https://arxiv.org/abs/1807.09912,"Abstract:  Compared to humans, machine learning models generally require significantly
more training examples and fail to extrapolate from experience to solve
previously unseen challenges. To help close this performance gap, we augment
single-task neural networks with a meta-recognition model which learns a
succinct model code via its autoencoder structure, using just a few informative
examples. The model code is then employed by a meta-generative model to
construct parameters for the task-specific model. We demonstrate that for
previously unseen tasks, without additional training, this Meta-Learning
Autoencoder (MeLA) framework can build models that closely match the true
underlying models, with loss significantly lower than given by fine-tuned
baseline networks, and performance that compares favorably with
state-of-the-art meta-learning algorithms. MeLA also adds the ability to
identify influential training examples and predict which additional data will
be most valuable to acquire to improve model prediction."
Isaac Chuang,arXiv:1801.07618,https://arxiv.org/abs/1801.07618,"Abstract:  Each time a learner in a self-paced online course is trying to answer an
assessment question, it takes some time to submit the answer, and if multiple
attempts are allowed and the first answer was incorrect, it takes some time to
submit the second attempt, and so on. Here we study the distribution of such
""response times"". We find that the log-normal statistical model for such times,
previously suggested in the literature, holds for online courses qualitatively.
Users who, according to this model, tend to take longer on submits are more
likely to complete the course, have a higher level of engagement and achieve a
higher grade. This finding can be the basis for designing interventions in
online courses, such as MOOCs, which would encourage some users to slow down."
Isaac Chuang,arXiv:1801.01081,https://arxiv.org/abs/1801.01081,"Abstract:  We present a novel set of reversible modular multipliers applicable to
quantum computing, derived from three classical techniques: 1) traditional
integer division, 2) Montgomery residue arithmetic, and 3) Barrett reduction.
Each multiplier computes an exact result for all binary input values, while
maintaining the asymptotic resource complexity of a single (non-modular)
integer multiplier. We additionally conduct an empirical resource analysis of
our designs in order to determine the total gate count and circuit depth of
each fully constructed circuit, with inputs as large as 2048 bits. Our
comparative analysis considers both circuit implementations which allow for
arbitrary (controlled) rotation gates, as well as those restricted to a typical
fault-tolerant gate set."
Isaac Chuang,arXiv:1709.05302,https://arxiv.org/abs/1709.05302,"Abstract:  We establish a symmetry-operator framework for designing quantum error
correcting~(QEC) codes based on fundamental properties of the underlying system
dynamics. Based on this framework, we propose three hardware-efficient bosonic
QEC codes that are suitable for $\chi^{(2)}$-interaction based quantum
computation: the $\chi^{(2)}$ parity-check code, the $\chi^{(2)}$ embedded
error-correcting code, and the $\chi^{(2)}$ binomial code, all of which detect
photon-loss or photon-gain errors by means of photon-number parity measurements
and then correct them via $\chi^{(2)}$ Hamiltonian evolutions and linear-optics
transformations. Our symmetry-operator framework provides a systematic
procedure for finding QEC codes that are not stabilizer codes. The $\chi^{(2)}$
binomial code is of special interest because, with $m\le N$ identified from
channel monitoring, it can correct $m$-photon loss errors, $m$-photon gain
errors, and $(m-1)$th-order dephasing errors using logical qudits that are
encoded in $O(N)$ photons. In comparison, other bosonic QEC codes require
$O(N^2)$ photons to correct the same degree of bosonic errors. Such improved
photon-efficiency underscores the additional error-correction power that can be
provided by channel monitoring. We develop quantum Hamming bounds for
photon-loss errors in the code subspaces associated with the $\chi^{(2)}$
parity-check code and the $\chi^{(2)}$ embedded error-correcting code, and we
prove that these codes saturate their respective bounds. Our $\chi^{(2)}$ QEC
codes exhibit hardware efficiency in that they address the principal error
mechanisms and exploit the available physical interactions of the underlying
hardware, thus reducing the physical resources required for implementing their
encoding, decoding, and error-correction operations, and their universal
encoded-basis gate sets."
Isaac Chuang,arXiv:1707.05391,https://arxiv.org/abs/1707.05391,"Abstract:  The exponential speedups promised by Hamiltonian simulation on a quantum
computer depends crucially on structure in both the Hamiltonian $\hat{H}$, and
the quantum circuit $\hat{U}$ that encodes its description. In the quest to
better approximate time-evolution $e^{-i\hat{H}t}$ with error $\epsilon$, we
motivate a systematic approach to understanding and exploiting structure, in a
setting where Hamiltonians are encoded as measurement operators of unitary
circuits $\hat{U}$ for generalized measurement. This allows us to define a
\emph{uniform spectral amplification} problem on this framework for expanding
the spectrum of encoded Hamiltonian with exponentially small distortion. We
present general solutions to uniform spectral amplification in a hierarchy
where factoring $\hat{U}$ into $n=1,2,3$ unitary oracles represents increasing
structural knowledge of the encoding. Combined with structural knowledge of the
Hamiltonian, specializing these results allow us simulate time-evolution by
$d$-sparse Hamiltonians using $\mathcal{O}\left(t(d \|\hat
H\|_{\text{max}}\|\hat H\|_{1})^{1/2}\log{(t\|\hat{H}\|/\epsilon)}\right)$
queries, where $\|\hat H\|\le \|\hat H\|_1\le d\|\hat H\|_{\text{max}}$. Up to
logarithmic factors, this is a polynomial improvement upon prior art using
$\mathcal{O}\left(td\|\hat
H\|_{\text{max}}+\frac{\log{(1/\epsilon)}}{\log\log{(1/\epsilon)}}\right)$ or
$\mathcal{O}(t^{3/2}(d \|\hat H\|_{\text{max}}\|\hat H\|_{1}\|\hat
H\|/\epsilon)^{1/2})$ queries. In the process, we also prove a matching lower
bound of $\Omega(t(d\|\hat H\|_{\text{max}}\|\hat H\|_{1})^{1/2})$ queries,
present a distortion-free generalization of spectral gap amplification, and an
amplitude amplification algorithm that performs multiplication on unknown state
amplitudes."
Isaac Chuang,arXiv:1707.00012,https://arxiv.org/abs/1707.00012,"Abstract:  A non-Clifford gate is required for universal quantum computation, and,
typically, this is the most error-prone and resource intensive logical
operation on an error-correcting code. Small, single-qubit rotations are
popular choices for this non-Clifford gate, but certain three-qubit gates, such
as Toffoli or controlled-controlled-Z (CCZ), are equivalent options that are
also more suited for implementing some quantum algorithms, for instance those
with coherent classical subroutines. Here, we calculate error rates and
resource overheads for implementing logical CCZ with pieceable fault-tolerance,
a non-transversal method for implementing logical gates. We provide a
comparison with a non-local magic-state scheme on a concatenated code and a
local magic-state scheme on the surface code. We find the pieceable
fault-tolerance scheme particularly advantaged over magic states on
concatenated codes and in certain regimes over magic states on the surface
code. Our results suggest that pieceable fault-tolerance is a promising
candidate for fault-tolerance in a near-future quantum computer."
Isaac Chuang,arXiv:1705.01936,https://arxiv.org/abs/1705.01936,"Abstract:  Noisy PN learning is the problem of binary classification when training
examples may be mislabeled (flipped) uniformly with noise rate rho1 for
positive examples and rho0 for negative examples. We propose Rank Pruning (RP)
to solve noisy PN learning and the open problem of estimating the noise rates,
i.e. the fraction of wrong positive and negative labels. Unlike prior
solutions, RP is time-efficient and general, requiring O(T) for any
unrestricted choice of probabilistic classifier with T fitting time. We prove
RP has consistent noise estimation and equivalent expected risk as learning
with uncorrupted labels in ideal conditions, and derive closed-form solutions
when conditions are non-ideal. RP achieves state-of-the-art noise estimation
and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the
amount of noise and performs similarly impressively when a large portion of
training examples are noise drawn from a third distribution. To highlight, RP
with a CNN classifier can predict if an MNIST digit is a ""one""or ""not"" with
only 0.25% error, and 0.46 error across all digits, even when 50% of positive
examples are mislabeled and 50% of observed positive labels are mislabeled
negative examples."
Isaac Chuang,arXiv:1704.03431,https://arxiv.org/abs/1704.03431,"Abstract:  We prove that universal quantum computation can be realized---using only
linear optics and $\chi^{(2)}$ (three-wave mixing) interactions---in any
$(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we
exhibit a strictly universal gate set for the qubit basis in the
one-pump-photon subspace. Next, we demonstrate qutrit-basis universality by
proving that $\chi^{(2)}$ Hamiltonians and photon-number operators generate the
full $\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing
how the qutrit controlled-$Z$ gate can be implemented with only linear optics
and $\chi^{(2)}$ interactions. We then use proof by induction to obtain our
general qudit result. Our induction proof relies on coherent photon
injection/subtraction, a technique enabled by $\chi^{(2)}$ interaction between
the encoding modes and ancillary modes. Finally, we show that coherent photon
injection is more than a conceptual tool in that it offers a route to preparing
high-photon-number Fock states from single-photon Fock states."
Isaac Chuang,arXiv:1610.06546,https://arxiv.org/abs/1610.06546,"Abstract:  Given a Hermitian operator $\hat{H}=\langle G|\hat{U}|G\rangle$ that is the
projection of an oracle $\hat{U}$ by state $|G\rangle$ created with oracle
$\hat{G}$, the problem of Hamiltonian simulation is approximating the time
evolution operator $e^{-i\hat{H}t}$ at time $t$ with error $\epsilon$. We show
that this can be done with query complexity
$\mathcal{O}\big(t+\frac{\log{(1/\epsilon)}}{\log\log{(1/\epsilon)}}\big)$ to
$\hat{G},\hat{U}$ that is optimal, not just in asymptotic limits, but for all
values $t,\epsilon$. Furthermore, only $2$ additional ancilla qubits are
required in total, together with $\mathcal{O}(1)$ additional single and
two-qubit gates per query. Our approach to Hamiltonian simulation subsumes
important prior art considering Hamiltonians which are $d$-sparse or a linear
combination of unitaries, leading to significant improvements in space
complexity, as well as a quadratic speed-up for precision simulations. It also
motivates useful new instances, such as where $\langle G|\hat{U}|G\rangle$ is a
density matrix. A key technical result is `qubitization' which uses
controlled-$\hat{U}$ and controlled-$\hat{G}$ to embed $\hat{H}$ in an
invariant $\text{SU}(2)$ subspace. A large class of operator functions of
$\hat{H}$ can then be computed with optimal query complexity, of which
$e^{-i\hat{H}t}$ is a special case."
Isaac Chuang,arXiv:1609.03603,https://arxiv.org/abs/1609.03603,"Abstract:  Fixed-point quantum search algorithms succeed at finding one of $M$ target
items among $N$ total items even when the run time of the algorithm is longer
than necessary. While the famous Grover's algorithm can search quadratically
faster than a classical computer, it lacks the fixed-point property --- the
fraction of target items must be known precisely to know when to terminate the
algorithm. Recently, Yoder, Low, and Chuang gave an optimal gate-model search
algorithm with the fixed-point property. Meanwhile, it is known that an
adiabatic quantum algorithm, operating by continuously varying a Hamiltonian,
can reproduce the quadratic speedup of gate-model Grover search. We ask, can an
adiabatic algorithm also reproduce the fixed-point property? We show that the
answer depends on what interpolation schedule is used, so as in the gate model,
there are both fixed-point and non-fixed-point versions of adiabatic search,
only some of which attain the quadratic quantum speedup. Guided by geometric
intuition on the Bloch sphere, we rigorously justify our claims with an
explicit upper bound on the error in the adiabatic approximation. We also show
that the fixed-point adiabatic search algorithm can be simulated in the gate
model with neither loss of the quadratic Grover speedup nor of the fixed-point
property. Finally, we discuss natural uses of fixed-point algorithms such as
preparation of a relatively prime state and oblivious amplitude amplification."
Isaac Chuang,arXiv:1606.02685,https://arxiv.org/abs/1606.02685,"Abstract:  The physics of quantum mechanics is the inspiration for, and underlies,
quantum computation. As such, one expects physical intuition to be highly
influential in the understanding and design of many quantum algorithms,
particularly simulation of physical systems. Surprisingly, this has been
challenging, with current Hamiltonian simulation algorithms remaining abstract
and often the result of sophisticated but unintuitive constructions. We contend
that physical intuition can lead to optimal simulation methods by showing that
a focus on simple single-qubit rotations elegantly furnishes an optimal
algorithm for Hamiltonian simulation, a universal problem that encapsulates all
the power of quantum computation. Specifically, we show that the query
complexity of implementing time evolution by a $d$-sparse Hamiltonian $\hat{H}$
for time-interval $t$ with error $\epsilon$ is
$\mathcal{O}(td\|\hat{H}\|_{\text{max}}+\frac{\log{(1/\epsilon)}}{\log{\log{(1/\epsilon)}}})$,
which matches lower bounds in all parameters. This connection is made through
general three-step ""quantum signal processing"" methodology, comprised of (1)
transducing eigenvalues of $\hat{H}$ into a single ancilla qubit, (2)
transforming these eigenvalues through an optimal-length sequence of
single-qubit rotations, and (3) projecting this ancilla with near unity success
probability."
Isaac Chuang,arXiv:1606.02188,https://arxiv.org/abs/1606.02188,"Abstract:  Classical imaging works by scattering photons from an object to be imaged,
and achieves resolution scaling as $1/\sqrt{t}$, with $t$ the imaging time. By
contrast, the laws of quantum mechanics allow one to utilize quantum coherence
to obtain imaging resolution that can scale as quickly as $1/t$ -- the
so-called ""Heisenberg limit."" However, ambiguities in the obtained signal often
preclude taking full advantage of this quantum enhancement, while imaging
techniques designed to be unambiguous often lose this optimal Heisenberg
scaling. Here, we demonstrate an imaging technique which combines unambiguous
detection of the target with Heisenberg scaling of the resolution. We also
demonstrate a binary search algorithm which can efficiently locate a coherent
target using the technique, resolving a target trapped ion to within 0.3% of
the $1/e^2$ diameter of the excitation beam."
Isaac Chuang,arXiv:1605.04210,https://arxiv.org/abs/1605.04210,"Abstract:  We report on a method for measuring the branching ratios of dipole
transitions of trapped atomic ions by performing nested sequences of population
inversions. This scheme is broadly applicable and does not use ultrafast pulsed
or narrow linewidth lasers. It is simple to perform and insensitive to
experimental variables such as laser and magnetic field noise as well as ion
heating. To demonstrate its effectiveness, we make the most accurate
measurements thus far of the branching ratios of both 5P1/2 and 5P3/2 states in
88Sr+ with sub-1% uncertainties. We measure 17.175(27) for the branching ratio
of 5P1/2-5S1/2, 15.845(71) for 5P3/2-5S1/2, and 0.05609(21) for 5P3/2-4D5/2,
ten- fold and thirty-fold improvements in precision for 5P1/2 and 5P3/2
branching ratios respectively over the best previous experimental values."
Isaac Chuang,arXiv:1603.03996,https://arxiv.org/abs/1603.03996,"Abstract:  The creation of composite quantum gates that implement quantum response
functions $\hat{U}(\theta)$ dependent on some parameter of interest $\theta$ is
often more of an art than a science. Through inspired design, a sequence of $L$
primitive gates also depending on $\theta$ can engineer a highly nontrivial
$\hat{U}(\theta)$ that enables myriad precision metrology, spectroscopy, and
control techniques. However, discovering new, useful examples of
$\hat{U}(\theta)$ requires great intuition to perceive the possibilities, and
often brute-force to find optimal implementations. We present a systematic and
efficient methodology for composite gate design of arbitrary length, where
phase-controlled primitive gates all rotating by $\theta$ act on a single spin.
We fully characterize the realizable family of $\hat{U}(\theta)$, provide an
efficient algorithm that decomposes a choice of $\hat{U}(\theta)$ into its
shortest sequence of gates, and show how to efficiently choose an achievable
$\hat{U}(\theta)$ that for fixed $L$, is an optimal approximation to objective
functions on its quadratures. A strong connection is forged with
\emph{classical} discrete-time signal processing, allowing us to swiftly
construct, as examples, compensated gates with optimal bandwidth that implement
arbitrary single spin rotations with sub-wavelength spatial selectivity."
Isaac Chuang,arXiv:1603.03948,https://arxiv.org/abs/1603.03948,"Abstract:  It is an oft-cited fact that no quantum code can support a set of
fault-tolerant logical gates that is both universal and transversal. This no-go
theorem is generally responsible for the interest in alternative universality
constructions including magic state distillation. Widely overlooked, however,
is the possibility of non-transversal, yet still fault-tolerant, gates that
work directly on small quantum codes. Here we demonstrate precisely the
existence of such gates. In particular, we show how the limits of
non-transversality can be overcome by performing rounds of intermediate
error-correction to create logical gates on stabilizer codes that use no
ancillas other than those required for syndrome measurement. Moreover, the
logical gates we construct, the most prominent examples being Toffoli and
controlled-controlled-Z, often complete universal gate sets on their codes. We
detail such universal constructions for the smallest quantum codes, the 5-qubit
and 7-qubit codes, and then proceed to generalize the approach. One remarkable
result of this generalization is that any nondegenerate stabilizer code with a
complete set of fault-tolerant single-qubit Clifford gates has a universal set
of fault-tolerant gates. Another is the interaction of logical qubits across
different stabilizer codes, which, for instance, implies a broadly applicable
method of code switching."
Isaac Chuang,arXiv:1508.05699,https://arxiv.org/abs/1508.05699,"Abstract:  We describe a cheating strategy enabled by the features of massive open
online courses (MOOCs) and detectable by virtue of the sophisticated data
systems that MOOCs provide. The strategy, Copying Answers using Multiple
Existences Online (CAMEO), involves a user who gathers solutions to assessment
questions using a ""harvester"" account and then submits correct answers using a
separate ""master"" account. We use ""clickstream"" learner data to detect CAMEO
use among 1.9 million course participants in 115 MOOCs from two universities.
Using conservative thresholds, we estimate CAMEO prevalence at 1,237
certificates, accounting for 1.3% of the certificates in the 69 MOOCs with
CAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO
strategy. CAMEO users are more likely to be young, male, and international than
other MOOC certificate earners. We identify preventive strategies that can
decrease CAMEO rates and show evidence of their effectiveness in science
courses."
Isaac Chuang,arXiv:1507.08852,https://arxiv.org/abs/1507.08852,"Abstract:  Quantum computers are able to outperform classical algorithms. This was long
recognized by the visionary Richard Feynman who pointed out in the 1980s that
quantum mechanical problems were better solved with quantum machines. It was
only in 1994 that Peter Shor came up with an algorithm that is able to
calculate the prime factors of a large number vastly more efficiently than
known possible with a classical computer. This paradigmatic algorithm
stimulated the flourishing research in quantum information processing and the
quest for an actual implementation of a quantum computer. Over the last fifteen
years, using skillful optimizations, several instances of a Shor algorithm have
been implemented on various platforms and clearly proved the feasibility of
quantum factoring. For general scalability, though, a different approach has to
be pursued. Here, we report the realization of a fully scalable Shor algorithm
as proposed by Kitaev. For this, we demonstrate factoring the number fifteen by
effectively employing and controlling seven qubits and four ""cache-qubits"",
together with the implementation of generalized arithmetic operations, known as
modular multipliers. The scalable algorithm has been realized with an ion-trap
quantum computer exhibiting success probabilities in excess of 90%."
Isaac Chuang,arXiv:1505.03381,https://arxiv.org/abs/1505.03381,"Abstract:  We study the vacuum-induced degradation of high-finesse optical cavities with
mirror coatings composed of SiO$_2$-Ta$_{2}$O$_{5}$ dielectric stacks, and
present methods to protect these coatings and to recover their initial quality
factor. For separate coatings with reflectivities centered at 370 nm and 422
nm, a vacuum-induced continuous increase in optical loss occurs if the
surface-layer coating is made of Ta$_{2}$O$_{5}$, while it does not occur if it
is made of SiO$_2$. The incurred optical loss can be reversed by filling the
vacuum chamber with oxygen at atmospheric pressure, and the recovery rate can
be strongly accelerated by continuous laser illumination at 422 nm. Both the
degradation and the recovery processes depend strongly on temperature. We find
that a 1 nm-thick layer of SiO$_2$ passivating the Ta$_{2}$O$_{5}$ surface
layer is sufficient to reduce the degradation rate by more than a factor of 10,
strongly supporting surface oxygen depletion as the primary degradation
mechanism."
Isaac Chuang,arXiv:1502.05739,https://arxiv.org/abs/1502.05739,"Abstract:  Scaling-up from prototype systems to dense arrays of ions on chip, or vast
networks of ions connected by photonic channels, will require developing
entirely new technologies that combine miniaturized ion trapping systems with
devices to capture, transmit and detect light, while refining how ions are
confined and controlled. Building a cohesive ion system from such diverse parts
involves many challenges, including navigating materials incompatibilities and
undesired coupling between elements. Here, we review our recent efforts to
create scalable ion systems incorporating unconventional materials such as
graphene and indium tin oxide, integrating devices like optical fibers and
mirrors, and exploring alternative ion loading and trapping techniques."
Isaac Chuang,arXiv:1409.7993,https://arxiv.org/abs/1409.7993,"Abstract:  Conventional wisdom dictates that to image the position of fluorescent atoms
or molecules, one should stimulate as much emission and collect as many photons
as possible. That is, in this classical case, it has always been assumed that
the coherence time of the system should be made short, and that the statistical
scaling $\sim1/\sqrt{t}$ defines the resolution limit for imaging time $t$.
However, here we show in contrast that given the same resources, a long
coherence time permits a higher resolution image. In this quantum regime, we
give a procedure for determining the position of a single two-level system, and
demonstrate that the standard errors of our position estimates scale at the
Heisenberg limit as $\sim 1/t$, a quadratic, and notably optimal, improvement
over the classical case."
Isaac Chuang,arXiv:1409.3305,https://arxiv.org/abs/1409.3305,"Abstract:  Grover's quantum search and its generalization, quantum amplitude
amplification, provide quadratic advantage over classical algorithms for a
diverse set of tasks, but are tricky to use without knowing beforehand what
fraction $\lambda$ of the initial state is comprised of the target states. In
contrast, fixed-point search algorithms need only a reliable lower bound on
this fraction, but, as a consequence, lose the very quadratic advantage that
makes Grover's algorithm so appealing. Here we provide the first version of
amplitude amplification that achieves fixed-point behavior without sacrificing
the quantum speedup. Our result incorporates an adjustable bound on the failure
probability, and, for a given number of oracle queries, guarantees that this
bound is satisfied over the broadest possible range of $\lambda$."
Isaac Chuang,arXiv:1402.7359,https://arxiv.org/abs/1402.7359,"Abstract:  Performing exact inference on Bayesian networks is known to be #P-hard.
Typically approximate inference techniques are used instead to sample from the
distribution on query variables given the values $e$ of evidence variables.
Classically, a single unbiased sample is obtained from a Bayesian network on
$n$ variables with at most $m$ parents per node in time
$\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the
evidence might occur in the first place. By implementing a quantum version of
rejection sampling, we obtain a square-root speedup, taking
$\mathcal{O}(n2^mP(e)^{-\frac12})$ time per sample. We exploit the Bayesian
network's graph structure to efficiently construct a quantum state, a q-sample,
representing the intended classical distribution, and also to efficiently apply
amplitude amplification, the source of our speedup. Thus, our speedup is
notable as it is unrelativized -- we count primitive operations and require no
blackbox oracle queries."
Isaac Chuang,arXiv:1310.3173,https://arxiv.org/abs/1310.3173,"Abstract:  Massive Open Online Courses are an exciting new avenue for instruction and
research, yet they are full of unknowns. In the Spring of 2013, MITx released
its first introductory physics MOOC through the edX platform, generating a
total enrollment of 43,000 students from around the world. We describe the
population of participants in terms of their age, gender, level of education,
and country of origin, highlighting both the diversity of 8.02x enrollees as
well as gender gap and retention. Using three midterm exams and the final as
waypoints, we highlight performance by different demographic subpopulations and
their retention rates. Our work is generally aimed at making a bridge between
available MOOC data and topics associated with the Physics Education Research
community."
Isaac Chuang,arXiv:1307.2211,https://arxiv.org/abs/1307.2211,"Abstract:  Implementing a single qubit unitary is often hampered by imperfect control.
Systematic amplitude errors $\epsilon$, caused by incorrect duration or
strength of a pulse, are an especially common problem. But a sequence of
imperfect pulses can provide a better implementation of a desired operation, as
compared to a single primitive pulse. We find optimal pulse sequences
consisting of $L$ primitive $\pi$ or $2\pi$ rotations that suppress such errors
to arbitrary order $\mathcal{O}(\epsilon^{n})$ on arbitrary initial states.
Optimality is demonstrated by proving an $L=\mathcal{O}(n)$ lower bound and
saturating it with $L=2n$ solutions. Closed-form solutions for arbitrary
rotation angles are given for $n=1,2,3,4$. Perturbative solutions for any $n$
are proven for small angles, while arbitrary angle solutions are obtained by
analytic continuation up to $n=12$. The derivation proceeds by a novel
algebraic and non-recursive approach, in which finding amplitude error
correcting sequences can be reduced to solving polynomial equations."
Isaac Chuang,arXiv:1302.2904,https://arxiv.org/abs/1302.2904,"Abstract:  We present a novel hybrid system where an optical cavity is integrated with a
microfabricated planar-electrode ion trap. The trap electrodes produce a
tunable periodic potential allowing the trapping of up to 50 separate ion
chains spaced by 160 $\mu$m along the cavity axis. Each chain can contain up to
20 individually addressable Yb\textsuperscript{+} ions coupled to the cavity
mode. We demonstrate deterministic distribution of ions between the sites of
the electrostatic periodic potential and control of the ion-cavity coupling.
The measured strength of this coupling should allow access to the strong
collective coupling regime with $\lesssim$10 ions. The optical cavity could
serve as a quantum information bus between ions or be used to generate a strong
wavelength-scale periodic optical potential."
Isaac Chuang,arXiv:1212.1443,https://arxiv.org/abs/1212.1443,"Abstract:  Fluorescence collection sets the efficiency of state detection and the rate
of entanglement generation between remote trapped ion qubits. Despite efforts
to improve light collection using various optical elements, solid angle capture
is limited to ~10% for implementations that are scalable to many ions. We
present an approach based on fluorescence detection through a transparent trap
using an integrated photodetector, combining collection efficiency approaching
50% with scalability. We microfabricate transparent surface traps with indium
tin oxide and verify stable trapping of single ions. The fluorescence from a
cloud of ions is detected using a photodiode sandwiched with a transparent
trap."
Isaac Chuang,arXiv:1207.5846,https://arxiv.org/abs/1207.5846,"Abstract:  Fermions, as a major class of quantum particles, provide platforms for
quantum information processing beyond the possibilities of spins or bosons
which have been studied more extensively. One particularly interesting model to
study, in view of recent progress in manipulating ultracold fermion gases, is
the fermionic version of measurement-based quantum computation (MBQC), which
implements full quantum computation with only single site measurements on a
proper fermionic many-body resource state. However, it is not known which
fermionic states can be used as the resource states for MBQC and how to find
them. In this paper, we generalize the framework of spin MBQC to fermions. In
particular, we provide a general formalism to construct many-body entangled
fermion resource states for MBQC based on the fermionic projected entangled
pair state representation. We give a specific fermionic state which enables
universal MBQC and demonstrate that the non-locality inherent in fermion
systems can be properly taken care of with suitable measurement schemes. Such a
framework opens up possibilities of finding MBQC resource states which can be
more readily realized in the lab."
Isaac Chuang,arXiv:1202.6640,https://arxiv.org/abs/1202.6640,"Abstract:  Previous analyses of conditional \phi-phase gates for photonic qubits that
treat cross-phase modulation (XPM) in a causal, multimode, quantum field
setting suggest that a large (~\pi rad) nonlinear phase shift is always
accompanied by fidelity-degrading noise [J. H. Shapiro, Phys. Rev. A 73, 062305
(2006); J. Gea-Banacloche, Phys. Rev. A 81, 043823 (2010)]. Using an atomic
V-system to model an XPM medium, we present a conditional phase gate that, for
sufficiently small nonzero \phi, has high fidelity. The gate is made cascadable
by using using a special measurement, principal mode projection, to exploit the
quantum Zeno effect and preclude the accumulation of fidelity-degrading
departures from the principal-mode Hilbert space when both control and target
photons illuminate the gate."
Isaac Chuang,arXiv:1109.2995,https://arxiv.org/abs/1109.2995,"Abstract:  We model electric field noise from fluctuating patch potentials on conducting
surfaces by taking into account the finite geometry of the ion trap electrodes
to gain insight into the origin of anomalous heating in ion traps. The scaling
of anomalous heating rates with surface distance, $d$, is obtained for several
generic geometries of relevance to current ion trap designs, ranging from
planar to spheroidal electrodes. The influence of patch size is studied both by
solving Laplace's equation in terms of the appropriate Green's function as well
as through an eigenfunction expansion. Scaling with surface distance is found
to be highly dependent on the choice of geometry and the relative scale between
the spatial extent of the electrode, the ion-electrode distance, and the patch
size. Our model generally supports the $d^{-4}$ dependence currently found by
most experiments and models, but also predicts geometry-driven deviations from
this trend."
Isaac Chuang,arXiv:1108.0092,https://arxiv.org/abs/1108.0092,"Abstract:  Electrical charging of metal surfaces due to photoelectric generation of
carriers is of concern in trapped ion quantum computation systems, due to the
high sensitivity of the ions' motional quantum states to deformation of the
trapping potential. The charging induced by typical laser frequencies involved
in doppler cooling and quantum control is studied here, with microfabricated
surface electrode traps made of aluminum, copper, and gold, operated at 6 K
with a single Sr$^+$ ion trapped 100 $\mu$m above the trap surface. The lasers
used are at 370, 405, 460, and 674 nm, and the typical photon flux at the trap
is 10$^{14}$ photons/cm$^2$/sec. Charging is detected by monitoring the ion's
micromotion signal, which is related to the number of charges created on the
trap. A wavelength and material dependence of the charging behavior is
observed: lasers at lower wavelengths cause more charging, and aluminum
exhibits more charging than copper or gold. We describe the charging dynamic
based on a rate equation approach."
Isaac Chuang,arXiv:1103.5256,https://arxiv.org/abs/1103.5256,"Abstract:  An atomic ion is trapped at the tip of a single-mode optical fiber in a
cryogenic (8 K) surface-electrode ion trap. The fiber serves as an integrated
source of laser light, which drives the quadrupole qubit transition of
$^{88}$Sr$^+$. Through \emph{in situ} translation of the nodal point of the
trapping field, the Gaussian beam profile of the fiber output is imaged, and
the fiber-ion displacement, in units of the mode waist at the ion, is optimized
to within $0.13\pm0.10$ of the mode center despite an initial offset of
$3.30\pm0.10$. Fiber-induced charging at $125 \mu$W is observed to be
${\sim}10$ V/m at an ion height of $670 \mu$m, with charging and discharging
time constants of $1.6\pm0.3$ s and $4.7\pm0.6$ s respectively. This work is of
importance to large-scale, ion-based quantum information processing, where
optics integration in surface-electrode designs may be a crucial enabling
technology."
Isaac Chuang,arXiv:1011.5259,https://arxiv.org/abs/1011.5259,"Abstract:  A novel approach to optics integration in ion traps is demonstrated based on
a surface electrode ion trap that is microfabricated on top of a dielectric
mirror. Additional optical losses due to fabrication are found to be as low as
80 ppm for light at 422 nm. The integrated mirror is used to demonstrate light
collection from, and imaging of, a single 88 Sr+ ion trapped $169\pm4 \mu$m
above the mirror."
Isaac Chuang,arXiv:1010.6108,https://arxiv.org/abs/1010.6108,"Abstract:  We fabricate superconducting ion traps with niobium and niobium nitride and
trap single 88Sr ions at cryogenic temperatures. The superconducting transition
is verified and characterized by measuring the resistance and critical current
using a 4-wire measurement on the trap structure, and observing change in the
rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz
at 6 K and shows no significant change across the superconducting transition,
suggesting that anomalous heating is primarily caused by noise sources on the
surface. This demonstration of superconducting ion traps opens up possibilities
for integrating trapped ions and molecular ions with superconducting devices."
Isaac Chuang,arXiv:1010.2717,https://arxiv.org/abs/1010.2717,"Abstract:  It is well known that the ground state energy of many-particle Hamiltonians
involving only 2-body interactions can be obtained using constrained
optimizations over density matrices which arise from reducing an N-particle
state. While determining which 2-particle density matrices are ""N-
representable"" is a computationally hard problem, all known extreme
N-representable 2-particle reduced density matrices arise from a unique
N-particle pre-image, satisfying a conjecture established in 1972. We present
explicit counterexamples to this conjecture through giving Hamiltonians with
2-body interactions which have degenerate ground states that cannot be
distinguished by any 2-body operator. We relate the existence of such
counterexamples to quantum error correction codes and topologically ordered
spin systems."
Isaac Chuang,arXiv:1009.0036,https://arxiv.org/abs/1009.0036,"Abstract:  Two-dimensional crystals of trapped ions are a promising system with which to
implement quantum simulations of challenging problems such as spin frustration.
Here, we present a design for a surface-electrode elliptical ion trap which
produces a 2-D ion crystal and is amenable to microfabrication, which would
enable higher simulated coupling rates, as well as interactions based on
magnetic forces generated by on-chip currents. Working in an 11 K cryogenic
environment, we experimentally verify to within 5% a numerical model of the
structure of ion crystals in the trap. We also explore the possibility of
implementing quantum simulation using magnetic forces, and calculate J-coupling
rates on the order of 10^3 / s for an ion crystal height of 10 microns, using a
current of 1 A."
Isaac Chuang,arXiv:1008.1603,https://arxiv.org/abs/1008.1603,"Abstract:  We present a model as well as experimental results for a surface electrode
radio-frequency Paul trap that has a circular electrode geometry well-suited
for trapping of single ions and two-dimensional planar ion crystals. The trap
design is compatible with microfabrication and offers a simple method by which
the height of the trapped ions above the surface may be changed \emph{in situ}.
We demonstrate trapping of single and few Sr+ ions over an ion height range of
200-1000 microns for several hours under Doppler laser cooling, and use these
to characterize the trap, finding good agreement with our model."
Isaac Chuang,arXiv:1003.1774,https://arxiv.org/abs/1003.1774,"Abstract:  The tensor product representation of quantum states leads to a promising
variational approach to study quantum phase and quantum phase transitions,
especially topological ordered phases which are impossible to handle with
conventional methods due to their long range entanglement. However, an
important issue arises when we use tensor product states (TPS) as variational
states to find the ground state of a Hamiltonian: can arbitrary variations in
the tensors that represent ground state of a Hamiltonian be induced by local
perturbations to the Hamiltonian? Starting from a tensor product state which is
the exact ground state of a Hamiltonian with $\mathbb{Z}_2$ topological order,
we show that, surprisingly, not all variations of the tensors correspond to the
variation of the ground state caused by local perturbations of the Hamiltonian.
Even in the absence of any symmetry requirement of the perturbed Hamiltonian,
one necessary condition for the variations of the tensors to be physical is
that they respect certain $\mathbb{Z}_2$ symmetry. We support this claim by
calculating explicitly the change in topological entanglement entropy with
different variations in the tensors. This finding will provide important
guidance to numerical variational study of topological phase and phase
transitions. It is also a crucial step in using TPS to study universal
properties of a quantum phase and its topological order."
Isaac Chuang,arXiv:1002.0085,https://arxiv.org/abs/1002.0085,"Abstract:  Entanglement, as studied in quantum information science, and non-local
quantum correlations, as studied in condensed matter physics, are fundamentally
akin to each other. However, their relationship is often hard to quantify due
to the lack of a general approach to study both on the same footing. In
particular, while entanglement and non-local correlations are properties of
states, both arise from symmetries of global operators that commute with the
system Hamiltonian. Here, we introduce a framework for completely classifying
the local and non-local properties of all such global operators, given the
Hamiltonian and a bi-partitioning of the system. This framework is limited to
descriptions based on stabilizer quantum codes, but may be generalized. We
illustrate the use of this framework to study entanglement and non-local
correlations by analyzing global symmetries in topological order, distribution
of entanglement and entanglement entropy."
Isaac Chuang,arXiv:0912.4892,https://arxiv.org/abs/0912.4892,"Abstract:  We demonstrate quantum control techniques for a single trapped ion in a
cryogenic, surface-electrode trap. A narrow optical transition of Sr+ along
with the ground and first excited motional states of the harmonic trapping
potential form a two-qubit system. The optical qubit transition is susceptible
to magnetic field fluctuations, which we stabilize with a simple and compact
method using superconducting rings. Decoherence of the motional qubit is
suppressed by the cryogenic environment. AC Stark shift correction is
accomplished by controlling the laser phase in the pulse sequencer, eliminating
the need for an additional laser. Quantum process tomography is implemented on
atomic and motional states using conditional pulse sequences. With these
techniques we demonstrate a Cirac-Zoller Controlled-NOT gate in a single ion
with a mean fidelity of 91(1)%."
Isaac Chuang,arXiv:0910.4129,https://arxiv.org/abs/0910.4129,"Abstract:  Graphs are closely related to quantum error-correcting codes: every
stabilizer code is locally equivalent to a graph code, and every codeword
stabilized code can be described by a graph and a classical code. For the
construction of good quantum codes of relatively large block length,
concatenated quantum codes and their generalizations play an important role. We
develop a systematic method for constructing concatenated quantum codes based
on ""graph concatenation"", where graphs representing the inner and outer codes
are concatenated via a simple graph operation called ""generalized local
complementation."" Our method applies to both binary and non-binary concatenated
quantum codes as well as their generalizations."
Isaac Chuang,arXiv:0905.0148,https://arxiv.org/abs/0905.0148,"Abstract:  We report a demonstration and quantitative characterization of
one-dimensional cavity cooling of a single trapped 88Sr+ ion in the resolved
sideband regime. We measure the spectrum of cavity transitions, the rates of
cavity heating and cooling, and the steady-state cooling limit. The cavity
cooling dynamics and cooling limit of 22.5(3) motional quanta, limited by the
moderate coupling between the ion and the cavity, are consistent with a simple
model [Phys. Rev. A 64, 033405] without any free parameters, validating the
rate equation model for cavity cooling."
Isaac Chuang,arXiv:0812.4067,https://arxiv.org/abs/0812.4067,"Abstract:  Many-body entangled quantum states studied in condensed matter physics can be
primary resources for quantum information, allowing any quantum computation to
be realized using measurements alone, on the state. Such a universal state
would be remarkably valuable, if only it were thermodynamically stable and
experimentally accessible, by virtue of being the unique ground state of a
physically reasonable Hamiltonian made of two-body, nearest neighbor
interactions. We introduce such a state, composed of six-state particles on a
hexagonal lattice, and describe a general method for analyzing its properties
based on its projected entangled pair state representation."
Isaac Chuang,arXiv:0811.2422,https://arxiv.org/abs/0811.2422,"Abstract:  Dense array of ions in microfabricated traps represent one possible way to
scale up ion trap quantum computing. The ability to address individual ions is
an important component of such a scheme. We demonstrate individual addressing
of trapped ions in a microfabricated surface-electrode trap using a magnetic
field gradient generated on-chip. A frequency splitting of 310(2) kHz for two
ions separated by 5 um is achieved. Selective single qubit operations are
performed on one of two trapped ions with an average of 2.2+/-1.0% crosstalk.
Coherence time as measured by the spin-echo technique is unaffected by the
field gradient."
Isaac Chuang,arXiv:0809.2824,https://arxiv.org/abs/0809.2824,"Abstract:  Quantum simulations of spin systems could enable the solution of problems
which otherwise require infeasible classical resources. Such a simulation may
be implemented using a well-controlled system of effective spins, such as a
two-dimensional lattice of locally interacting ions. We propose here a layered
planar rf trap design that can be used to create arbitrary two-dimensional
lattices of ions. The design also leads naturally to ease of microfabrication.
As a first experimental demonstration, we confine strontium-88 ions in a
mm-scale lattice trap and verify numerical models of the trap by measuring the
motional frequencies. We also confine 440 nm diameter charged microspheres and
observe ion-ion repulsion between ions in neighboring lattice sites. Our
design, when scaled to smaller ion-ion distances, is appropriate for quantum
simulation schemes, e.g. that of Porras and Cirac (PRL 92 207901 (2004)). We
note, however, that in practical realizations of the trap, an increase in the
secular frequency with decreasing ion spacing may make a coupling rate that is
large relative to the decoherence rate in such a trap difficult to achieve."
Isaac Chuang,arXiv:0808.3086,https://arxiv.org/abs/0808.3086,"Abstract:  The codeword stabilized (CWS) quantum codes formalism presents a unifying
approach to both additive and nonadditive quantum error-correcting codes
(arXiv:0708.1021 [quant-ph]), but only for binary states. Here we generalize
the CWS framework to the nonbinary case (of both prime and nonprime dimension)
and map the search for nonbinary quantum codes to a corresponding search
problem for classical nonbinary codes with specific error patterns. We show
that while the additivity properties of nonbinary CWS codes are similar to the
binary case, the structural properties of the nonbinary codes differ
substantially from the binary case, even for prime dimensions. In particular,
we identify specific structure patterns of stabilizer groups, based on which
efficient constructions might be possible for codes that encode more dimensions
than any stabilizer codes of the same length and distance; similar methods
cannot be applied in the binary case. Understanding of these structural
properties can help prune the search space and facilitate the identification of
good nonbinary CWS codes."
Isaac Chuang,arXiv:0804.2665,https://arxiv.org/abs/0804.2665,"Abstract:  Electric field noise from fluctuating patch potentials is a significant
problem for a broad range of precision experiments, including trapped ion
quantum computation and single spin detection. Recent results demonstrated
strong suppression of this noise by cryogenic cooling, suggesting an underlying
thermal process. We present measurements characterizing the temperature and
frequency dependence of the noise from 7 to 100 K, using a single Sr+ ion
trapped 75 um above the surface of a gold plated surface electrode ion trap.
The noise amplitude is observed to have an approximate 1/f spectrum around 1
MHz, and grows rapidly with temperature as T^beta for beta from 2 to 4. The
data are consistent with microfabricated cantilever measurements of non-contact
friction but do not extrapolate to the DC measurements with neutral atoms or
contact potential probes."
Isaac Chuang,arXiv:0803.3232,https://arxiv.org/abs/0803.3232,"Abstract:  The codeword stabilized (""CWS"") quantum codes formalism presents a unifying
approach to both additive and nonadditive quantum error-correcting codes
(arXiv:0708.1021). This formalism reduces the problem of constructing such
quantum codes to finding a binary classical code correcting an error pattern
induced by a graph state. Finding such a classical code can be very difficult.
Here, we consider an algorithm which maps the search for CWS codes to a problem
of identifying maximum cliques in a graph. While solving this problem is in
general very hard, we prove three structure theorems which reduce the search
space, specifying certain admissible and optimal ((n,K,d)) additive codes. In
particular, we find there does not exist any ((7,3,3)) CWS code though the
linear programming bound does not rule it out. The complexity of the CWS search
algorithm is compared with the contrasting method introduced by Aggarwal and
Calderbank (arXiv:cs/0610159)."
Isaac Chuang,arXiv:0801.2360,https://arxiv.org/abs/0801.2360,"Abstract:  A long-standing open problem in fault-tolerant quantum computation has been
to find a universal set of transversal gates. As three of us proved in arXiv:
0706.1382, such a set does not exist for binary stabilizer codes. Here we
generalize our work to show that for subsystem stabilizer codes in $d$
dimensional Hilbert space, such a universal set of transversal gates cannot
exist for even one encoded qudit, for any dimension $d$, prime or nonprime.
This result strongly supports the idea that other primitives, such as quantum
teleportation, are necessary for universal fault-tolerant quantum computation,
and may be an important factor for fault tolerance noise thresholds."
Isaac Chuang,arXiv:0712.2084,https://arxiv.org/abs/0712.2084,"Abstract:  Teleportation is a crucial element in fault-tolerant quantum computation and
a complete understanding of its capacity is very important for the practical
implementation of optimal fault-tolerant architectures. It is known that
stabilizer codes support a natural set of gates that can be more easily
implemented by teleportation than any other gates. These gates belong to the so
called $\mathcal{C}_k$ hierarchy introduced by Gottesman and Chuang (Nature
\textbf{402}, 390). Moreover, a subset of $\mathcal{C}_k$ gates, called
semi-Clifford operations, can be implemented by an even simpler architecture
than the traditional teleportation setup (Phys. Rev. \textbf{A62}, 052316).
However, the precise set of gates in $\mathcal{C}_k$ remains unknown, even for
a fixed number of qubits $n$, which prevents us from knowing exactly what
teleportation is capable of. In this paper we study the structure of
$\mathcal{C}_k$ in terms of semi-Clifford operations, which send by conjugation
at least one maximal abelian subgroup of the $n$-qubit Pauli group into another
one. We show that for $n=1,2$, all the $\mathcal{C}_k$ gates are semi-Clifford,
which is also true for $\{n=3,k=3\}$. However, this is no longer true for
$\{n>2,k>3\}$. To measure the capability of this teleportation primitive, we
introduce a quantity called `teleportation depth', which characterizes how many
teleportation steps are necessary, on average, to implement a given gate. We
calculate upper bounds for teleportation depth by decomposing gates into both
semi-Clifford $\mathcal{C}_k$ gates and those $\mathcal{C}_k$ gates beyond
semi-Clifford operations, and compare their efficiency."
David Clark,arXiv:1806.10235,https://arxiv.org/abs/1806.10235,"Abstract:  Traditional program analysis analyses a program language, that is, all
programs that can be written in the language. There is a difference, however,
between all possible programs that can be written and the corpus of actual
programs written in a language. We seek to exploit this difference: for a given
program, we apply a bespoke program transformation Indexify to convert
expressions that current SMT solvers do not, in general, handle, such as
constraints on strings, into equisatisfiable expressions that they do handle.
To this end, Indexify replaces operators in hard-to-handle expressions with
homomorphic versions that behave the same on a finite subset of the domain of
the original operator, and return bottom denoting unknown outside of that
subset. By focusing on what literals and expressions are most useful for
analysing a given program, Indexify constructs a small, finite theory that
extends the power of a solver on the expressions a target program builds.
Indexify's bespoke nature necessarily means that its evaluation must be
experimental, resting on a demonstration of its effectiveness in practice. We
have developed Indexif}, a tool for Indexify. We demonstrate its utility and
effectiveness by applying it to two real world benchmarks --- string
expressions in coreutils and floats in fdlibm53. Indexify reduces
time-to-completion on coreutils from Klee's 49.5m on average to 6.0m. It
increases branch coverage on coreutils from 30.10% for Klee and 14.79% for
Zesti to 66.83%. When indexifying floats in fdlibm53, Indexifyl increases
branch coverage from 34.45% to 71.56% over Klee. For a restricted class of
inputs, Indexify permits the symbolic execution of program paths unreachable
with previous techniques: it covers more than twice as many branches in
coreutils as Klee."
David Clark,arXiv:1805.08889,https://arxiv.org/abs/1805.08889,"Abstract:  Neuromorphic architectures achieve low-power operation by using many simple
spiking neurons in lieu of traditional hardware. Here, we develop methods for
precise linear computations in spiking neural networks and use these methods to
map the evolution of a linear dynamical system (LDS) onto an existing
neuromorphic chip: IBM's TrueNorth. We analytically characterize, and
numerically validate, the discrepancy between the spiking LDS state sequence
and that of its non-spiking counterpart. These analytical results shed light on
the multiway tradeoff between time, space, energy, and accuracy in neuromorphic
computation. To demonstrate the utility of our work, we implemented a
neuromorphic Kalman filter (KF) and used it for offline decoding of human vocal
pitch from neural data. The neuromorphic KF could be used for low-power
filtering in domains beyond neuroscience, such as navigation or robotics."
David Clark,arXiv:1711.06355,https://arxiv.org/abs/1711.06355,"Abstract:  We present a study of comet C/2017 K2 (PANSTARRS) using prediscovery archival
data taken from 2013 to 2017. Our measurements show that the comet has been
marginally increasing in activity since at least 2013 May (heliocentric
distance of $r_{\mathrm{H}} = 23.7$ AU pre-perihelion). We estimate the
mass-loss rate during the period 2013--2017 as $\overline{\dot{M}} \approx
\left(2.4 \pm 1.1 \right) \times 10^{2}$ kg s$^{-1}$, which requires a minimum
active surface area of $\sim$10--10$^2$ km$^{2}$ for sublimation of
supervolatiles such as CO and CO$_2$, by assuming a nominal cometary albedo
$p_V = 0.04 \pm 0.02$. The corresponding lower limit to the nucleus radius is a
few kilometers. Our Monte Carlo dust simulations show that dust grains in the
coma are $\gtrsim0.5$ mm in radius, with ejection speeds from $\sim$1--3 m
s$^{-1}$, and have been emitted in a protracted manner since 2013, confirming
estimates by Jewitt et al. (2017). The current heliocentric orbit is
hyperbolic. Our N-body backward dynamical integration of the orbit suggests
that the comet is most likely (with a probability of $\sim$98\%) from the Oort
spike. The calculated median reciprocal of the semimajor axis 1 Myr ago was
$a_{\mathrm{med}}^{-1} = \left( 3.61 \pm 1.71 \right) \times 10^{-5}$ AU$^{-1}$
(in a reference system of the solar-system barycentre)."
David Clark,arXiv:1609.02404,https://arxiv.org/abs/1609.02404,"Abstract:  Malware creators have been getting their way for too long now. String-based
similarity measures can leverage ground truth in a scalable way and can operate
at a level of abstraction that is difficult to combat from the code level. We
introduce ITect, a scalable approach to malware similarity detection based on
information theory. ITect targets file entropy patterns in different ways to
achieve 100% precision with 90% accuracy but it could target 100% recall
instead. It outperforms VirusTotal for precision and accuracy on combined
Kaggle and VirusShare malware."
David Clark,arXiv:1602.03123,https://arxiv.org/abs/1602.03123,"Abstract:  Temporarily Captured Orbiters (TCOs) are Near-Earth Objects (NEOs) which make
a few orbits of Earth before returning to heliocentric orbits. Only one TCO has
been observed to date, 2006 RH120, captured by Earth for one year before
escaping. Detailed modeling predicts capture should occur from the NEO
population predominantly through the Sun-Earth L1 and L2 points, with 1% of
TCOs impacting Earth and approximately 0.1% of meteoroids being TCOs. Although
thousands of meteoroid orbits have been measured, none until now have
conclusively exhibited TCO behaviour, largely due to difficulties in measuring
initial meteoroid speed with sufficient precision. We report on a precise
meteor observation of January 13, 2014 by a new generation of all-sky fireball
digital camera systems operated in the Czech Republic as part of the European
Fireball Network, providing the lowest natural object entry speed observed in
decades long monitoring by networks world-wide. Modeling atmospheric
deceleration and fragmentation yields an initial mass of ~5 kg and diameter of
15 cm, with a maximum Earth-relative velocity just over 11.0 km/s. Spectral
observations prove its natural origin. Back-integration across observational
uncertainties yields a 92 - 98% probability of TCO behaviour, with close lunar
dynamical interaction. The capture duration varies across observational
uncertainties from 48 days to 5+ years. We also report on two low-speed impacts
recorded by US Government sensors, and we examine Prairie Network event PN39078
from 1965 having an extremely low entry speed of 10.9 km/s. In these cases
uncertainties in measurement and origin make TCO designation uncertain."
David Clark,arXiv:1509.07606,https://arxiv.org/abs/1509.07606,"Abstract:  We present studies of C/2015 D1 (SOHO), the first sunskirting comet ever seen
from ground stations over the past half century. The Solar and Heliospheric
Observatory (SOHO) witnessed its peculiar light curve with a huge dip followed
by a flareup around perihelion: the dip was likely caused by sublimation of
olivines, directly evidenced by a coincident temporary disappearance of the
tail. The flareup likely reflects a disintegration event, which we suggest was
triggered by intense thermal stress established within the nucleus interior.
Photometric data reveal an increasingly dusty coma, indicative of volatile
depletion. A catastrophic mass loss rate of $\sim$10$^{5}$ kg s$^{-1}$ around
perihelion was seen. Ground-based Xingming Observatory spotted the
post-perihelion debris cloud. Our morphological simulations of post-perihelion
images find newly released dust grains of size $a \gtrsim 10$ $\mu$m in radius,
however, a temporal increase in $a_{\min}$ was also witnessed, possibly due to
swift dispersions of smaller grains swept away by radiation forces without
replenishment. Together with the fading profile of the light curve, a power law
dust size distribution with index $\gamma = 3.2 \pm 0.1$ is derived. We
detected no active remaining cometary nuclei over $\sim$0.1 km in radius in
post-perihelion images acquired at Lowell Observatory. Applying radial
non-gravitational parameter, $\mathcal{A}_{1} = \left(1.209 \pm 0.118 \right)
\times 10^{-6}$ AU day$^{-2}$, from an isothermal water-ice sublimation model
to the SOHO astrometry significantly reduces residuals and sinusoidal trends in
the orbit determination. The nucleus mass $\sim$10$^{8}$--10$^{9}$ kg, and the
radius $\sim$50--150 m (bulk density $\rho_{\mathrm{d}} = 0.4$ g cm$^{-3}$
assumed) before the disintegration are deduced from the photometric data;
consistent results were determined from the non-gravitational effects."
David Clark,arXiv:1506.03482,https://arxiv.org/abs/1506.03482,"Abstract:  A common and natural intuition among software testers is that test cases need
to differ if a software system is to be tested properly and its quality
ensured. Consequently, much research has gone into formulating distance
measures for how test cases, their inputs and/or their outputs differ. However,
common to these proposals is that they are data type specific and/or calculate
the diversity only between pairs of test inputs, traces or outputs.
We propose a new metric to measure the diversity of sets of tests: the test
set diameter (TSDm). It extends our earlier, pairwise test diversity metrics
based on recent advances in information theory regarding the calculation of the
normalized compression distance (NCD) for multisets. An advantage is that TSDm
can be applied regardless of data type and on any test-related information, not
only the test inputs. A downside is the increased computational time compared
to competing approaches.
Our experiments on four different systems show that the test set diameter can
help select test sets with higher structural and fault coverage than random
selection even when only applied to test inputs. This can enable early test
design and selection, prior to even having a software system to test, and
complement other types of test automation and analysis. We argue that this
quantification of test set diversity creates a number of opportunities to
better understand software quality and provides practical ways to increase it."
David Clark,arXiv:1502.07661,https://arxiv.org/abs/1502.07661,"Abstract:  This work focuses on a specific front of the malware detection arms-race,
namely the detection of persistent, disk-resident malware. We exploit
normalised compression distance (NCD), an information theoretic measure,
applied directly to binaries. Given a zoo of labelled malware and benign-ware,
we ask whether a suspect program is more similar to our malware or to our
benign-ware. Our approach classifies malware with 97.1% accuracy and a false
positive rate of 3%. We achieve our results with off-the-shelf compressors and
a standard machine learning classifier and without any specialised knowledge.
An end-user need only collect a zoo of malware and benign-ware and then can
immediately apply our techniques.
We apply statistical rigour to our experiments and our selection of data. We
demonstrate that accuracy can be optimised by combining NCD with the
compressibility rates of the executables. We demonstrate that malware reported
within a more narrow time frame of a few days is more homogenous than malware
reported over a longer one of two years but that our method still classifies
the latter with 95.2% accuracy and a 5% false positive rate. Due to the use of
compression, the time and computation cost of our method is non-trivial. We
show that simple approximation techniques can improve the time complexity of
our approach by up to 63%.
We compare our results to the results of applying the 59 anti-malware
programs used on the VirusTotal web site to our malware. Our approach does
better than any single one of them as well as the 59 used collectively."
David Clark,arXiv:1306.5339,https://arxiv.org/abs/1306.5339,"Abstract:  We give a new solution to the famous Gion shrine geometry problem from
eighteenth-century Japan. Like the classical Japanese solution, ours is given
in the form of a degree ten equation. However, our polynomial has the advantage
of being much easier to write down. We also provide some additional analysis,
including a discussion of existence and uniqueness."
David Clark,arXiv:1008.4747,https://arxiv.org/abs/1008.4747,"Abstract:  This paper develops a general method for constructing entanglement-assisted
quantum low-density parity-check (LDPC) codes, which is based on combinatorial
design theory. Explicit constructions are given for entanglement-assisted
quantum error-correcting codes (EAQECCs) with many desirable properties. These
properties include the requirement of only one initial entanglement bit, high
error correction performance, high rates, and low decoding complexity. The
proposed method produces infinitely many new codes with a wide variety of
parameters and entanglement requirements. Our framework encompasses various
codes including the previously known entanglement-assisted quantum LDPC codes
having the best error correction performance and many new codes with better
block error rates in simulations over the depolarizing channel. We also
determine important parameters of several well-known classes of quantum and
classical LDPC codes for previously unsettled cases."
David Clark,arXiv:0911.0665,https://arxiv.org/abs/0911.0665,"Abstract:  We present high sensitivity H91$\alpha$ and 3.5 cm radio continuum
observations toward the planetary nebula NGC 3242. The electron temperature
determined assuming local thermodynamic equilibrium is consistent within
$\sim$10% with that derived from optical lines and the Balmer discontinuity.
The line emission and the continuum emission have very similar spatial
distribution, suggesting that at this wavelength there is no other continuum
process present in a significant manner. In particular, we conclude that
emission from spinning dust is not important at this wavelength. In this radio
recombination line the nebula presents a radial velocity structure consistent
with that obtained from observations of optical lines."
David Clark,arXiv:0806.0601,https://arxiv.org/abs/0806.0601,"Abstract:  We prove that Morrison and Nieh's categorification of the su(3) quantum knot
invariant is functorial with respect to tangle cobordisms. This is in contrast
to the categorified su(2) theory, which was not functorial as originally
defined. We use methods of Bar-Natan to construct explicit chain maps for each
variation of the third Reidemeister move. Then, to show functoriality, we
modify arguments used by Clark, Morrison, and Walker to show that induced chain
maps are invariant under Carter and Saito's movie moves."
David Clark,arXiv:0707.0040,https://arxiv.org/abs/0707.0040,"Abstract:  Neutron resonance spectrometry (NRS) has been used to measure the temperature
inside Mo samples during shock loading. The temperatures obtained were
significantly higher than predicted assuming ideal hydrodynamic loading. The
effect of plastic flow and non-ideal projectile behavior were assessed. Plastic
flow was calculated self-consistently with the shock jump conditions: this is
necessary for a rigorous estimate of the locus of shock states accessible.
Plastic flow was estimated to contribute a temperature rise of 53K compared
with hydrodynamic flow. Simulations were performed of the operation of the
explosively-driven projectile system used to induce the shock in the Mo sample.
The simulations predicted that the projectile was significantly curved on
impact, and still accelerating. The resulting spatial variations in load,
including radial components of velocity, were predicted to increase the
apparent temperature that would be deduced from the width of the neutron
resonance by 160K. These corrections are sufficient to reconcile the apparent
temperatures deduced using NRS with the accepted properties of Mo, in
particular its equation of state."
David Clark,arXiv:0704.1850,https://arxiv.org/abs/0704.1850,"Abstract:  Shock and release temperatures in Mo were calculated, taking account of
heating from plastic flow predicted using the Steinberg-Guinan model. Plastic
flow was calculated self-consistently with the shock jump conditions: this is
necessary for a rigorous estimate of the locus of shock states accessible. The
temperatures obtained were significantly higher than predicted assuming ideal
hydrodynamic loading. The temperatures were compared with surface emission
spectrometry measurements for Mo shocked to around 60GPa and then released into
vacuum or into a LiF window. Shock loading was induced by the impact of a
planar projectile, accelerated by high explosive or in a gas gun. Surface
velocimetry showed an elastic wave at the start of release from the shocked
state; the amplitude of the elastic wave matched the prediction to around 10%,
indicating that the predicted flow stress in the shocked state was reasonable.
The measured temperatures were consistent with the simulations, indicating that
the fraction of plastic work converted to heat was in the range 70-100% for
these loading conditions."
David Clark,arXiv:cond-mat/0702693,https://arxiv.org/abs/cond-mat/0702693,"Abstract:  The hydrodynamic operation of the `Forest Flyer' type of explosive launching
system for shock physics projectiles was investigated in detail using one- and
two-dimensional continuum dynamics simulations. The simulations were
insensitive to uncertainties in the material properties, and reproduced
measurements of the projectile. The most commonly-used variant, with an Al
alloy case, was predicted to produce a slightly curved projectile, subjected to
some shock heating, and likely exhibiting some porosity from tensile damage.
The flatness can be improved by using a case of lower shock impedance, such as
polymethyl methacrylate. High-impedance cases, including Al alloys but with
denser materials improving the launching efficiency, can be used if designed
according to the physics of oblique shock reflection. The tensile stress
induced in the projectile depends on the relative thickness of the explosive,
expansion gap, and projectile. The thinner the projectile with respect to the
explosive, the smaller the tensile stress. If the explosive is initiated with a
plane wave lens, the tensile stress is lower than for initiation with multiple
detonators over a plane. The previous plane wave lens designs did however
induce a tensile stress close to the spall strength of the projectile. The
tensile stress can be reduced by changes in the component thicknesses.
Experiments to verify the operation of explosively-launched projectiles should
attempt to measure porosity induced in the projectile: arrival time
measurements may be insensitive to porous regions caused by damaged or
recollected material."
David Clark,arXiv:math/0701339,https://arxiv.org/abs/math/0701339,"Abstract:  We describe a modification of Khovanov homology (math.QA/9908171), in the
spirit of Bar-Natan (math.GT/0410495), which makes the theory properly
functorial with respect to link cobordisms.
This requires introducing `disorientations' in the category of smoothings and
abstract cobordisms between them used in Bar-Natan's definition.
Disorientations have `seams' separating oppositely oriented regions, coming
with a preferred normal direction. The seams satisfy certain relations (just as
the underlying cobordisms satisfy relations such as the neck cutting relation).
We construct explicit chain maps for the various Reidemeister moves, then
prove that the compositions of chain maps associated to each side of each of
Carter and Saito's movie moves (MR1238875, MR1445361) always agree. These
calculations are greatly simplified by following arguments due to Bar-Natan and
Khovanov, which ensure that the two compositions must agree, up to a sign. We
set up this argument in our context by proving a result about duality in
Khovanov homology, generalising previous results about mirror images of knots
to a `local' result about tangles. Along the way, we reproduce Jacobsson's sign
table (math.GT/0206303) for the original `unoriented theory', with a few
disagreements."
David Clark,arXiv:cs/9901011,https://arxiv.org/abs/cs/9901011,"Abstract:  The Internet has revolutionized the computer and communications world like
nothing before. The invention of the telegraph, telephone, radio, and computer
set the stage for this unprecedented integration of capabilities. The Internet
is at once a world-wide broadcasting capability, a mechanism for information
dissemination, and a medium for collaboration and interaction between
individuals and their computers without regard for geographic location.
In this paper, several of us involved in the development and evolution of the
Internet share our views of its origins and history. This is intended to be a
brief, necessarily cursory and incomplete history. This history revolves around
four distinct aspects. There is the technological evolution that began with
early research on packet switching and the ARPANET (and related technologies),
and where current research continues to expand the horizons of the
infrastructure along several dimensions, such as scale, performance, and higher
level functionality. There is the operations and management aspect of a global
and complex operational infrastructure. There is the social aspect, which
resulted in a broad community of Internauts working together to create and
evolve the technology. And there is the commercialization aspect, resulting in
an extremely effective transition of research results into a broadly deployed
and available information infrastructure."
